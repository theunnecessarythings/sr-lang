package torch

// --- Wrappers ---
torch_bindings :: import "torch_bindings"

io :: import "std/io"
os :: import "std/os"

// ===== Pool + guards + simple tracked creators =====
__pool: [dyn]torch_bindings.Tensor = []

torch_version :: extern proc() *const u8
version :: proc() *const u8 {
    return torch_version()
}

read_and_clean_error :: proc() {
    err := torch_bindings.get_and_reset_last_err()
    if err.^i64 == 0 { return }
    defer torch_bindings.at_free(err.^*void)
    io.print("Torch Error: %s\n", err)
    os.exit(1)
}

wrap_and_track :: proc(raw: torch_bindings.Tensor) Tensor {
    __pool.append(raw)
    read_and_clean_error()
    return Tensor{ t: raw }
}

MemoryGuard :: struct {}
MemoryGuard.init :: proc(pool_name: string) MemoryGuard {
    _ = pool_name
    return MemoryGuard{}
}
MemoryGuard.deinit :: proc(self: *MemoryGuard) void {
    _ = self
    i: usize = 0
    while i < __pool.len { 
        torch_bindings.at_free(__pool[i]) 
        i = i + 1
    }
    __pool = []
}

autograd_set_enabled :: proc(enabled: bool) void {
    _ = torch_bindings.at_grad_set_enabled(if enabled { 1 } else { 0 })
}
NoGradGuard :: struct { prev: bool }
NoGradGuard.init :: proc() NoGradGuard {
    autograd_set_enabled(false)
    return NoGradGuard{ prev: true }
}
NoGradGuard.deinit :: proc(self: *NoGradGuard) void { 
    autograd_set_enabled(self.prev)
}

// Simple dtype/device for examples
DType :: enum { Float }
Device :: enum { Cpu }
TensorOptions :: struct { dtype: DType, device: Device }
FLOAT_CPU : TensorOptions = TensorOptions{ dtype: DType.Float, device: Device.Cpu }

zeros_tracked :: proc(size: [dyn]i64, opts: TensorOptions) Tensor {
    _ = opts
    if (size.len == 0) { 
        return wrap_and_track(torch_bindings.at_new_tensor()) 
    }
    t := Tensor.zeros(&size[0], size.len.(i32), 6, -1) // Float, Cpu
    // Tensor.zeros already tracks; return as-is to avoid double-tracking
    return t
}

// ===== Minimal nn: Linear, ReLU, Sequential to unblock examples =====
ForwardFn :: *proc(*void, *Tensor) Tensor
ModuleNode :: struct { ctx: *void, call: ForwardFn }
NNModule :: struct { is_training: bool }
NNModule.init :: proc() NNModule { return NNModule{ is_training: true } }

LinearOptions :: struct { in_features: i64, out_features: i64, bias: bool, options: TensorOptions }
LinearOptions.default :: proc(in_f: i64, out_f: i64) LinearOptions { 
    return LinearOptions{ in_features: in_f, out_features: out_f, bias: true, options: FLOAT_CPU }
}
Linear :: struct { base: NNModule, weight: Tensor, has_bias: bool, bias: Tensor, opts: LinearOptions }
Linear.init :: proc(opts: LinearOptions) Linear {
    m := NNModule.init()
    lin := Linear{ base: m, weight: Tensor{ t: null }, has_bias: false, bias: Tensor{ t: null }, opts: opts }
    Linear.reset(&lin)
    return lin
}
Linear.reset :: proc(self: *Linear) void {
    dims: [2]i64 = undefined
    dims[0] = self.opts.out_features
    dims[1] = self.opts.in_features
    self.weight = Tensor.empty(&dims[0], 2, 6, -1)
    _ = self.weight.set_requires_grad(true)
    if (self.opts.bias) {
        bdims: [1]i64 = undefined
        bdims[0] = self.opts.out_features
        self.bias = Tensor.empty(&bdims[0], 1, 6, -1)
        _ = self.bias.set_requires_grad(true)
        self.has_bias = true
    } else {
        self.has_bias = false
    }
}
Linear.forward :: proc(self: *Linear, input: *Tensor) Tensor {
    if (self.has_bias) {
        r := input.linear(self.weight, self.bias)
        // input.linear already tracks the result
        return r
    }
    r2 := input.linear(self.weight, Tensor{ t: null })
    // input.linear already tracks the result
    return r2
}
linear_forward_call :: @[llvm_fn] proc(ctx: *void, input: *Tensor) Tensor {
    lin: *Linear = ctx.^*Linear
    return lin.forward(input)
}
Linear.node :: proc(self: *Linear) ModuleNode {
    return ModuleNode{ ctx: self.^*void, call: &linear_forward_call }
}
Linear.base_ptr :: proc(self: *Linear) *NNModule {
    return &self.base
}

ReLU :: struct { base: NNModule, inplace: bool }
ReLU.init :: proc(inplace: bool) ReLU { return ReLU{ base: NNModule.init(), inplace: inplace } }
ReLU.forward :: proc(self: *ReLU, input: *Tensor) Tensor {
    t := input.relu()
    // input.relu already tracks the result
    return t
}
relu_forward_call :: @[llvm_fn] proc(ctx: *void, input: *Tensor) Tensor {
    m: *ReLU = ctx.^*ReLU
    return m.forward(input)
}
ReLU.node :: proc(self: *ReLU) ModuleNode {
    return ModuleNode{ ctx: self.^*void, call: &relu_forward_call }
}
ReLU.base_ptr :: proc(self: *ReLU) *NNModule {
    return &self.base
}

LayerNormOptions :: struct {
    normalized_shape: [dyn]i64,
    eps: f64,
    elementwise_affine: bool,
}

LayerNorm :: struct {
    base: NNModule,
    weight: Tensor,
    bias: Tensor,
    opts: LayerNormOptions,
}

LayerNorm.init :: proc(opts: LayerNormOptions) LayerNorm {
    m := NNModule.init()
    ln := LayerNorm{ base: m, weight: Tensor{ t: null }, bias: Tensor{ t: null }, opts: opts }
    if ln.opts.elementwise_affine {
        ln.weight = Tensor.empty(ln.opts.normalized_shape.ptr, ln.opts.normalized_shape.len.(i32), 6, -1)
        ln.bias = Tensor.empty(ln.opts.normalized_shape.ptr, ln.opts.normalized_shape.len.(i32), 6, -1)
        _ = ln.weight.set_requires_grad(true)
        _ = ln.bias.set_requires_grad(true)
    }
    return ln
}

LayerNorm.forward :: proc(self: *LayerNorm, input: *Tensor) Tensor {
    (output, _, _) := input.native_layer_norm(self.opts.normalized_shape.ptr, self.opts.normalized_shape.len.(i32), self.weight, self.bias, self.opts.eps)
    return output
}

Embedding :: struct {
    base: NNModule,
    weight: Tensor,
    num_embeddings: i64,
    embedding_dim: i64,
    padding_idx: i64,
}

Embedding.init :: proc(num_embeddings: i64, embedding_dim: i64) Embedding {
    m := NNModule.init()
    emb := Embedding{ base: m, weight: Tensor{ t: null }, num_embeddings: num_embeddings, embedding_dim: embedding_dim, padding_idx: -1 }
    dims: [2]i64 = [num_embeddings, embedding_dim]
    emb.weight = Tensor.empty(&dims[0], 2, 6, -1)
    _ = emb.weight.set_requires_grad(true)
    return emb
}

Embedding.forward :: proc(self: *Embedding, input: Tensor) Tensor {
    return Tensor.embedding(self.weight, input, self.padding_idx, false, false)
}


Sequential :: struct { base: NNModule, nodes: [dyn]ModuleNode }
Sequential.init :: proc() Sequential {
    return Sequential{ base: NNModule.init(), nodes: [] }
}
Sequential.add :: proc(self: *Sequential, name: string, node: ModuleNode, child_base: *NNModule) void {
    _ = name
    _ = child_base
    self.nodes.append(node)
}
Sequential.forward :: proc(self: *Sequential, input: *Tensor) Tensor {
    out := input.*
    i: usize = 0
    while i < self.nodes.len {
        node := self.nodes[i]
        tmp := node.call.*(node.ctx, &out)
        out = tmp
        i = i + 1
    }
    return out
}

Tensor :: struct { t: torch_bindings.Tensor }

Tensor.from_blob :: proc(data: *void, dims: *i64, ndims: usize, strides: *i64, nstrides: usize, dtype: i32, device: i32) Tensor {
    raw := torch_bindings.at_tensor_of_blob(data, dims, ndims, strides, nstrides, dtype, device)
    return wrap_and_track(raw)
}

Tensor.itemf :: proc(self: *Tensor) f32 {
    return torch_bindings.at_tensor_item_float(self.t)
}

// --- Helpers from torch_api.h ---

// Tensor convenience methods
Tensor.free :: proc(self: *Tensor) void { 
    torch_bindings.at_free(self.t)
}
Tensor.print :: proc(self: *Tensor) void { 
    torch_bindings.at_print(self.t)
}
Tensor.defined :: proc(self: *Tensor) bool {
    return torch_bindings.at_defined(self.t) != 0
}
Tensor.is_sparse :: proc(self: *Tensor) bool {
    return torch_bindings.at_is_sparse(self.t) != 0 
}
Tensor.is_contiguous :: proc(self: *Tensor) bool { 
    return torch_bindings.at_is_contiguous(self.t) != 0 
}
Tensor.device :: proc(self: *Tensor) i32 { 
    return torch_bindings.at_device(self.t) 
}
Tensor.dim :: proc(self: *Tensor) usize {
    return torch_bindings.at_dim(self.t) 
}
Tensor.size :: proc(self: *Tensor, dim: i64) i64 {
    dims: [16]i64 = undefined
    torch_bindings.at_shape(self.t, &dims[0])
    return dims[dim]
}
Tensor.scalar_type :: proc(self: *Tensor) i32 { 
    return torch_bindings.at_scalar_type(self.t)
}
Tensor.data_ptr :: proc(self: *Tensor) *void { 
    return torch_bindings.at_data_ptr(self.t)
}
Tensor.shallow_clone :: proc(self: *Tensor) Tensor { 
    return wrap_and_track(torch_bindings.at_shallow_clone(self.t))
}
Tensor.copy_from :: proc(self: *Tensor, src: Tensor) void { 
    torch_bindings.at_copy_(self.t, src.t)
}
Tensor.backward :: proc(self: *Tensor, keep_graph: bool, create_graph: bool) void {
    torch_bindings.at_backward(self.t, if keep_graph {1} else {0}, if create_graph {1} else {0})
}
Tensor.requires_grad :: proc(self: *Tensor) bool { 
    return torch_bindings.at_requires_grad(self.t) != 0
}

// IO helpers
Tensor.save :: proc(self: *Tensor, filename: string) void { torch_bindings.at_save(self.t, filename.ptr) }
Tensor.to_cstring :: proc(self: *Tensor, line_size: i32) *const u8 { return torch_bindings.at_to_string(self.t, line_size) }

Tensor.load :: proc(filename: string) Tensor { return wrap_and_track(torch_bindings.at_load(filename.ptr)) }
Tensor.load_image :: proc(filename: string) Tensor { return wrap_and_track(torch_bindings.at_load_image(filename.ptr)) }
Tensor.save_image :: proc(self: *Tensor, filename: string) i32 { return torch_bindings.at_save_image(self.t, filename.ptr) }
Tensor.resize_image :: proc(self: *Tensor, w: i32, h: i32) Tensor { return wrap_and_track(torch_bindings.at_resize_image(self.t, w, h)) }

// Scalar helpers
scalar_int :: proc(v: i64) torch_bindings.Scalar { return torch_bindings.ats_int(v) }
scalar_float :: proc(v: f64) torch_bindings.Scalar { return torch_bindings.ats_float(v) }
scalar_to_int :: proc(s: torch_bindings.Scalar) i64 { return torch_bindings.ats_to_int(s) }
scalar_to_float :: proc(s: torch_bindings.Scalar) f64 { return torch_bindings.ats_to_float(s) }
scalar_free :: proc(s: torch_bindings.Scalar) void { torch_bindings.ats_free(s) }

// Global/Context helpers
manual_seed :: proc(seed: i64) void { torch_bindings.at_manual_seed(seed) }
get_last_error_cstr :: proc() *const u8 { return torch_bindings.get_and_reset_last_err() }

cuda_is_available :: proc() bool { return torch_bindings.atc_cuda_is_available() != 0 }
cuda_device_count :: proc() i32 { return torch_bindings.atc_cuda_device_count() }
cudnn_is_available :: proc() bool { return torch_bindings.atc_cudnn_is_available() != 0 }
cuda_manual_seed :: proc(seed: u64) void { torch_bindings.atc_manual_seed(seed) }
cuda_manual_seed_all :: proc(seed: u64) void { torch_bindings.atc_manual_seed_all(seed) }
cuda_synchronize :: proc(device_index: i64) void { torch_bindings.atc_synchronize(device_index) }

get_num_threads :: proc() i32 { return torch_bindings.at_get_num_threads() }
set_num_threads :: proc(n: i32) void { torch_bindings.at_set_num_threads(n) }
get_num_interop_threads :: proc() i32 { return torch_bindings.at_get_num_interop_threads() }
set_num_interop_threads :: proc(n: i32) void { torch_bindings.at_set_num_interop_threads(n) }

autocast_is_enabled :: proc() bool { return torch_bindings.at_autocast_is_enabled() }
autocast_set_enabled :: proc(enabled: bool) bool { return torch_bindings.at_autocast_set_enabled(enabled) }
autocast_clear_cache :: proc() void { torch_bindings.at_autocast_clear_cache() }

// Optimizer wrapper
Optimizer :: struct { o: torch_bindings.Optimizer }

Optimizer.adam :: proc(learning_rate: f64, beta1: f64, beta2: f64, weight_decay: f64, eps: f64, amsgrad: bool) Optimizer {
    return Optimizer{ o: torch_bindings.ato_adam(learning_rate, beta1, beta2, weight_decay, eps, amsgrad) }
}
Optimizer.adamw :: proc(learning_rate: f64, beta1: f64, beta2: f64, weight_decay: f64, eps: f64, amsgrad: bool) Optimizer {
    return Optimizer{ o: torch_bindings.ato_adamw(learning_rate, beta1, beta2, weight_decay, eps, amsgrad) }
}
Optimizer.rms_prop :: proc(learning_rate: f64, alpha: f64, eps: f64, weight_decay: f64, momentum: f64, centered: bool) Optimizer {
    return Optimizer{ o: torch_bindings.ato_rms_prop(learning_rate, alpha, eps, weight_decay, momentum, if centered { 1 } else {0}) }
}
Optimizer.sgd :: proc(learning_rate: f64, momentum: f64, dampening: f64, weight_decay: f64, nesterov: bool) Optimizer {
    return Optimizer{ o: torch_bindings.ato_sgd(learning_rate, momentum, dampening, weight_decay, if nesterov {1} else {0}) }
}
Optimizer.add_parameters :: proc(self: *Optimizer, t: Tensor, group: usize) void { torch_bindings.ato_add_parameters(self.o, t.t, group) }
Optimizer.set_learning_rate :: proc(self: *Optimizer, learning_rate: f64) void { torch_bindings.ato_set_learning_rate(self.o, learning_rate) }
Optimizer.zero_grad :: proc(self: *Optimizer) void { torch_bindings.ato_zero_grad(self.o) }
Optimizer.step :: proc(self: *Optimizer) void { torch_bindings.ato_step(self.o) }
Optimizer.free :: proc(self: *Optimizer) void { torch_bindings.ato_free(self.o) }

// TorchScript Module wrapper (subset)
Module :: struct { m: torch_bindings.Module }

Module.load :: proc(path: string) Module { return Module{ m: torch_bindings.atm_load(path.ptr) } }
Module.load_on_device :: proc(path: string, device: i32) Module { return Module{ m: torch_bindings.atm_load_on_device(path.ptr, device) } }
Module.eval :: proc(self: *Module) void { torch_bindings.atm_eval(self.m) }
Module.train :: proc(self: *Module) void { torch_bindings.atm_train(self.m) }
Module.free :: proc(self: *Module) void { torch_bindings.atm_free(self.m) }
Module.to :: proc(self: *Module, device: i32, dtype: i32, non_blocking: bool) void { torch_bindings.atm_to(self.m, device, dtype, non_blocking) }
Module.save :: proc(self: *Module, path: string) void { torch_bindings.atm_save(self.m, path.ptr) }
// Low-level forward that accepts a pointer to a contiguous array of Tensors
Module.forward_raw :: proc(self: *Module, tensors: *torch_bindings.Tensor, ntensors: i32) Tensor {
    return wrap_and_track(torch_bindings.atm_forward(self.m, tensors, ntensors))
}

Tensor.__and__ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___and__(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.__and__tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___and__tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.__iand__ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___iand__(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.__iand__tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___iand__tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.__ilshift__ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___ilshift__(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.__ilshift__tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___ilshift__tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.__ior__ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___ior__(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.__ior__tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___ior__tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.__irshift__ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___irshift__(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.__irshift__tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___irshift__tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.__ixor__ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___ixor__(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.__ixor__tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___ixor__tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.__lshift__ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___lshift__(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.__lshift__scalar_out_ :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___lshift__scalar_out_(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.__lshift__tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___lshift__tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.__lshift__tensor_out_ :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___lshift__tensor_out_(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.__or__ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___or__(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.__or__tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___or__tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.__rshift__ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___rshift__(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.__rshift__scalar_out_ :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___rshift__scalar_out_(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.__rshift__tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___rshift__tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.__rshift__tensor_out_ :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___rshift__tensor_out_(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.__xor__ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___xor__(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.__xor__tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg___xor__tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor._adaptive_avg_pool2d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__adaptive_avg_pool2d(&outs[0], self.t, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor._adaptive_avg_pool2d_backward :: proc(self: *Tensor, grad_output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__adaptive_avg_pool2d_backward(&outs[0], grad_output.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._adaptive_avg_pool2d_backward_out :: proc(self: *Tensor, out: Tensor, grad_output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__adaptive_avg_pool2d_backward_out(&outs[0], out.t, grad_output.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._adaptive_avg_pool2d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__adaptive_avg_pool2d_out(&outs[0], out.t, self.t, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor._adaptive_avg_pool3d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__adaptive_avg_pool3d(&outs[0], self.t, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor._adaptive_avg_pool3d_backward :: proc(self: *Tensor, grad_output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__adaptive_avg_pool3d_backward(&outs[0], grad_output.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._adaptive_avg_pool3d_backward_out :: proc(self: *Tensor, out: Tensor, grad_output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__adaptive_avg_pool3d_backward_out(&outs[0], out.t, grad_output.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._adaptive_avg_pool3d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__adaptive_avg_pool3d_out(&outs[0], out.t, self.t, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor._add_batch_dim :: proc(self: *Tensor, batch_dim: i64, level: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__add_batch_dim(&outs[0], self.t, batch_dim, level)
    return wrap_and_track(outs[0])
}

Tensor._add_relu :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__add_relu(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor._add_relu_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__add_relu_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor._add_relu_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__add_relu_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor._add_relu_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__add_relu_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor._add_relu_scalar_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__add_relu_scalar_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor._add_relu_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__add_relu_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor._addmm_activation :: proc(self: *Tensor, mat1: Tensor, mat2: Tensor, use_gelu: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__addmm_activation(&outs[0], self.t, mat1.t, mat2.t, use_gelu)
    return wrap_and_track(outs[0])
}

Tensor._addmm_activation_out :: proc(self: *Tensor, out: Tensor, mat1: Tensor, mat2: Tensor, use_gelu: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__addmm_activation_out(&outs[0], out.t, self.t, mat1.t, mat2.t, use_gelu)
    return wrap_and_track(outs[0])
}

Tensor._aminmax :: proc(self: *Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__aminmax(&outs[0], self.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._aminmax_dim :: proc(self: *Tensor, dim: i64, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__aminmax_dim(&outs[0], self.t, dim, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._aminmax_dim_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, dim: i64, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__aminmax_dim_out(&outs[0], out0.t, out1.t, self.t, dim, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._aminmax_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__aminmax_out(&outs[0], out0.t, out1.t, self.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._amp_update_scale :: proc(self: *Tensor, growth_tracker: Tensor, found_inf: Tensor, scale_growth_factor: f64, scale_backoff_factor: f64, growth_interval: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__amp_update_scale(&outs[0], self.t, growth_tracker.t, found_inf.t, scale_growth_factor, scale_backoff_factor, growth_interval)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._amp_update_scale_ :: proc(self: *Tensor, growth_tracker: Tensor, found_inf: Tensor, scale_growth_factor: f64, scale_backoff_factor: f64, growth_interval: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__amp_update_scale_(&outs[0], self.t, growth_tracker.t, found_inf.t, scale_growth_factor, scale_backoff_factor, growth_interval)
    return wrap_and_track(outs[0])
}

Tensor._amp_update_scale_out :: proc(self: *Tensor, out: Tensor, growth_tracker: Tensor, found_inf: Tensor, scale_growth_factor: f64, scale_backoff_factor: f64, growth_interval: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__amp_update_scale_out(&outs[0], out.t, self.t, growth_tracker.t, found_inf.t, scale_growth_factor, scale_backoff_factor, growth_interval)
    return wrap_and_track(outs[0])
}

Tensor._assert_scalar :: proc(self_scalar: torch_bindings.Scalar, assert_msg_ptr: *u8, assert_msg_len: i32) void {
    torch_bindings.atg__assert_scalar(self_scalar, assert_msg_ptr, assert_msg_len)
}

Tensor._assert_tensor_metadata :: proc(a: Tensor, size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32, dtype: i32, device: i32, layout: i8) void {
    torch_bindings.atg__assert_tensor_metadata(a.t, size_data, size_len, stride_data, stride_len, dtype, device, layout)
}

Tensor._autocast_to_full_precision :: proc(self: *Tensor, cuda_enabled: bool, cpu_enabled: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__autocast_to_full_precision(&outs[0], self.t, cuda_enabled, cpu_enabled)
    return wrap_and_track(outs[0])
}

Tensor._autocast_to_reduced_precision :: proc(self: *Tensor, cuda_enabled: bool, cpu_enabled: bool, cuda_dtype: i32, cpu_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__autocast_to_reduced_precision(&outs[0], self.t, cuda_enabled, cpu_enabled, cuda_dtype, cpu_dtype)
    return wrap_and_track(outs[0])
}

Tensor._batch_norm_no_update :: proc(self: *Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, momentum: f64, eps: f64) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg__batch_norm_no_update(&outs[0], self.t, weight.t, bias.t, running_mean.t, running_var.t, momentum, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor._batch_norm_no_update_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, out3: Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, momentum: f64, eps: f64) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg__batch_norm_no_update_out(&outs[0], out0.t, out1.t, out2.t, out3.t, self.t, weight.t, bias.t, running_mean.t, running_var.t, momentum, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor._batch_norm_with_update :: proc(self: *Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, momentum: f64, eps: f64) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg__batch_norm_with_update(&outs[0], self.t, weight.t, bias.t, running_mean.t, running_var.t, momentum, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor._batch_norm_with_update_functional :: proc(self: *Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, momentum: f64, eps: f64) (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor) {
    outs: [6] torch_bindings.Tensor = undefined
    torch_bindings.atg__batch_norm_with_update_functional(&outs[0], self.t, weight.t, bias.t, running_mean.t, running_var.t, momentum, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]), wrap_and_track(outs[4]), wrap_and_track(outs[5]))
}

Tensor._batch_norm_with_update_out :: proc(self: *Tensor, out: Tensor, save_mean: Tensor, save_invstd: Tensor, reserve: Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, momentum: f64, eps: f64) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg__batch_norm_with_update_out(&outs[0], out.t, save_mean.t, save_invstd.t, reserve.t, self.t, weight.t, bias.t, running_mean.t, running_var.t, momentum, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor._cast_byte :: proc(self: *Tensor, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cast_byte(&outs[0], self.t, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor._cast_char :: proc(self: *Tensor, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cast_char(&outs[0], self.t, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor._cast_double :: proc(self: *Tensor, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cast_double(&outs[0], self.t, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor._cast_float :: proc(self: *Tensor, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cast_float(&outs[0], self.t, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor._cast_half :: proc(self: *Tensor, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cast_half(&outs[0], self.t, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor._cast_int :: proc(self: *Tensor, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cast_int(&outs[0], self.t, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor._cast_long :: proc(self: *Tensor, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cast_long(&outs[0], self.t, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor._cast_short :: proc(self: *Tensor, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cast_short(&outs[0], self.t, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor._cdist_backward :: proc(grad: Tensor, x1: Tensor, x2: Tensor, p: f64, cdist: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cdist_backward(&outs[0], grad.t, x1.t, x2.t, p, cdist.t)
    return wrap_and_track(outs[0])
}

Tensor._cdist_backward_out :: proc(out: Tensor, grad: Tensor, x1: Tensor, x2: Tensor, p: f64, cdist: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cdist_backward_out(&outs[0], out.t, grad.t, x1.t, x2.t, p, cdist.t)
    return wrap_and_track(outs[0])
}

Tensor._cholesky_solve_helper :: proc(self: *Tensor, A: Tensor, upper: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cholesky_solve_helper(&outs[0], self.t, A.t, upper)
    return wrap_and_track(outs[0])
}

Tensor._cholesky_solve_helper_out :: proc(self: *Tensor, out: Tensor, A: Tensor, upper: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cholesky_solve_helper_out(&outs[0], out.t, self.t, A.t, upper)
    return wrap_and_track(outs[0])
}

Tensor._chunk_cat :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32, dim: i64, num_chunks: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__chunk_cat(&outs[0], tensors_data, tensors_len, dim, num_chunks)
    return wrap_and_track(outs[0])
}

Tensor._chunk_cat_out :: proc(out: Tensor, tensors_data: *torch_bindings.Tensor, tensors_len: i32, dim: i64, num_chunks: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__chunk_cat_out(&outs[0], out.t, tensors_data, tensors_len, dim, num_chunks)
    return wrap_and_track(outs[0])
}

Tensor._coalesce :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__coalesce(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._coalesce_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__coalesce_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._coalesced :: proc(self: *Tensor, coalesced: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__coalesced(&outs[0], self.t, coalesced)
    return wrap_and_track(outs[0])
}

Tensor._coalesced_ :: proc(self: *Tensor, coalesced: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__coalesced_(&outs[0], self.t, coalesced)
    return wrap_and_track(outs[0])
}

Tensor._coalesced_out :: proc(self: *Tensor, out: Tensor, coalesced: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__coalesced_out(&outs[0], out.t, self.t, coalesced)
    return wrap_and_track(outs[0])
}

Tensor._compute_linear_combination :: proc(self: *Tensor, coefficients: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__compute_linear_combination(&outs[0], self.t, coefficients.t)
    return wrap_and_track(outs[0])
}

Tensor._compute_linear_combination_out :: proc(self: *Tensor, out: Tensor, coefficients: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__compute_linear_combination_out(&outs[0], out.t, self.t, coefficients.t)
    return wrap_and_track(outs[0])
}

Tensor._conj :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__conj(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._conj_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__conj_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._conj_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__conj_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._conj_physical :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__conj_physical(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._conj_physical_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__conj_physical_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._conv_depthwise2d :: proc(self: *Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__conv_depthwise2d(&outs[0], self.t, weight.t, kernel_size_data, kernel_size_len, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor._conv_depthwise2d_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__conv_depthwise2d_out(&outs[0], out.t, self.t, weight.t, kernel_size_data, kernel_size_len, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor._convert_indices_from_coo_to_csr :: proc(self: *Tensor, size: i64, out_int32: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__convert_indices_from_coo_to_csr(&outs[0], self.t, size, out_int32)
    return wrap_and_track(outs[0])
}

Tensor._convert_indices_from_coo_to_csr_out :: proc(self: *Tensor, out: Tensor, size: i64, out_int32: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__convert_indices_from_coo_to_csr_out(&outs[0], out.t, self.t, size, out_int32)
    return wrap_and_track(outs[0])
}

Tensor._convert_indices_from_csr_to_coo :: proc(crow_indices: Tensor, col_indices: Tensor, out_int32: bool, transpose: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__convert_indices_from_csr_to_coo(&outs[0], crow_indices.t, col_indices.t, out_int32, transpose)
    return wrap_and_track(outs[0])
}

Tensor._convert_indices_from_csr_to_coo_out :: proc(out: Tensor, crow_indices: Tensor, col_indices: Tensor, out_int32: bool, transpose: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__convert_indices_from_csr_to_coo_out(&outs[0], out.t, crow_indices.t, col_indices.t, out_int32, transpose)
    return wrap_and_track(outs[0])
}

Tensor._convert_weight_to_int4pack :: proc(self: *Tensor, innerKTiles: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__convert_weight_to_int4pack(&outs[0], self.t, innerKTiles)
    return wrap_and_track(outs[0])
}

Tensor._convert_weight_to_int4pack_for_cpu :: proc(self: *Tensor, innerKTiles: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__convert_weight_to_int4pack_for_cpu(&outs[0], self.t, innerKTiles)
    return wrap_and_track(outs[0])
}

Tensor._convolution :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, transposed: bool, output_padding_data: *i64, output_padding_len: i32, groups: i64, benchmark: bool, deterministic: bool, cudnn_enabled: bool, allow_tf32: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__convolution(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, transposed, output_padding_data, output_padding_len, groups, benchmark, deterministic, cudnn_enabled, allow_tf32)
    return wrap_and_track(outs[0])
}

Tensor._convolution_deprecated :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, transposed: bool, output_padding_data: *i64, output_padding_len: i32, groups: i64, benchmark: bool, deterministic: bool, cudnn_enabled: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__convolution_deprecated(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, transposed, output_padding_data, output_padding_len, groups, benchmark, deterministic, cudnn_enabled)
    return wrap_and_track(outs[0])
}

Tensor._convolution_mode :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_ptr: *u8, padding_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__convolution_mode(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_ptr, padding_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor._convolution_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, transposed: bool, output_padding_data: *i64, output_padding_len: i32, groups: i64, benchmark: bool, deterministic: bool, cudnn_enabled: bool, allow_tf32: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__convolution_out(&outs[0], out.t, self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, transposed, output_padding_data, output_padding_len, groups, benchmark, deterministic, cudnn_enabled, allow_tf32)
    return wrap_and_track(outs[0])
}

Tensor._copy_from :: proc(self: *Tensor, dst: Tensor, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__copy_from(&outs[0], self.t, dst.t, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor._copy_from_and_resize :: proc(self: *Tensor, dst: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__copy_from_and_resize(&outs[0], self.t, dst.t)
    return wrap_and_track(outs[0])
}

Tensor._copy_from_and_resize_out :: proc(self: *Tensor, out: Tensor, dst: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__copy_from_and_resize_out(&outs[0], out.t, self.t, dst.t)
    return wrap_and_track(outs[0])
}

Tensor._copy_from_out :: proc(self: *Tensor, out: Tensor, dst: Tensor, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__copy_from_out(&outs[0], out.t, self.t, dst.t, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor._cslt_compress :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cslt_compress(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._cslt_sparse_mm :: proc(compressed_A: Tensor, dense_B: Tensor, bias: Tensor, alpha: Tensor, out_dtype: i32, transpose_result: bool, alg_id: i64, split_k: i64, split_k_mode: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cslt_sparse_mm(&outs[0], compressed_A.t, dense_B.t, bias.t, alpha.t, out_dtype, transpose_result, alg_id, split_k, split_k_mode)
    return wrap_and_track(outs[0])
}

Tensor._cslt_sparse_mm_search :: proc(compressed_A: Tensor, dense_B: Tensor, bias: Tensor, alpha: Tensor, out_dtype: i32, transpose_result: bool) i64 {
    return torch_bindings.atg__cslt_sparse_mm_search(compressed_A.t, dense_B.t, bias.t, alpha.t, out_dtype, transpose_result)
}

Tensor._ctc_loss :: proc(log_probs: Tensor, targets: Tensor, input_lengths_data: *i64, input_lengths_len: i32, target_lengths_data: *i64, target_lengths_len: i32, blank: i64, zero_infinity: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__ctc_loss(&outs[0], log_probs.t, targets.t, input_lengths_data, input_lengths_len, target_lengths_data, target_lengths_len, blank, zero_infinity)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._ctc_loss_backward :: proc(grad: Tensor, log_probs: Tensor, targets: Tensor, input_lengths_data: *i64, input_lengths_len: i32, target_lengths_data: *i64, target_lengths_len: i32, neg_log_likelihood: Tensor, log_alpha: Tensor, blank: i64, zero_infinity: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__ctc_loss_backward(&outs[0], grad.t, log_probs.t, targets.t, input_lengths_data, input_lengths_len, target_lengths_data, target_lengths_len, neg_log_likelihood.t, log_alpha.t, blank, zero_infinity)
    return wrap_and_track(outs[0])
}

Tensor._ctc_loss_backward_out :: proc(out: Tensor, grad: Tensor, log_probs: Tensor, targets: Tensor, input_lengths_data: *i64, input_lengths_len: i32, target_lengths_data: *i64, target_lengths_len: i32, neg_log_likelihood: Tensor, log_alpha: Tensor, blank: i64, zero_infinity: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__ctc_loss_backward_out(&outs[0], out.t, grad.t, log_probs.t, targets.t, input_lengths_data, input_lengths_len, target_lengths_data, target_lengths_len, neg_log_likelihood.t, log_alpha.t, blank, zero_infinity)
    return wrap_and_track(outs[0])
}

Tensor._ctc_loss_backward_tensor :: proc(grad: Tensor, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor, neg_log_likelihood: Tensor, log_alpha: Tensor, blank: i64, zero_infinity: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__ctc_loss_backward_tensor(&outs[0], grad.t, log_probs.t, targets.t, input_lengths.t, target_lengths.t, neg_log_likelihood.t, log_alpha.t, blank, zero_infinity)
    return wrap_and_track(outs[0])
}

Tensor._ctc_loss_out :: proc(out0: Tensor, out1: Tensor, log_probs: Tensor, targets: Tensor, input_lengths_data: *i64, input_lengths_len: i32, target_lengths_data: *i64, target_lengths_len: i32, blank: i64, zero_infinity: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__ctc_loss_out(&outs[0], out0.t, out1.t, log_probs.t, targets.t, input_lengths_data, input_lengths_len, target_lengths_data, target_lengths_len, blank, zero_infinity)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._ctc_loss_tensor :: proc(log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor, blank: i64, zero_infinity: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__ctc_loss_tensor(&outs[0], log_probs.t, targets.t, input_lengths.t, target_lengths.t, blank, zero_infinity)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._ctc_loss_tensor_out :: proc(out0: Tensor, out1: Tensor, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor, blank: i64, zero_infinity: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__ctc_loss_tensor_out(&outs[0], out0.t, out1.t, log_probs.t, targets.t, input_lengths.t, target_lengths.t, blank, zero_infinity)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._cudnn_attention_backward :: proc(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, philox_seed: Tensor, philox_offset: Tensor, attn_bias: Tensor, cum_seq_q: Tensor, cum_seq_k: Tensor, max_q: i64, max_k: i64, dropout_p: f64, is_causal: bool, scale_v: f64, scale_null: u8) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__cudnn_attention_backward(&outs[0], grad_out.t, query.t, key.t, value.t, out.t, logsumexp.t, philox_seed.t, philox_offset.t, attn_bias.t, cum_seq_q.t, cum_seq_k.t, max_q, max_k, dropout_p, is_causal, scale_v, scale_null)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._cudnn_ctc_loss :: proc(log_probs: Tensor, targets: Tensor, input_lengths_data: *i64, input_lengths_len: i32, target_lengths_data: *i64, target_lengths_len: i32, blank: i64, deterministic: bool, zero_infinity: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__cudnn_ctc_loss(&outs[0], log_probs.t, targets.t, input_lengths_data, input_lengths_len, target_lengths_data, target_lengths_len, blank, deterministic, zero_infinity)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._cudnn_ctc_loss_out :: proc(out0: Tensor, out1: Tensor, log_probs: Tensor, targets: Tensor, input_lengths_data: *i64, input_lengths_len: i32, target_lengths_data: *i64, target_lengths_len: i32, blank: i64, deterministic: bool, zero_infinity: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__cudnn_ctc_loss_out(&outs[0], out0.t, out1.t, log_probs.t, targets.t, input_lengths_data, input_lengths_len, target_lengths_data, target_lengths_len, blank, deterministic, zero_infinity)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._cudnn_ctc_loss_tensor :: proc(log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor, blank: i64, deterministic: bool, zero_infinity: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__cudnn_ctc_loss_tensor(&outs[0], log_probs.t, targets.t, input_lengths.t, target_lengths.t, blank, deterministic, zero_infinity)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._cudnn_init_dropout_state :: proc(dropout: f64, train: bool, dropout_seed: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cudnn_init_dropout_state(&outs[0], dropout, train, dropout_seed, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor._cudnn_init_dropout_state_out :: proc(out: Tensor, dropout: f64, train: bool, dropout_seed: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cudnn_init_dropout_state_out(&outs[0], out.t, dropout, train, dropout_seed)
    return wrap_and_track(outs[0])
}

Tensor._cudnn_rnn :: proc(self: *Tensor, weight_data: *torch_bindings.Tensor, weight_len: i32, weight_stride0: i64, weight_buf: Tensor, hx: Tensor, cx: Tensor, mode: i64, hidden_size: i64, proj_size: i64, num_layers: i64, batch_first: bool, dropout: f64, train: bool, bidirectional: bool, batch_sizes_data: *i64, batch_sizes_len: i32, dropout_state: Tensor) (Tensor, Tensor, Tensor, Tensor, Tensor) {
    outs: [5] torch_bindings.Tensor = undefined
    torch_bindings.atg__cudnn_rnn(&outs[0], self.t, weight_data, weight_len, weight_stride0, weight_buf.t, hx.t, cx.t, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes_data, batch_sizes_len, dropout_state.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]), wrap_and_track(outs[4]))
}

Tensor._cudnn_rnn_flatten_weight :: proc(weight_arr_data: *torch_bindings.Tensor, weight_arr_len: i32, weight_stride0: i64, input_size: i64, mode: i64, hidden_size: i64, proj_size: i64, num_layers: i64, batch_first: bool, bidirectional: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cudnn_rnn_flatten_weight(&outs[0], weight_arr_data, weight_arr_len, weight_stride0, input_size, mode, hidden_size, proj_size, num_layers, batch_first, bidirectional)
    return wrap_and_track(outs[0])
}

Tensor._cudnn_rnn_flatten_weight_out :: proc(out: Tensor, weight_arr_data: *torch_bindings.Tensor, weight_arr_len: i32, weight_stride0: i64, input_size: i64, mode: i64, hidden_size: i64, proj_size: i64, num_layers: i64, batch_first: bool, bidirectional: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__cudnn_rnn_flatten_weight_out(&outs[0], out.t, weight_arr_data, weight_arr_len, weight_stride0, input_size, mode, hidden_size, proj_size, num_layers, batch_first, bidirectional)
    return wrap_and_track(outs[0])
}

Tensor._cudnn_rnn_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, out3: Tensor, out4: Tensor, weight_data: *torch_bindings.Tensor, weight_len: i32, weight_stride0: i64, weight_buf: Tensor, hx: Tensor, cx: Tensor, mode: i64, hidden_size: i64, proj_size: i64, num_layers: i64, batch_first: bool, dropout: f64, train: bool, bidirectional: bool, batch_sizes_data: *i64, batch_sizes_len: i32, dropout_state: Tensor) (Tensor, Tensor, Tensor, Tensor, Tensor) {
    outs: [5] torch_bindings.Tensor = undefined
    torch_bindings.atg__cudnn_rnn_out(&outs[0], out0.t, out1.t, out2.t, out3.t, out4.t, self.t, weight_data, weight_len, weight_stride0, weight_buf.t, hx.t, cx.t, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes_data, batch_sizes_len, dropout_state.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]), wrap_and_track(outs[4]))
}

Tensor._debug_has_internal_overlap :: proc(self: *Tensor) i64 {
    return torch_bindings.atg__debug_has_internal_overlap(self.t)
}

Tensor._dim_arange :: proc(like: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__dim_arange(&outs[0], like.t, dim)
    return wrap_and_track(outs[0])
}

Tensor._dimi :: proc(self: *Tensor) i64 {
    return torch_bindings.atg__dimi(self.t)
}

Tensor._dimv :: proc(self: *Tensor) i64 {
    return torch_bindings.atg__dimv(self.t)
}

Tensor._dirichlet_grad :: proc(x: Tensor, alpha: Tensor, total: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__dirichlet_grad(&outs[0], x.t, alpha.t, total.t)
    return wrap_and_track(outs[0])
}

Tensor._dirichlet_grad_out :: proc(out: Tensor, x: Tensor, alpha: Tensor, total: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__dirichlet_grad_out(&outs[0], out.t, x.t, alpha.t, total.t)
    return wrap_and_track(outs[0])
}

Tensor._dyn_quant_matmul_4bit :: proc(inp: Tensor, packed_weights: Tensor, block_size: i64, in_features: i64, out_features: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__dyn_quant_matmul_4bit(&outs[0], inp.t, packed_weights.t, block_size, in_features, out_features)
    return wrap_and_track(outs[0])
}

Tensor._dyn_quant_pack_4bit_weight :: proc(weights: Tensor, scales_zeros: Tensor, bias: Tensor, block_size: i64, in_features: i64, out_features: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__dyn_quant_pack_4bit_weight(&outs[0], weights.t, scales_zeros.t, bias.t, block_size, in_features, out_features)
    return wrap_and_track(outs[0])
}

Tensor._efficient_attention_backward :: proc(grad_out_: Tensor, query: Tensor, key: Tensor, value: Tensor, bias: Tensor, out: Tensor, cu_seqlens_q: Tensor, cu_seqlens_k: Tensor, max_seqlen_q: i64, max_seqlen_k: i64, logsumexp: Tensor, dropout_p: f64, philox_seed: Tensor, philox_offset: Tensor, custom_mask_type: i64, bias_requires_grad: bool, scale_v: f64, scale_null: u8, num_splits_key_v: i64, num_splits_key_null: u8, window_size_v: i64, window_size_null: u8, shared_storage_dqdkdv: bool) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg__efficient_attention_backward(&outs[0], grad_out_.t, query.t, key.t, value.t, bias.t, out.t, cu_seqlens_q.t, cu_seqlens_k.t, max_seqlen_q, max_seqlen_k, logsumexp.t, dropout_p, philox_seed.t, philox_offset.t, custom_mask_type, bias_requires_grad, scale_v, scale_null, num_splits_key_v, num_splits_key_null, window_size_v, window_size_null, shared_storage_dqdkdv)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor._efficientzerotensor :: proc(size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__efficientzerotensor(&outs[0], size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor._efficientzerotensor_out :: proc(out: Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__efficientzerotensor_out(&outs[0], out.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor._embedding_bag :: proc(weight: Tensor, indices: Tensor, offsets: Tensor, scale_grad_by_freq: bool, mode: i64, sparse: bool, per_sample_weights: Tensor, include_last_offset: bool, padding_idx: i64) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg__embedding_bag(&outs[0], weight.t, indices.t, offsets.t, scale_grad_by_freq, mode, sparse, per_sample_weights.t, include_last_offset, padding_idx)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor._embedding_bag_backward :: proc(grad: Tensor, indices: Tensor, offsets: Tensor, offset2bag: Tensor, bag_size: Tensor, maximum_indices: Tensor, num_weights: i64, scale_grad_by_freq: bool, mode: i64, sparse: bool, per_sample_weights: Tensor, padding_idx: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__embedding_bag_backward(&outs[0], grad.t, indices.t, offsets.t, offset2bag.t, bag_size.t, maximum_indices.t, num_weights, scale_grad_by_freq, mode, sparse, per_sample_weights.t, padding_idx)
    return wrap_and_track(outs[0])
}

Tensor._embedding_bag_dense_backward :: proc(grad: Tensor, indices: Tensor, offset2bag: Tensor, bag_size: Tensor, maximum_indices: Tensor, num_weights: i64, scale_grad_by_freq: bool, mode: i64, per_sample_weights: Tensor, padding_idx: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__embedding_bag_dense_backward(&outs[0], grad.t, indices.t, offset2bag.t, bag_size.t, maximum_indices.t, num_weights, scale_grad_by_freq, mode, per_sample_weights.t, padding_idx)
    return wrap_and_track(outs[0])
}

Tensor._embedding_bag_dense_backward_out :: proc(out: Tensor, grad: Tensor, indices: Tensor, offset2bag: Tensor, bag_size: Tensor, maximum_indices: Tensor, num_weights: i64, scale_grad_by_freq: bool, mode: i64, per_sample_weights: Tensor, padding_idx: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__embedding_bag_dense_backward_out(&outs[0], out.t, grad.t, indices.t, offset2bag.t, bag_size.t, maximum_indices.t, num_weights, scale_grad_by_freq, mode, per_sample_weights.t, padding_idx)
    return wrap_and_track(outs[0])
}

Tensor._embedding_bag_forward_only :: proc(weight: Tensor, indices: Tensor, offsets: Tensor, scale_grad_by_freq: bool, mode: i64, sparse: bool, per_sample_weights: Tensor, include_last_offset: bool, padding_idx: i64) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg__embedding_bag_forward_only(&outs[0], weight.t, indices.t, offsets.t, scale_grad_by_freq, mode, sparse, per_sample_weights.t, include_last_offset, padding_idx)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor._embedding_bag_forward_only_out :: proc(out0: Tensor, out1: Tensor, out2: Tensor, out3: Tensor, weight: Tensor, indices: Tensor, offsets: Tensor, scale_grad_by_freq: bool, mode: i64, sparse: bool, per_sample_weights: Tensor, include_last_offset: bool, padding_idx: i64) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg__embedding_bag_forward_only_out(&outs[0], out0.t, out1.t, out2.t, out3.t, weight.t, indices.t, offsets.t, scale_grad_by_freq, mode, sparse, per_sample_weights.t, include_last_offset, padding_idx)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor._embedding_bag_out :: proc(out0: Tensor, out1: Tensor, out2: Tensor, out3: Tensor, weight: Tensor, indices: Tensor, offsets: Tensor, scale_grad_by_freq: bool, mode: i64, sparse: bool, per_sample_weights: Tensor, include_last_offset: bool, padding_idx: i64) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg__embedding_bag_out(&outs[0], out0.t, out1.t, out2.t, out3.t, weight.t, indices.t, offsets.t, scale_grad_by_freq, mode, sparse, per_sample_weights.t, include_last_offset, padding_idx)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor._embedding_bag_per_sample_weights_backward :: proc(grad: Tensor, weight: Tensor, indices: Tensor, offsets: Tensor, offset2bag: Tensor, mode: i64, padding_idx: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__embedding_bag_per_sample_weights_backward(&outs[0], grad.t, weight.t, indices.t, offsets.t, offset2bag.t, mode, padding_idx)
    return wrap_and_track(outs[0])
}

Tensor._embedding_bag_per_sample_weights_backward_out :: proc(out: Tensor, grad: Tensor, weight: Tensor, indices: Tensor, offsets: Tensor, offset2bag: Tensor, mode: i64, padding_idx: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__embedding_bag_per_sample_weights_backward_out(&outs[0], out.t, grad.t, weight.t, indices.t, offsets.t, offset2bag.t, mode, padding_idx)
    return wrap_and_track(outs[0])
}

Tensor._embedding_bag_sparse_backward :: proc(grad: Tensor, indices: Tensor, offsets: Tensor, offset2bag: Tensor, bag_size: Tensor, num_weights: i64, scale_grad_by_freq: bool, mode: i64, per_sample_weights: Tensor, padding_idx: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__embedding_bag_sparse_backward(&outs[0], grad.t, indices.t, offsets.t, offset2bag.t, bag_size.t, num_weights, scale_grad_by_freq, mode, per_sample_weights.t, padding_idx)
    return wrap_and_track(outs[0])
}

Tensor._empty_affine_quantized :: proc(size_data: *i64, size_len: i32, options_kind: i32, options_device: i32, scale: f64, zero_point: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__empty_affine_quantized(&outs[0], size_data, size_len, options_kind, options_device, scale, zero_point)
    return wrap_and_track(outs[0])
}

Tensor._empty_affine_quantized_out :: proc(out: Tensor, size_data: *i64, size_len: i32, scale: f64, zero_point: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__empty_affine_quantized_out(&outs[0], out.t, size_data, size_len, scale, zero_point)
    return wrap_and_track(outs[0])
}

Tensor._empty_per_channel_affine_quantized :: proc(size_data: *i64, size_len: i32, scales: Tensor, zero_points: Tensor, axis: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__empty_per_channel_affine_quantized(&outs[0], size_data, size_len, scales.t, zero_points.t, axis, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor._empty_per_channel_affine_quantized_out :: proc(out: Tensor, size_data: *i64, size_len: i32, scales: Tensor, zero_points: Tensor, axis: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__empty_per_channel_affine_quantized_out(&outs[0], out.t, size_data, size_len, scales.t, zero_points.t, axis)
    return wrap_and_track(outs[0])
}

Tensor._euclidean_dist :: proc(x1: Tensor, x2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__euclidean_dist(&outs[0], x1.t, x2.t)
    return wrap_and_track(outs[0])
}

Tensor._euclidean_dist_out :: proc(out: Tensor, x1: Tensor, x2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__euclidean_dist_out(&outs[0], out.t, x1.t, x2.t)
    return wrap_and_track(outs[0])
}

Tensor._fake_quantize_learnable_per_channel_affine :: proc(self: *Tensor, scale: Tensor, zero_point: Tensor, axis: i64, quant_min: i64, quant_max: i64, grad_factor: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__fake_quantize_learnable_per_channel_affine(&outs[0], self.t, scale.t, zero_point.t, axis, quant_min, quant_max, grad_factor)
    return wrap_and_track(outs[0])
}

Tensor._fake_quantize_learnable_per_channel_affine_backward :: proc(self: *Tensor, grad: Tensor, scale: Tensor, zero_point: Tensor, axis: i64, quant_min: i64, quant_max: i64, grad_factor: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__fake_quantize_learnable_per_channel_affine_backward(&outs[0], grad.t, self.t, scale.t, zero_point.t, axis, quant_min, quant_max, grad_factor)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._fake_quantize_learnable_per_channel_affine_out :: proc(self: *Tensor, out: Tensor, scale: Tensor, zero_point: Tensor, axis: i64, quant_min: i64, quant_max: i64, grad_factor: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__fake_quantize_learnable_per_channel_affine_out(&outs[0], out.t, self.t, scale.t, zero_point.t, axis, quant_min, quant_max, grad_factor)
    return wrap_and_track(outs[0])
}

Tensor._fake_quantize_learnable_per_tensor_affine :: proc(self: *Tensor, scale: Tensor, zero_point: Tensor, quant_min: i64, quant_max: i64, grad_factor: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__fake_quantize_learnable_per_tensor_affine(&outs[0], self.t, scale.t, zero_point.t, quant_min, quant_max, grad_factor)
    return wrap_and_track(outs[0])
}

Tensor._fake_quantize_learnable_per_tensor_affine_backward :: proc(self: *Tensor, grad: Tensor, scale: Tensor, zero_point: Tensor, quant_min: i64, quant_max: i64, grad_factor: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__fake_quantize_learnable_per_tensor_affine_backward(&outs[0], grad.t, self.t, scale.t, zero_point.t, quant_min, quant_max, grad_factor)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._fake_quantize_learnable_per_tensor_affine_out :: proc(self: *Tensor, out: Tensor, scale: Tensor, zero_point: Tensor, quant_min: i64, quant_max: i64, grad_factor: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__fake_quantize_learnable_per_tensor_affine_out(&outs[0], out.t, self.t, scale.t, zero_point.t, quant_min, quant_max, grad_factor)
    return wrap_and_track(outs[0])
}

Tensor._fake_quantize_per_tensor_affine_cachemask_tensor_qparams :: proc(self: *Tensor, scale: Tensor, zero_point: Tensor, fake_quant_enabled: Tensor, quant_min: i64, quant_max: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__fake_quantize_per_tensor_affine_cachemask_tensor_qparams(&outs[0], self.t, scale.t, zero_point.t, fake_quant_enabled.t, quant_min, quant_max)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._fake_quantize_per_tensor_affine_cachemask_tensor_qparams_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, scale: Tensor, zero_point: Tensor, fake_quant_enabled: Tensor, quant_min: i64, quant_max: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__fake_quantize_per_tensor_affine_cachemask_tensor_qparams_out(&outs[0], out0.t, out1.t, self.t, scale.t, zero_point.t, fake_quant_enabled.t, quant_min, quant_max)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._fft_c2c :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, normalization: i64, forward: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__fft_c2c(&outs[0], self.t, dim_data, dim_len, normalization, forward)
    return wrap_and_track(outs[0])
}

Tensor._fft_c2c_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, normalization: i64, forward: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__fft_c2c_out(&outs[0], out.t, self.t, dim_data, dim_len, normalization, forward)
    return wrap_and_track(outs[0])
}

Tensor._fft_c2r :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, normalization: i64, last_dim_size: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__fft_c2r(&outs[0], self.t, dim_data, dim_len, normalization, last_dim_size)
    return wrap_and_track(outs[0])
}

Tensor._fft_c2r_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, normalization: i64, last_dim_size: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__fft_c2r_out(&outs[0], out.t, self.t, dim_data, dim_len, normalization, last_dim_size)
    return wrap_and_track(outs[0])
}

Tensor._fft_r2c :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, normalization: i64, onesided: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__fft_r2c(&outs[0], self.t, dim_data, dim_len, normalization, onesided)
    return wrap_and_track(outs[0])
}

Tensor._fft_r2c_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, normalization: i64, onesided: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__fft_r2c_out(&outs[0], out.t, self.t, dim_data, dim_len, normalization, onesided)
    return wrap_and_track(outs[0])
}

Tensor._fill_mem_eff_dropout_mask_ :: proc(self: *Tensor, dropout_p: f64, seed: i64, offset: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__fill_mem_eff_dropout_mask_(&outs[0], self.t, dropout_p, seed, offset)
    return wrap_and_track(outs[0])
}

Tensor._flash_attention_backward :: proc(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, cum_seq_q: Tensor, cum_seq_k: Tensor, max_q: i64, max_k: i64, dropout_p: f64, is_causal: bool, rng_state: Tensor, unused: Tensor, scale_v: f64, scale_null: u8, window_size_left_v: i64, window_size_left_null: u8, window_size_right_v: i64, window_size_right_null: u8) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__flash_attention_backward(&outs[0], grad_out.t, query.t, key.t, value.t, out.t, logsumexp.t, cum_seq_q.t, cum_seq_k.t, max_q, max_k, dropout_p, is_causal, rng_state.t, unused.t, scale_v, scale_null, window_size_left_v, window_size_left_null, window_size_right_v, window_size_right_null)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._foobar :: proc(self: *Tensor, arg1: bool, arg2: bool, arg3: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__foobar(&outs[0], self.t, arg1, arg2, arg3)
    return wrap_and_track(outs[0])
}

Tensor._foobar_out :: proc(self: *Tensor, out: Tensor, arg1: bool, arg2: bool, arg3: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__foobar_out(&outs[0], out.t, self.t, arg1, arg2, arg3)
    return wrap_and_track(outs[0])
}

Tensor._functional_assert_async :: proc(self: *Tensor, assert_msg_ptr: *u8, assert_msg_len: i32, dep_token: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__functional_assert_async(&outs[0], self.t, assert_msg_ptr, assert_msg_len, dep_token.t)
    return wrap_and_track(outs[0])
}

Tensor._functional_assert_scalar :: proc(self_scalar: torch_bindings.Scalar, assert_msg_ptr: *u8, assert_msg_len: i32, dep_token: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__functional_assert_scalar(&outs[0], self_scalar, assert_msg_ptr, assert_msg_len, dep_token.t)
    return wrap_and_track(outs[0])
}

Tensor._functional_sym_constrain_range :: proc(size: torch_bindings.Scalar, min_v: i64, min_null: u8, max_v: i64, max_null: u8, dep_token: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__functional_sym_constrain_range(&outs[0], size, min_v, min_null, max_v, max_null, dep_token.t)
    return wrap_and_track(outs[0])
}

Tensor._functional_sym_constrain_range_for_size :: proc(size: torch_bindings.Scalar, min_v: i64, min_null: u8, max_v: i64, max_null: u8, dep_token: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__functional_sym_constrain_range_for_size(&outs[0], size, min_v, min_null, max_v, max_null, dep_token.t)
    return wrap_and_track(outs[0])
}

Tensor._fused_dropout :: proc(self: *Tensor, p: f64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__fused_dropout(&outs[0], self.t, p)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._fused_dropout_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, p: f64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__fused_dropout_out(&outs[0], out0.t, out1.t, self.t, p)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._fused_moving_avg_obs_fq_helper :: proc(self: *Tensor, observer_on: Tensor, fake_quant_on: Tensor, running_min: Tensor, running_max: Tensor, scale: Tensor, zero_point: Tensor, averaging_const: f64, quant_min: i64, quant_max: i64, ch_axis: i64, per_row_fake_quant: bool, symmetric_quant: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__fused_moving_avg_obs_fq_helper(&outs[0], self.t, observer_on.t, fake_quant_on.t, running_min.t, running_max.t, scale.t, zero_point.t, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._fused_moving_avg_obs_fq_helper_functional :: proc(self: *Tensor, observer_on: Tensor, fake_quant_on: Tensor, running_min: Tensor, running_max: Tensor, scale: Tensor, zero_point: Tensor, averaging_const: f64, quant_min: i64, quant_max: i64, ch_axis: i64, per_row_fake_quant: bool, symmetric_quant: bool) (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor) {
    outs: [6] torch_bindings.Tensor = undefined
    torch_bindings.atg__fused_moving_avg_obs_fq_helper_functional(&outs[0], self.t, observer_on.t, fake_quant_on.t, running_min.t, running_max.t, scale.t, zero_point.t, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]), wrap_and_track(outs[4]), wrap_and_track(outs[5]))
}

Tensor._fused_moving_avg_obs_fq_helper_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, observer_on: Tensor, fake_quant_on: Tensor, running_min: Tensor, running_max: Tensor, scale: Tensor, zero_point: Tensor, averaging_const: f64, quant_min: i64, quant_max: i64, ch_axis: i64, per_row_fake_quant: bool, symmetric_quant: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__fused_moving_avg_obs_fq_helper_out(&outs[0], out0.t, out1.t, self.t, observer_on.t, fake_quant_on.t, running_min.t, running_max.t, scale.t, zero_point.t, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._fused_rms_norm :: proc(self: *Tensor, normalized_shape_data: *i64, normalized_shape_len: i32, weight: Tensor, eps_v: f64, eps_null: u8) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__fused_rms_norm(&outs[0], self.t, normalized_shape_data, normalized_shape_len, weight.t, eps_v, eps_null)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._fused_sdp_choice :: proc(query: Tensor, key: Tensor, value: Tensor, attn_mask: Tensor, dropout_p: f64, is_causal: bool, scale_v: f64, scale_null: u8, enable_gqa: bool) i64 {
    return torch_bindings.atg__fused_sdp_choice(query.t, key.t, value.t, attn_mask.t, dropout_p, is_causal, scale_v, scale_null, enable_gqa)
}

Tensor._fw_primal :: proc(self: *Tensor, level: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__fw_primal(&outs[0], self.t, level)
    return wrap_and_track(outs[0])
}

Tensor._fw_primal_copy :: proc(self: *Tensor, level: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__fw_primal_copy(&outs[0], self.t, level)
    return wrap_and_track(outs[0])
}

Tensor._fw_primal_copy_out :: proc(self: *Tensor, out: Tensor, level: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__fw_primal_copy_out(&outs[0], out.t, self.t, level)
    return wrap_and_track(outs[0])
}

Tensor._gather_sparse_backward :: proc(self: *Tensor, dim: i64, index: Tensor, grad: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__gather_sparse_backward(&outs[0], self.t, dim, index.t, grad.t)
    return wrap_and_track(outs[0])
}

Tensor._grid_sampler_2d_cpu_fallback :: proc(self: *Tensor, grid: Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__grid_sampler_2d_cpu_fallback(&outs[0], self.t, grid.t, interpolation_mode, padding_mode, align_corners)
    return wrap_and_track(outs[0])
}

Tensor._grid_sampler_2d_cpu_fallback_backward :: proc(self: *Tensor, grad_output: Tensor, grid: Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__grid_sampler_2d_cpu_fallback_backward(&outs[0], grad_output.t, self.t, grid.t, interpolation_mode, padding_mode, align_corners)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._grid_sampler_2d_cpu_fallback_out :: proc(self: *Tensor, out: Tensor, grid: Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__grid_sampler_2d_cpu_fallback_out(&outs[0], out.t, self.t, grid.t, interpolation_mode, padding_mode, align_corners)
    return wrap_and_track(outs[0])
}

Tensor._grouped_mm :: proc(self: *Tensor, mat2: Tensor, offs: Tensor, bias: Tensor, out_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__grouped_mm(&outs[0], self.t, mat2.t, offs.t, bias.t, out_dtype)
    return wrap_and_track(outs[0])
}

Tensor._has_compatible_shallow_copy_type :: proc(self: *Tensor, from: Tensor) bool {
    return torch_bindings.atg__has_compatible_shallow_copy_type(self.t, from.t)
}

Tensor._has_same_storage_numel :: proc(self: *Tensor, other: Tensor) bool {
    return torch_bindings.atg__has_same_storage_numel(self.t, other.t)
}

Tensor._histogramdd_bin_edges :: proc(self: *Tensor, bins_data: *i64, bins_len: i32, range_data: *f64, range_len: i32, weight: Tensor, density: bool) [dyn] Tensor {
    ptr := torch_bindings.atg__histogramdd_bin_edges(self.t, bins_data, bins_len, range_data, range_len, weight.t, density)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor._histogramdd_bin_edges_out :: proc(self: *Tensor, out_data: *torch_bindings.Tensor, out_len: i32, bins_data: *i64, bins_len: i32, range_data: *f64, range_len: i32, weight: Tensor, density: bool) void {
    torch_bindings.atg__histogramdd_bin_edges_out(out_data, out_len, self.t, bins_data, bins_len, range_data, range_len, weight.t, density)
}

Tensor._histogramdd_from_bin_cts :: proc(self: *Tensor, bins_data: *i64, bins_len: i32, range_data: *f64, range_len: i32, weight: Tensor, density: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__histogramdd_from_bin_cts(&outs[0], self.t, bins_data, bins_len, range_data, range_len, weight.t, density)
    return wrap_and_track(outs[0])
}

Tensor._histogramdd_from_bin_cts_out :: proc(self: *Tensor, out: Tensor, bins_data: *i64, bins_len: i32, range_data: *f64, range_len: i32, weight: Tensor, density: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__histogramdd_from_bin_cts_out(&outs[0], out.t, self.t, bins_data, bins_len, range_data, range_len, weight.t, density)
    return wrap_and_track(outs[0])
}

Tensor._histogramdd_from_bin_tensors :: proc(self: *Tensor, bins_data: *torch_bindings.Tensor, bins_len: i32, weight: Tensor, density: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__histogramdd_from_bin_tensors(&outs[0], self.t, bins_data, bins_len, weight.t, density)
    return wrap_and_track(outs[0])
}

Tensor._histogramdd_from_bin_tensors_out :: proc(self: *Tensor, out: Tensor, bins_data: *torch_bindings.Tensor, bins_len: i32, weight: Tensor, density: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__histogramdd_from_bin_tensors_out(&outs[0], out.t, self.t, bins_data, bins_len, weight.t, density)
    return wrap_and_track(outs[0])
}

Tensor._index_put_impl :: proc(self: *Tensor, indices_data: *torch_bindings.Tensor, indices_len: i32, values: Tensor, accumulate: bool, unsafe: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__index_put_impl(&outs[0], self.t, indices_data, indices_len, values.t, accumulate, unsafe)
    return wrap_and_track(outs[0])
}

Tensor._index_put_impl_ :: proc(self: *Tensor, indices_data: *torch_bindings.Tensor, indices_len: i32, values: Tensor, accumulate: bool, unsafe: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__index_put_impl_(&outs[0], self.t, indices_data, indices_len, values.t, accumulate, unsafe)
    return wrap_and_track(outs[0])
}

Tensor._index_put_impl_out :: proc(self: *Tensor, out: Tensor, indices_data: *torch_bindings.Tensor, indices_len: i32, values: Tensor, accumulate: bool, unsafe: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__index_put_impl_out(&outs[0], out.t, self.t, indices_data, indices_len, values.t, accumulate, unsafe)
    return wrap_and_track(outs[0])
}

Tensor._indices :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__indices(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._indices_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__indices_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._indices_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__indices_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._int_mm :: proc(self: *Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__int_mm(&outs[0], self.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor._int_mm_out :: proc(self: *Tensor, out: Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__int_mm_out(&outs[0], out.t, self.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor._is_all_true :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__is_all_true(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._is_any_true :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__is_any_true(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._is_zerotensor :: proc(self: *Tensor) bool {
    return torch_bindings.atg__is_zerotensor(self.t)
}

Tensor._lazy_clone :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__lazy_clone(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._linalg_check_errors :: proc(info: Tensor, api_name_ptr: *u8, api_name_len: i32, is_matrix: bool) void {
    torch_bindings.atg__linalg_check_errors(info.t, api_name_ptr, api_name_len, is_matrix)
}

Tensor._linalg_det :: proc(A: Tensor) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__linalg_det(&outs[0], A.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._linalg_det_result :: proc(result: Tensor, LU: Tensor, pivots: Tensor, A: Tensor) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__linalg_det_result(&outs[0], result.t, LU.t, pivots.t, A.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._linalg_eigh :: proc(A: Tensor, UPLO_ptr: *u8, UPLO_len: i32, compute_v: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__linalg_eigh(&outs[0], A.t, UPLO_ptr, UPLO_len, compute_v)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._linalg_eigh_eigenvalues :: proc(eigenvalues: Tensor, eigenvectors: Tensor, A: Tensor, UPLO_ptr: *u8, UPLO_len: i32, compute_v: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__linalg_eigh_eigenvalues(&outs[0], eigenvalues.t, eigenvectors.t, A.t, UPLO_ptr, UPLO_len, compute_v)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._linalg_eigvals :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__linalg_eigvals(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._linalg_slogdet :: proc(A: Tensor) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg__linalg_slogdet(&outs[0], A.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor._linalg_slogdet_sign :: proc(sign: Tensor, logabsdet: Tensor, LU: Tensor, pivots: Tensor, A: Tensor) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg__linalg_slogdet_sign(&outs[0], sign.t, logabsdet.t, LU.t, pivots.t, A.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor._linalg_solve_ex :: proc(A: Tensor, B: Tensor, left: bool, check_errors: bool) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg__linalg_solve_ex(&outs[0], A.t, B.t, left, check_errors)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor._linalg_solve_ex_result :: proc(result: Tensor, LU: Tensor, pivots: Tensor, info: Tensor, A: Tensor, B: Tensor, left: bool, check_errors: bool) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg__linalg_solve_ex_result(&outs[0], result.t, LU.t, pivots.t, info.t, A.t, B.t, left, check_errors)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor._linalg_svd :: proc(A: Tensor, full_matrices: bool, compute_uv: bool, driver_ptr: *u8, driver_len: i32) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__linalg_svd(&outs[0], A.t, full_matrices, compute_uv, driver_ptr, driver_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._linalg_svd_u :: proc(U: Tensor, S: Tensor, Vh: Tensor, A: Tensor, full_matrices: bool, compute_uv: bool, driver_ptr: *u8, driver_len: i32) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__linalg_svd_u(&outs[0], U.t, S.t, Vh.t, A.t, full_matrices, compute_uv, driver_ptr, driver_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._log_softmax :: proc(self: *Tensor, dim: i64, half_to_float: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__log_softmax(&outs[0], self.t, dim, half_to_float)
    return wrap_and_track(outs[0])
}

Tensor._log_softmax_backward_data :: proc(grad_output: Tensor, output: Tensor, dim: i64, input_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__log_softmax_backward_data(&outs[0], grad_output.t, output.t, dim, input_dtype)
    return wrap_and_track(outs[0])
}

Tensor._log_softmax_backward_data_out :: proc(out: Tensor, grad_output: Tensor, output: Tensor, dim: i64, input_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__log_softmax_backward_data_out(&outs[0], out.t, grad_output.t, output.t, dim, input_dtype)
    return wrap_and_track(outs[0])
}

Tensor._log_softmax_out :: proc(self: *Tensor, out: Tensor, dim: i64, half_to_float: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__log_softmax_out(&outs[0], out.t, self.t, dim, half_to_float)
    return wrap_and_track(outs[0])
}

Tensor._logcumsumexp :: proc(self: *Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__logcumsumexp(&outs[0], self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor._logcumsumexp_out :: proc(self: *Tensor, out: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__logcumsumexp_out(&outs[0], out.t, self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor._lstm_mps :: proc(self: *Tensor, hx_data: *torch_bindings.Tensor, hx_len: i32, params_data: *torch_bindings.Tensor, params_len: i32, has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool, batch_first: bool) (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor) {
    outs: [6] torch_bindings.Tensor = undefined
    torch_bindings.atg__lstm_mps(&outs[0], self.t, hx_data, hx_len, params_data, params_len, has_biases, num_layers, dropout, train, bidirectional, batch_first)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]), wrap_and_track(outs[4]), wrap_and_track(outs[5]))
}

Tensor._lstm_mps_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, out3: Tensor, out4: Tensor, out5: Tensor, hx_data: *torch_bindings.Tensor, hx_len: i32, params_data: *torch_bindings.Tensor, params_len: i32, has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool, batch_first: bool) (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor) {
    outs: [6] torch_bindings.Tensor = undefined
    torch_bindings.atg__lstm_mps_out(&outs[0], out0.t, out1.t, out2.t, out3.t, out4.t, out5.t, self.t, hx_data, hx_len, params_data, params_len, has_biases, num_layers, dropout, train, bidirectional, batch_first)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]), wrap_and_track(outs[4]), wrap_and_track(outs[5]))
}

Tensor._lu_with_info :: proc(self: *Tensor, pivot: bool, check_errors: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__lu_with_info(&outs[0], self.t, pivot, check_errors)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._make_dep_token :: proc(options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__make_dep_token(&outs[0], options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor._make_dual :: proc(primal: Tensor, tangent: Tensor, level: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__make_dual(&outs[0], primal.t, tangent.t, level)
    return wrap_and_track(outs[0])
}

Tensor._make_dual_copy :: proc(primal: Tensor, tangent: Tensor, level: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__make_dual_copy(&outs[0], primal.t, tangent.t, level)
    return wrap_and_track(outs[0])
}

Tensor._make_dual_copy_out :: proc(out: Tensor, primal: Tensor, tangent: Tensor, level: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__make_dual_copy_out(&outs[0], out.t, primal.t, tangent.t, level)
    return wrap_and_track(outs[0])
}

Tensor._make_per_channel_quantized_tensor :: proc(self: *Tensor, scale: Tensor, zero_point: Tensor, axis: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__make_per_channel_quantized_tensor(&outs[0], self.t, scale.t, zero_point.t, axis)
    return wrap_and_track(outs[0])
}

Tensor._make_per_channel_quantized_tensor_out :: proc(self: *Tensor, out: Tensor, scale: Tensor, zero_point: Tensor, axis: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__make_per_channel_quantized_tensor_out(&outs[0], out.t, self.t, scale.t, zero_point.t, axis)
    return wrap_and_track(outs[0])
}

Tensor._make_per_tensor_quantized_tensor :: proc(self: *Tensor, scale: f64, zero_point: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__make_per_tensor_quantized_tensor(&outs[0], self.t, scale, zero_point)
    return wrap_and_track(outs[0])
}

Tensor._make_per_tensor_quantized_tensor_out :: proc(self: *Tensor, out: Tensor, scale: f64, zero_point: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__make_per_tensor_quantized_tensor_out(&outs[0], out.t, self.t, scale, zero_point)
    return wrap_and_track(outs[0])
}

Tensor._masked_scale :: proc(self: *Tensor, mask: Tensor, scale: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__masked_scale(&outs[0], self.t, mask.t, scale)
    return wrap_and_track(outs[0])
}

Tensor._masked_scale_out :: proc(self: *Tensor, out: Tensor, mask: Tensor, scale: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__masked_scale_out(&outs[0], out.t, self.t, mask.t, scale)
    return wrap_and_track(outs[0])
}

Tensor._masked_softmax :: proc(self: *Tensor, mask: Tensor, dim_v: i64, dim_null: u8, mask_type_v: i64, mask_type_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__masked_softmax(&outs[0], self.t, mask.t, dim_v, dim_null, mask_type_v, mask_type_null)
    return wrap_and_track(outs[0])
}

Tensor._masked_softmax_backward :: proc(grad_output: Tensor, output: Tensor, mask: Tensor, dim_v: i64, dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__masked_softmax_backward(&outs[0], grad_output.t, output.t, mask.t, dim_v, dim_null)
    return wrap_and_track(outs[0])
}

Tensor._masked_softmax_backward_out :: proc(out: Tensor, grad_output: Tensor, output: Tensor, mask: Tensor, dim_v: i64, dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__masked_softmax_backward_out(&outs[0], out.t, grad_output.t, output.t, mask.t, dim_v, dim_null)
    return wrap_and_track(outs[0])
}

Tensor._masked_softmax_out :: proc(self: *Tensor, out: Tensor, mask: Tensor, dim_v: i64, dim_null: u8, mask_type_v: i64, mask_type_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__masked_softmax_out(&outs[0], out.t, self.t, mask.t, dim_v, dim_null, mask_type_v, mask_type_null)
    return wrap_and_track(outs[0])
}

Tensor._mixed_dtypes_linear :: proc(self: *Tensor, weight: Tensor, scale: Tensor, bias: Tensor, activation_ptr: *u8, activation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__mixed_dtypes_linear(&outs[0], self.t, weight.t, scale.t, bias.t, activation_ptr, activation_len)
    return wrap_and_track(outs[0])
}

Tensor._mkldnn_reshape :: proc(self: *Tensor, shape_data: *i64, shape_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__mkldnn_reshape(&outs[0], self.t, shape_data, shape_len)
    return wrap_and_track(outs[0])
}

Tensor._mkldnn_reshape_out :: proc(self: *Tensor, out: Tensor, shape_data: *i64, shape_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__mkldnn_reshape_out(&outs[0], out.t, self.t, shape_data, shape_len)
    return wrap_and_track(outs[0])
}

Tensor._mkldnn_transpose :: proc(self: *Tensor, dim0: i64, dim1: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__mkldnn_transpose(&outs[0], self.t, dim0, dim1)
    return wrap_and_track(outs[0])
}

Tensor._mkldnn_transpose_ :: proc(self: *Tensor, dim0: i64, dim1: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__mkldnn_transpose_(&outs[0], self.t, dim0, dim1)
    return wrap_and_track(outs[0])
}

Tensor._mkldnn_transpose_out :: proc(self: *Tensor, out: Tensor, dim0: i64, dim1: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__mkldnn_transpose_out(&outs[0], out.t, self.t, dim0, dim1)
    return wrap_and_track(outs[0])
}

Tensor._mps_convolution :: proc(self: *Tensor, weight: Tensor, bias: Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__mps_convolution(&outs[0], self.t, weight.t, bias.t, padding_data, padding_len, stride_data, stride_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor._mps_convolution_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__mps_convolution_out(&outs[0], out.t, self.t, weight.t, bias.t, padding_data, padding_len, stride_data, stride_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor._mps_convolution_transpose :: proc(self: *Tensor, weight: Tensor, padding_data: *i64, padding_len: i32, output_padding_data: *i64, output_padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__mps_convolution_transpose(&outs[0], self.t, weight.t, padding_data, padding_len, output_padding_data, output_padding_len, stride_data, stride_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor._mps_convolution_transpose_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, padding_data: *i64, padding_len: i32, output_padding_data: *i64, output_padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__mps_convolution_transpose_out(&outs[0], out.t, self.t, weight.t, padding_data, padding_len, output_padding_data, output_padding_len, stride_data, stride_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor._native_batch_norm_legit :: proc(self: *Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, training: bool, momentum: f64, eps: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__native_batch_norm_legit(&outs[0], self.t, weight.t, bias.t, running_mean.t, running_var.t, training, momentum, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._native_batch_norm_legit_functional :: proc(self: *Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, training: bool, momentum: f64, eps: f64) (Tensor, Tensor, Tensor, Tensor, Tensor) {
    outs: [5] torch_bindings.Tensor = undefined
    torch_bindings.atg__native_batch_norm_legit_functional(&outs[0], self.t, weight.t, bias.t, running_mean.t, running_var.t, training, momentum, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]), wrap_and_track(outs[4]))
}

Tensor._native_batch_norm_legit_no_stats :: proc(self: *Tensor, weight: Tensor, bias: Tensor, training: bool, momentum: f64, eps: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__native_batch_norm_legit_no_stats(&outs[0], self.t, weight.t, bias.t, training, momentum, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._native_batch_norm_legit_no_stats_out :: proc(self: *Tensor, out: Tensor, save_mean: Tensor, save_invstd: Tensor, weight: Tensor, bias: Tensor, training: bool, momentum: f64, eps: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__native_batch_norm_legit_no_stats_out(&outs[0], out.t, save_mean.t, save_invstd.t, self.t, weight.t, bias.t, training, momentum, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._native_batch_norm_legit_no_training :: proc(self: *Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, momentum: f64, eps: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__native_batch_norm_legit_no_training(&outs[0], self.t, weight.t, bias.t, running_mean.t, running_var.t, momentum, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._native_batch_norm_legit_no_training_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, momentum: f64, eps: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__native_batch_norm_legit_no_training_out(&outs[0], out0.t, out1.t, out2.t, self.t, weight.t, bias.t, running_mean.t, running_var.t, momentum, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._native_batch_norm_legit_out :: proc(self: *Tensor, out: Tensor, save_mean: Tensor, save_invstd: Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, training: bool, momentum: f64, eps: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__native_batch_norm_legit_out(&outs[0], out.t, save_mean.t, save_invstd.t, self.t, weight.t, bias.t, running_mean.t, running_var.t, training, momentum, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._native_multi_head_attention :: proc(query: Tensor, key: Tensor, value: Tensor, embed_dim: i64, num_head: i64, qkv_weight: Tensor, qkv_bias: Tensor, proj_weight: Tensor, proj_bias: Tensor, mask: Tensor, need_weights: bool, average_attn_weights: bool, mask_type_v: i64, mask_type_null: u8) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__native_multi_head_attention(&outs[0], query.t, key.t, value.t, embed_dim, num_head, qkv_weight.t, qkv_bias.t, proj_weight.t, proj_bias.t, mask.t, need_weights, average_attn_weights, mask_type_v, mask_type_null)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._native_multi_head_attention_out :: proc(out0: Tensor, out1: Tensor, query: Tensor, key: Tensor, value: Tensor, embed_dim: i64, num_head: i64, qkv_weight: Tensor, qkv_bias: Tensor, proj_weight: Tensor, proj_bias: Tensor, mask: Tensor, need_weights: bool, average_attn_weights: bool, mask_type_v: i64, mask_type_null: u8) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__native_multi_head_attention_out(&outs[0], out0.t, out1.t, query.t, key.t, value.t, embed_dim, num_head, qkv_weight.t, qkv_bias.t, proj_weight.t, proj_bias.t, mask.t, need_weights, average_attn_weights, mask_type_v, mask_type_null)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._neg_view :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__neg_view(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._neg_view_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__neg_view_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._neg_view_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__neg_view_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_compute_contiguous_strides_offsets :: proc(nested_size: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_compute_contiguous_strides_offsets(&outs[0], nested_size.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._nested_from_padded :: proc(padded: Tensor, cpu_nested_shape_example: Tensor, fuse_transform_0213: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_from_padded(&outs[0], padded.t, cpu_nested_shape_example.t, fuse_transform_0213)
    return wrap_and_track(outs[0])
}

Tensor._nested_from_padded_and_nested_example :: proc(padded: Tensor, nt_example: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_from_padded_and_nested_example(&outs[0], padded.t, nt_example.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_from_padded_and_nested_example_out :: proc(out: Tensor, padded: Tensor, nt_example: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_from_padded_and_nested_example_out(&outs[0], out.t, padded.t, nt_example.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_from_padded_out :: proc(out: Tensor, padded: Tensor, cpu_nested_shape_example: Tensor, fuse_transform_0213: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_from_padded_out(&outs[0], out.t, padded.t, cpu_nested_shape_example.t, fuse_transform_0213)
    return wrap_and_track(outs[0])
}

Tensor._nested_from_padded_tensor :: proc(padded: Tensor, offsets: Tensor, dummy: Tensor, ragged_idx: i64, min_seqlen: Tensor, max_seqlen: Tensor, sum_S_v: i64, sum_S_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_from_padded_tensor(&outs[0], padded.t, offsets.t, dummy.t, ragged_idx, min_seqlen.t, max_seqlen.t, sum_S_v, sum_S_null)
    return wrap_and_track(outs[0])
}

Tensor._nested_get_jagged_dummy :: proc(r#any: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_get_jagged_dummy(&outs[0], r#any.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_get_lengths :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_get_lengths(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_get_max_seqlen :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_get_max_seqlen(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_get_min_seqlen :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_get_min_seqlen(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_get_offsets :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_get_offsets(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_get_ragged_idx :: proc(self: *Tensor) i64 {
    return torch_bindings.atg__nested_get_ragged_idx(self.t)
}

Tensor._nested_get_values :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_get_values(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_get_values_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_get_values_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_get_values_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_get_values_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_select_backward :: proc(self: *Tensor, grad_output: Tensor, dim: i64, index: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_select_backward(&outs[0], grad_output.t, self.t, dim, index)
    return wrap_and_track(outs[0])
}

Tensor._nested_sum_backward :: proc(self: *Tensor, grad: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_sum_backward(&outs[0], grad.t, self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor._nested_view_from_buffer :: proc(self: *Tensor, nested_size: Tensor, nested_strides: Tensor, offsets: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_view_from_buffer(&outs[0], self.t, nested_size.t, nested_strides.t, offsets.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_view_from_buffer_copy :: proc(self: *Tensor, nested_size: Tensor, nested_strides: Tensor, offsets: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_view_from_buffer_copy(&outs[0], self.t, nested_size.t, nested_strides.t, offsets.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_view_from_buffer_copy_out :: proc(self: *Tensor, out: Tensor, nested_size: Tensor, nested_strides: Tensor, offsets: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_view_from_buffer_copy_out(&outs[0], out.t, self.t, nested_size.t, nested_strides.t, offsets.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_view_from_jagged :: proc(self: *Tensor, offsets: Tensor, dummy: Tensor, lengths: Tensor, ragged_idx: i64, min_seqlen: Tensor, max_seqlen: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_view_from_jagged(&outs[0], self.t, offsets.t, dummy.t, lengths.t, ragged_idx, min_seqlen.t, max_seqlen.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_view_from_jagged_copy :: proc(self: *Tensor, offsets: Tensor, dummy: Tensor, lengths: Tensor, ragged_idx: i64, min_seqlen: Tensor, max_seqlen: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_view_from_jagged_copy(&outs[0], self.t, offsets.t, dummy.t, lengths.t, ragged_idx, min_seqlen.t, max_seqlen.t)
    return wrap_and_track(outs[0])
}

Tensor._nested_view_from_jagged_copy_out :: proc(self: *Tensor, out: Tensor, offsets: Tensor, dummy: Tensor, lengths: Tensor, ragged_idx: i64, min_seqlen: Tensor, max_seqlen: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nested_view_from_jagged_copy_out(&outs[0], out.t, self.t, offsets.t, dummy.t, lengths.t, ragged_idx, min_seqlen.t, max_seqlen.t)
    return wrap_and_track(outs[0])
}

Tensor._new_zeros_with_same_feature_meta :: proc(self: *Tensor, other: Tensor, self_num_batch_dims: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__new_zeros_with_same_feature_meta(&outs[0], self.t, other.t, self_num_batch_dims)
    return wrap_and_track(outs[0])
}

Tensor._new_zeros_with_same_feature_meta_out :: proc(self: *Tensor, out: Tensor, other: Tensor, self_num_batch_dims: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__new_zeros_with_same_feature_meta_out(&outs[0], out.t, self.t, other.t, self_num_batch_dims)
    return wrap_and_track(outs[0])
}

Tensor._nnpack_available :: proc() bool {
    return torch_bindings.atg__nnpack_available()
}

Tensor._nnpack_spatial_convolution :: proc(self: *Tensor, weight: Tensor, bias: Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nnpack_spatial_convolution(&outs[0], self.t, weight.t, bias.t, padding_data, padding_len, stride_data, stride_len)
    return wrap_and_track(outs[0])
}

Tensor._nnpack_spatial_convolution_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__nnpack_spatial_convolution_out(&outs[0], out.t, self.t, weight.t, bias.t, padding_data, padding_len, stride_data, stride_len)
    return wrap_and_track(outs[0])
}

Tensor._nnz :: proc(self: *Tensor) i64 {
    return torch_bindings.atg__nnz(self.t)
}

Tensor._pack_padded_sequence :: proc(self: *Tensor, lengths: Tensor, batch_first: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__pack_padded_sequence(&outs[0], self.t, lengths.t, batch_first)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._pack_padded_sequence_backward :: proc(grad: Tensor, input_size_data: *i64, input_size_len: i32, batch_sizes: Tensor, batch_first: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__pack_padded_sequence_backward(&outs[0], grad.t, input_size_data, input_size_len, batch_sizes.t, batch_first)
    return wrap_and_track(outs[0])
}

Tensor._pack_padded_sequence_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, lengths: Tensor, batch_first: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__pack_padded_sequence_out(&outs[0], out0.t, out1.t, self.t, lengths.t, batch_first)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._pad_circular :: proc(self: *Tensor, pad_data: *i64, pad_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__pad_circular(&outs[0], self.t, pad_data, pad_len)
    return wrap_and_track(outs[0])
}

Tensor._pad_enum :: proc(self: *Tensor, pad_data: *i64, pad_len: i32, mode: i64, value_v: f64, value_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__pad_enum(&outs[0], self.t, pad_data, pad_len, mode, value_v, value_null)
    return wrap_and_track(outs[0])
}

Tensor._pad_packed_sequence :: proc(data: Tensor, batch_sizes: Tensor, batch_first: bool, padding_value: torch_bindings.Scalar, total_length: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__pad_packed_sequence(&outs[0], data.t, batch_sizes.t, batch_first, padding_value, total_length)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._pdist_backward :: proc(self: *Tensor, grad: Tensor, p: f64, pdist: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__pdist_backward(&outs[0], grad.t, self.t, p, pdist.t)
    return wrap_and_track(outs[0])
}

Tensor._pdist_backward_out :: proc(self: *Tensor, out: Tensor, grad: Tensor, p: f64, pdist: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__pdist_backward_out(&outs[0], out.t, grad.t, self.t, p, pdist.t)
    return wrap_and_track(outs[0])
}

Tensor._pin_memory :: proc(self: *Tensor, device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__pin_memory(&outs[0], self.t, device)
    return wrap_and_track(outs[0])
}

Tensor._pin_memory_out :: proc(self: *Tensor, out: Tensor, device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__pin_memory_out(&outs[0], out.t, self.t, device)
    return wrap_and_track(outs[0])
}

Tensor._prelu_kernel :: proc(self: *Tensor, weight: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__prelu_kernel(&outs[0], self.t, weight.t)
    return wrap_and_track(outs[0])
}

Tensor._prelu_kernel_backward :: proc(self: *Tensor, grad_output: Tensor, weight: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__prelu_kernel_backward(&outs[0], grad_output.t, self.t, weight.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._print :: proc(s_ptr: *u8, s_len: i32) void {
    torch_bindings.atg__print(s_ptr, s_len)
}

Tensor._propagate_xla_data :: proc(self: *Tensor, output: Tensor) void {
    torch_bindings.atg__propagate_xla_data(self.t, output.t)
}

Tensor._remove_batch_dim :: proc(self: *Tensor, level: i64, batch_size: i64, out_dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__remove_batch_dim(&outs[0], self.t, level, batch_size, out_dim)
    return wrap_and_track(outs[0])
}

Tensor._reshape_alias :: proc(self: *Tensor, size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__reshape_alias(&outs[0], self.t, size_data, size_len, stride_data, stride_len)
    return wrap_and_track(outs[0])
}

Tensor._reshape_alias_copy :: proc(self: *Tensor, size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__reshape_alias_copy(&outs[0], self.t, size_data, size_len, stride_data, stride_len)
    return wrap_and_track(outs[0])
}

Tensor._reshape_alias_copy_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__reshape_alias_copy_out(&outs[0], out.t, self.t, size_data, size_len, stride_data, stride_len)
    return wrap_and_track(outs[0])
}

Tensor._reshape_copy :: proc(self: *Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__reshape_copy(&outs[0], self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor._reshape_from_tensor :: proc(self: *Tensor, shape: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__reshape_from_tensor(&outs[0], self.t, shape.t)
    return wrap_and_track(outs[0])
}

Tensor._resize_output :: proc(self: *Tensor, size_data: *i64, size_len: i32, device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__resize_output(&outs[0], self.t, size_data, size_len, device)
    return wrap_and_track(outs[0])
}

Tensor._resize_output_ :: proc(self: *Tensor, size_data: *i64, size_len: i32, device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__resize_output_(&outs[0], self.t, size_data, size_len, device)
    return wrap_and_track(outs[0])
}

Tensor._resize_output_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32, device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__resize_output_out(&outs[0], out.t, self.t, size_data, size_len, device)
    return wrap_and_track(outs[0])
}

Tensor._rowwise_prune :: proc(weight: Tensor, mask: Tensor, compressed_indices_dtype: i32) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__rowwise_prune(&outs[0], weight.t, mask.t, compressed_indices_dtype)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._safe_softmax :: proc(self: *Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__safe_softmax(&outs[0], self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor._sample_dirichlet :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sample_dirichlet(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._sample_dirichlet_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sample_dirichlet_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._saturate_weight_to_fp16 :: proc(weight: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__saturate_weight_to_fp16(&outs[0], weight.t)
    return wrap_and_track(outs[0])
}

Tensor._scaled_dot_product_attention_math :: proc(query: Tensor, key: Tensor, value: Tensor, attn_mask: Tensor, dropout_p: f64, is_causal: bool, dropout_mask: Tensor, scale_v: f64, scale_null: u8, enable_gqa: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__scaled_dot_product_attention_math(&outs[0], query.t, key.t, value.t, attn_mask.t, dropout_p, is_causal, dropout_mask.t, scale_v, scale_null, enable_gqa)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._scaled_dot_product_attention_math_for_mps :: proc(query: Tensor, key: Tensor, value: Tensor, attn_mask: Tensor, dropout_p: f64, is_causal: bool, dropout_mask: Tensor, scale_v: f64, scale_null: u8) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__scaled_dot_product_attention_math_for_mps(&outs[0], query.t, key.t, value.t, attn_mask.t, dropout_p, is_causal, dropout_mask.t, scale_v, scale_null)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._scaled_dot_product_cudnn_attention_backward :: proc(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, philox_seed: Tensor, philox_offset: Tensor, attn_bias: Tensor, cum_seq_q: Tensor, cum_seq_k: Tensor, max_q: i64, max_k: i64, dropout_p: f64, is_causal: bool, scale_v: f64, scale_null: u8) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__scaled_dot_product_cudnn_attention_backward(&outs[0], grad_out.t, query.t, key.t, value.t, out.t, logsumexp.t, philox_seed.t, philox_offset.t, attn_bias.t, cum_seq_q.t, cum_seq_k.t, max_q, max_k, dropout_p, is_causal, scale_v, scale_null)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._scaled_dot_product_efficient_attention :: proc(query: Tensor, key: Tensor, value: Tensor, attn_bias: Tensor, compute_log_sumexp: bool, dropout_p: f64, is_causal: bool, scale_v: f64, scale_null: u8) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg__scaled_dot_product_efficient_attention(&outs[0], query.t, key.t, value.t, attn_bias.t, compute_log_sumexp, dropout_p, is_causal, scale_v, scale_null)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor._scaled_dot_product_flash_attention_backward :: proc(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, cum_seq_q: Tensor, cum_seq_k: Tensor, max_q: i64, max_k: i64, dropout_p: f64, is_causal: bool, philox_seed: Tensor, philox_offset: Tensor, scale_v: f64, scale_null: u8) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__scaled_dot_product_flash_attention_backward(&outs[0], grad_out.t, query.t, key.t, value.t, out.t, logsumexp.t, cum_seq_q.t, cum_seq_k.t, max_q, max_k, dropout_p, is_causal, philox_seed.t, philox_offset.t, scale_v, scale_null)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._scaled_dot_product_flash_attention_for_cpu :: proc(query: Tensor, key: Tensor, value: Tensor, dropout_p: f64, is_causal: bool, attn_mask: Tensor, scale_v: f64, scale_null: u8) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__scaled_dot_product_flash_attention_for_cpu(&outs[0], query.t, key.t, value.t, dropout_p, is_causal, attn_mask.t, scale_v, scale_null)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._scaled_dot_product_flash_attention_for_cpu_backward :: proc(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, dropout_p: f64, is_causal: bool, attn_mask: Tensor, scale_v: f64, scale_null: u8) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__scaled_dot_product_flash_attention_for_cpu_backward(&outs[0], grad_out.t, query.t, key.t, value.t, out.t, logsumexp.t, dropout_p, is_causal, attn_mask.t, scale_v, scale_null)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._scaled_grouped_mm :: proc(self: *Tensor, mat2: Tensor, scale_a: Tensor, scale_b: Tensor, offs: Tensor, bias: Tensor, scale_result: Tensor, out_dtype: i32, use_fast_accum: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__scaled_grouped_mm(&outs[0], self.t, mat2.t, scale_a.t, scale_b.t, offs.t, bias.t, scale_result.t, out_dtype, use_fast_accum)
    return wrap_and_track(outs[0])
}

Tensor._scaled_mm :: proc(self: *Tensor, mat2: Tensor, scale_a: Tensor, scale_b: Tensor, bias: Tensor, scale_result: Tensor, out_dtype: i32, use_fast_accum: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__scaled_mm(&outs[0], self.t, mat2.t, scale_a.t, scale_b.t, bias.t, scale_result.t, out_dtype, use_fast_accum)
    return wrap_and_track(outs[0])
}

Tensor._scaled_mm_out :: proc(self: *Tensor, out: Tensor, mat2: Tensor, scale_a: Tensor, scale_b: Tensor, bias: Tensor, scale_result: Tensor, out_dtype: i32, use_fast_accum: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__scaled_mm_out(&outs[0], out.t, self.t, mat2.t, scale_a.t, scale_b.t, bias.t, scale_result.t, out_dtype, use_fast_accum)
    return wrap_and_track(outs[0])
}

Tensor._scatter_reduce :: proc(self: *Tensor, dim: i64, index: Tensor, src: Tensor, reduce_ptr: *u8, reduce_len: i32, include_self: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__scatter_reduce(&outs[0], self.t, dim, index.t, src.t, reduce_ptr, reduce_len, include_self)
    return wrap_and_track(outs[0])
}

Tensor._scatter_reduce_ :: proc(self: *Tensor, dim: i64, index: Tensor, src: Tensor, reduce_ptr: *u8, reduce_len: i32, include_self: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__scatter_reduce_(&outs[0], self.t, dim, index.t, src.t, reduce_ptr, reduce_len, include_self)
    return wrap_and_track(outs[0])
}

Tensor._scatter_reduce_two_out :: proc(self: *Tensor, out: Tensor, dim: i64, index: Tensor, src: Tensor, reduce_ptr: *u8, reduce_len: i32, include_self: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__scatter_reduce_two_out(&outs[0], out.t, self.t, dim, index.t, src.t, reduce_ptr, reduce_len, include_self)
    return wrap_and_track(outs[0])
}

Tensor._segment_reduce_backward :: proc(grad: Tensor, output: Tensor, data: Tensor, reduce_ptr: *u8, reduce_len: i32, lengths: Tensor, offsets: Tensor, axis: i64, initial: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__segment_reduce_backward(&outs[0], grad.t, output.t, data.t, reduce_ptr, reduce_len, lengths.t, offsets.t, axis, initial)
    return wrap_and_track(outs[0])
}

Tensor._segment_reduce_backward_out :: proc(out: Tensor, grad: Tensor, output: Tensor, data: Tensor, reduce_ptr: *u8, reduce_len: i32, lengths: Tensor, offsets: Tensor, axis: i64, initial: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__segment_reduce_backward_out(&outs[0], out.t, grad.t, output.t, data.t, reduce_ptr, reduce_len, lengths.t, offsets.t, axis, initial)
    return wrap_and_track(outs[0])
}

Tensor._shape_as_tensor :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__shape_as_tensor(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._slow_conv2d_backward :: proc(self: *Tensor, grad_input: Tensor, grad_weight: Tensor, grad_bias: Tensor, grad_output: Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__slow_conv2d_backward(&outs[0], grad_input.t, grad_weight.t, grad_bias.t, grad_output.t, self.t, weight.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._sobol_engine_draw :: proc(quasi: Tensor, n: i64, sobolstate: Tensor, dimension: i64, num_generated: i64, dtype: i32) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__sobol_engine_draw(&outs[0], quasi.t, n, sobolstate.t, dimension, num_generated, dtype)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._sobol_engine_ff_ :: proc(self: *Tensor, n: i64, sobolstate: Tensor, dimension: i64, num_generated: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sobol_engine_ff_(&outs[0], self.t, n, sobolstate.t, dimension, num_generated)
    return wrap_and_track(outs[0])
}

Tensor._sobol_engine_initialize_state_ :: proc(self: *Tensor, dimension: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sobol_engine_initialize_state_(&outs[0], self.t, dimension)
    return wrap_and_track(outs[0])
}

Tensor._sobol_engine_scramble_ :: proc(self: *Tensor, ltm: Tensor, dimension: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sobol_engine_scramble_(&outs[0], self.t, ltm.t, dimension)
    return wrap_and_track(outs[0])
}

Tensor._softmax :: proc(self: *Tensor, dim: i64, half_to_float: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__softmax(&outs[0], self.t, dim, half_to_float)
    return wrap_and_track(outs[0])
}

Tensor._softmax_backward_data :: proc(grad_output: Tensor, output: Tensor, dim: i64, input_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__softmax_backward_data(&outs[0], grad_output.t, output.t, dim, input_dtype)
    return wrap_and_track(outs[0])
}

Tensor._softmax_backward_data_out :: proc(grad_input: Tensor, grad_output: Tensor, output: Tensor, dim: i64, input_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__softmax_backward_data_out(&outs[0], grad_input.t, grad_output.t, output.t, dim, input_dtype)
    return wrap_and_track(outs[0])
}

Tensor._softmax_out :: proc(self: *Tensor, out: Tensor, dim: i64, half_to_float: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__softmax_out(&outs[0], out.t, self.t, dim, half_to_float)
    return wrap_and_track(outs[0])
}

Tensor._sparse_addmm :: proc(self: *Tensor, mat1: Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_addmm(&outs[0], self.t, mat1.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor._sparse_addmm_out :: proc(self: *Tensor, out: Tensor, mat1: Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_addmm_out(&outs[0], out.t, self.t, mat1.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor._sparse_broadcast_to :: proc(self: *Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_broadcast_to(&outs[0], self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor._sparse_broadcast_to_copy :: proc(self: *Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_broadcast_to_copy(&outs[0], self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor._sparse_broadcast_to_copy_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_broadcast_to_copy_out(&outs[0], out.t, self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor._sparse_bsc_tensor_unsafe :: proc(ccol_indices: Tensor, row_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_bsc_tensor_unsafe(&outs[0], ccol_indices.t, row_indices.t, values.t, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor._sparse_bsr_tensor_unsafe :: proc(crow_indices: Tensor, col_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_bsr_tensor_unsafe(&outs[0], crow_indices.t, col_indices.t, values.t, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor._sparse_compressed_tensor_unsafe :: proc(compressed_indices: Tensor, plain_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_compressed_tensor_unsafe(&outs[0], compressed_indices.t, plain_indices.t, values.t, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor._sparse_compressed_tensor_with_dims :: proc(nnz: i64, dense_dim: i64, size_data: *i64, size_len: i32, blocksize_data: *i64, blocksize_len: i32, index_dtype: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_compressed_tensor_with_dims(&outs[0], nnz, dense_dim, size_data, size_len, blocksize_data, blocksize_len, index_dtype, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor._sparse_coo_tensor_unsafe :: proc(indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32, is_coalesced: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_coo_tensor_unsafe(&outs[0], indices.t, values.t, size_data, size_len, options_kind, options_device, is_coalesced)
    return wrap_and_track(outs[0])
}

Tensor._sparse_coo_tensor_with_dims :: proc(sparse_dim: i64, dense_dim: i64, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_coo_tensor_with_dims(&outs[0], sparse_dim, dense_dim, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor._sparse_coo_tensor_with_dims_and_tensors :: proc(sparse_dim: i64, dense_dim: i64, size_data: *i64, size_len: i32, indices: Tensor, values: Tensor, options_kind: i32, options_device: i32, is_coalesced: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_coo_tensor_with_dims_and_tensors(&outs[0], sparse_dim, dense_dim, size_data, size_len, indices.t, values.t, options_kind, options_device, is_coalesced)
    return wrap_and_track(outs[0])
}

Tensor._sparse_coo_tensor_with_dims_and_tensors_out :: proc(out: Tensor, sparse_dim: i64, dense_dim: i64, size_data: *i64, size_len: i32, indices: Tensor, values: Tensor, is_coalesced: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_coo_tensor_with_dims_and_tensors_out(&outs[0], out.t, sparse_dim, dense_dim, size_data, size_len, indices.t, values.t, is_coalesced)
    return wrap_and_track(outs[0])
}

Tensor._sparse_coo_tensor_with_dims_out :: proc(out: Tensor, sparse_dim: i64, dense_dim: i64, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_coo_tensor_with_dims_out(&outs[0], out.t, sparse_dim, dense_dim, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor._sparse_csc_tensor_unsafe :: proc(ccol_indices: Tensor, row_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_csc_tensor_unsafe(&outs[0], ccol_indices.t, row_indices.t, values.t, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor._sparse_csr_prod :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_csr_prod(&outs[0], self.t, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor._sparse_csr_prod_dim_dtype_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_csr_prod_dim_dtype_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor._sparse_csr_sum :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_csr_sum(&outs[0], self.t, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor._sparse_csr_sum_dim_dtype_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_csr_sum_dim_dtype_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor._sparse_csr_tensor_unsafe :: proc(crow_indices: Tensor, col_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_csr_tensor_unsafe(&outs[0], crow_indices.t, col_indices.t, values.t, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor._sparse_log_softmax :: proc(self: *Tensor, dim: i64, half_to_float: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_log_softmax(&outs[0], self.t, dim, half_to_float)
    return wrap_and_track(outs[0])
}

Tensor._sparse_log_softmax_backward_data :: proc(self: *Tensor, grad_output: Tensor, output: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_log_softmax_backward_data(&outs[0], grad_output.t, output.t, dim, self.t)
    return wrap_and_track(outs[0])
}

Tensor._sparse_log_softmax_backward_data_out :: proc(self: *Tensor, out: Tensor, grad_output: Tensor, output: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_log_softmax_backward_data_out(&outs[0], out.t, grad_output.t, output.t, dim, self.t)
    return wrap_and_track(outs[0])
}

Tensor._sparse_log_softmax_int :: proc(self: *Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_log_softmax_int(&outs[0], self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor._sparse_log_softmax_out :: proc(self: *Tensor, out: Tensor, dim: i64, half_to_float: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_log_softmax_out(&outs[0], out.t, self.t, dim, half_to_float)
    return wrap_and_track(outs[0])
}

Tensor._sparse_mask_projection :: proc(self: *Tensor, mask: Tensor, accumulate_matches: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_mask_projection(&outs[0], self.t, mask.t, accumulate_matches)
    return wrap_and_track(outs[0])
}

Tensor._sparse_mask_projection_out :: proc(self: *Tensor, out: Tensor, mask: Tensor, accumulate_matches: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_mask_projection_out(&outs[0], out.t, self.t, mask.t, accumulate_matches)
    return wrap_and_track(outs[0])
}

Tensor._sparse_mm :: proc(sparse: Tensor, dense: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_mm(&outs[0], sparse.t, dense.t)
    return wrap_and_track(outs[0])
}

Tensor._sparse_mm_reduce :: proc(sparse: Tensor, dense: Tensor, reduce_ptr: *u8, reduce_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_mm_reduce(&outs[0], sparse.t, dense.t, reduce_ptr, reduce_len)
    return wrap_and_track(outs[0])
}

Tensor._sparse_mm_reduce_impl :: proc(self: *Tensor, other: Tensor, reduce_ptr: *u8, reduce_len: i32) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_mm_reduce_impl(&outs[0], self.t, other.t, reduce_ptr, reduce_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._sparse_semi_structured_apply :: proc(self: *Tensor, thread_masks: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_semi_structured_apply(&outs[0], self.t, thread_masks.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._sparse_semi_structured_apply_dense :: proc(self: *Tensor, thread_masks: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_semi_structured_apply_dense(&outs[0], self.t, thread_masks.t)
    return wrap_and_track(outs[0])
}

Tensor._sparse_semi_structured_linear :: proc(self: *Tensor, weight: Tensor, meta: Tensor, bias: Tensor, activation_ptr: *u8, activation_len: i32, out_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_semi_structured_linear(&outs[0], self.t, weight.t, meta.t, bias.t, activation_ptr, activation_len, out_dtype)
    return wrap_and_track(outs[0])
}

Tensor._sparse_semi_structured_mm :: proc(mat1: Tensor, mat1_meta: Tensor, mat2: Tensor, out_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_semi_structured_mm(&outs[0], mat1.t, mat1_meta.t, mat2.t, out_dtype)
    return wrap_and_track(outs[0])
}

Tensor._sparse_semi_structured_tile :: proc(self: *Tensor, algorithm_ptr: *u8, algorithm_len: i32, use_cutlass: bool) (Tensor, Tensor, Tensor, Tensor, Tensor) {
    outs: [5] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_semi_structured_tile(&outs[0], self.t, algorithm_ptr, algorithm_len, use_cutlass)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]), wrap_and_track(outs[4]))
}

Tensor._sparse_softmax :: proc(self: *Tensor, dim: i64, half_to_float: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_softmax(&outs[0], self.t, dim, half_to_float)
    return wrap_and_track(outs[0])
}

Tensor._sparse_softmax_backward_data :: proc(self: *Tensor, grad_output: Tensor, output: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_softmax_backward_data(&outs[0], grad_output.t, output.t, dim, self.t)
    return wrap_and_track(outs[0])
}

Tensor._sparse_softmax_backward_data_out :: proc(self: *Tensor, out: Tensor, grad_output: Tensor, output: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_softmax_backward_data_out(&outs[0], out.t, grad_output.t, output.t, dim, self.t)
    return wrap_and_track(outs[0])
}

Tensor._sparse_softmax_int :: proc(self: *Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_softmax_int(&outs[0], self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor._sparse_softmax_out :: proc(self: *Tensor, out: Tensor, dim: i64, half_to_float: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_softmax_out(&outs[0], out.t, self.t, dim, half_to_float)
    return wrap_and_track(outs[0])
}

Tensor._sparse_sparse_matmul :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_sparse_matmul(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor._sparse_sparse_matmul_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_sparse_matmul_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor._sparse_sum :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_sum(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._sparse_sum_backward :: proc(self: *Tensor, grad: Tensor, dim_data: *i64, dim_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_sum_backward(&outs[0], grad.t, self.t, dim_data, dim_len)
    return wrap_and_track(outs[0])
}

Tensor._sparse_sum_backward_out :: proc(self: *Tensor, out: Tensor, grad: Tensor, dim_data: *i64, dim_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_sum_backward_out(&outs[0], out.t, grad.t, self.t, dim_data, dim_len)
    return wrap_and_track(outs[0])
}

Tensor._sparse_sum_dim :: proc(self: *Tensor, dim_data: *i64, dim_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_sum_dim(&outs[0], self.t, dim_data, dim_len)
    return wrap_and_track(outs[0])
}

Tensor._sparse_sum_dim_dtype :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_sum_dim_dtype(&outs[0], self.t, dim_data, dim_len, dtype)
    return wrap_and_track(outs[0])
}

Tensor._sparse_sum_dim_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_sum_dim_out(&outs[0], out.t, self.t, dim_data, dim_len)
    return wrap_and_track(outs[0])
}

Tensor._sparse_sum_dtype :: proc(self: *Tensor, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__sparse_sum_dtype(&outs[0], self.t, dtype)
    return wrap_and_track(outs[0])
}

Tensor._spdiags :: proc(diagonals: Tensor, offsets: Tensor, shape_data: *i64, shape_len: i32, layout: i8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__spdiags(&outs[0], diagonals.t, offsets.t, shape_data, shape_len, layout)
    return wrap_and_track(outs[0])
}

Tensor._spdiags_out :: proc(out: Tensor, diagonals: Tensor, offsets: Tensor, shape_data: *i64, shape_len: i32, layout: i8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__spdiags_out(&outs[0], out.t, diagonals.t, offsets.t, shape_data, shape_len, layout)
    return wrap_and_track(outs[0])
}

Tensor._spsolve :: proc(A: Tensor, B: Tensor, left: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__spsolve(&outs[0], A.t, B.t, left)
    return wrap_and_track(outs[0])
}

Tensor._stack :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__stack(&outs[0], tensors_data, tensors_len, dim)
    return wrap_and_track(outs[0])
}

Tensor._stack_out :: proc(out: Tensor, tensors_data: *torch_bindings.Tensor, tensors_len: i32, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__stack_out(&outs[0], out.t, tensors_data, tensors_len, dim)
    return wrap_and_track(outs[0])
}

Tensor._standard_gamma :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__standard_gamma(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._standard_gamma_grad :: proc(self: *Tensor, output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__standard_gamma_grad(&outs[0], self.t, output.t)
    return wrap_and_track(outs[0])
}

Tensor._standard_gamma_grad_out :: proc(self: *Tensor, out: Tensor, output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__standard_gamma_grad_out(&outs[0], out.t, self.t, output.t)
    return wrap_and_track(outs[0])
}

Tensor._standard_gamma_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__standard_gamma_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._test_ambiguous_defaults :: proc(dummy: Tensor, a: i64, b: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_ambiguous_defaults(&outs[0], dummy.t, a, b)
    return wrap_and_track(outs[0])
}

Tensor._test_ambiguous_defaults_b :: proc(dummy: Tensor, a: i64, b_ptr: *u8, b_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_ambiguous_defaults_b(&outs[0], dummy.t, a, b_ptr, b_len)
    return wrap_and_track(outs[0])
}

Tensor._test_autograd_multiple_dispatch :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_autograd_multiple_dispatch(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._test_autograd_multiple_dispatch_fullcoverage_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_autograd_multiple_dispatch_fullcoverage_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._test_autograd_multiple_dispatch_ntonly :: proc(self: *Tensor, b: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_autograd_multiple_dispatch_ntonly(&outs[0], self.t, b)
    return wrap_and_track(outs[0])
}

Tensor._test_autograd_multiple_dispatch_view :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_autograd_multiple_dispatch_view(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._test_autograd_multiple_dispatch_view_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_autograd_multiple_dispatch_view_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._test_autograd_multiple_dispatch_view_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_autograd_multiple_dispatch_view_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._test_check_tensor :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_check_tensor(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._test_functorch_fallback :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_functorch_fallback(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor._test_functorch_fallback_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_functorch_fallback_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor._test_optional_filled_intlist :: proc(values: Tensor, addends_data: *i64, addends_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_optional_filled_intlist(&outs[0], values.t, addends_data, addends_len)
    return wrap_and_track(outs[0])
}

Tensor._test_optional_filled_intlist_out :: proc(out: Tensor, values: Tensor, addends_data: *i64, addends_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_optional_filled_intlist_out(&outs[0], out.t, values.t, addends_data, addends_len)
    return wrap_and_track(outs[0])
}

Tensor._test_optional_floatlist :: proc(values: Tensor, addends_data: *f64, addends_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_optional_floatlist(&outs[0], values.t, addends_data, addends_len)
    return wrap_and_track(outs[0])
}

Tensor._test_optional_floatlist_out :: proc(out: Tensor, values: Tensor, addends_data: *f64, addends_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_optional_floatlist_out(&outs[0], out.t, values.t, addends_data, addends_len)
    return wrap_and_track(outs[0])
}

Tensor._test_optional_intlist :: proc(values: Tensor, addends_data: *i64, addends_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_optional_intlist(&outs[0], values.t, addends_data, addends_len)
    return wrap_and_track(outs[0])
}

Tensor._test_optional_intlist_out :: proc(out: Tensor, values: Tensor, addends_data: *i64, addends_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_optional_intlist_out(&outs[0], out.t, values.t, addends_data, addends_len)
    return wrap_and_track(outs[0])
}

Tensor._test_parallel_materialize :: proc(self: *Tensor, num_parallel: i64, skip_first: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_parallel_materialize(&outs[0], self.t, num_parallel, skip_first)
    return wrap_and_track(outs[0])
}

Tensor._test_serialization_subcmul :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_serialization_subcmul(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor._test_string_default :: proc(dummy: Tensor, a_ptr: *u8, a_len: i32, b_ptr: *u8, b_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_string_default(&outs[0], dummy.t, a_ptr, a_len, b_ptr, b_len)
    return wrap_and_track(outs[0])
}

Tensor._test_warn_in_autograd :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_warn_in_autograd(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._test_warn_in_autograd_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__test_warn_in_autograd_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._to_copy :: proc(self: *Tensor, options_kind: i32, options_device: i32, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_copy(&outs[0], self.t, options_kind, options_device, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor._to_copy_out :: proc(self: *Tensor, out: Tensor, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_copy_out(&outs[0], out.t, self.t, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor._to_cpu :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) [dyn] Tensor {
    ptr := torch_bindings.atg__to_cpu(tensors_data, tensors_len)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor._to_dense :: proc(self: *Tensor, dtype: i32, masked_grad: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_dense(&outs[0], self.t, dtype, masked_grad)
    return wrap_and_track(outs[0])
}

Tensor._to_dense_out :: proc(self: *Tensor, out: Tensor, dtype: i32, masked_grad: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_dense_out(&outs[0], out.t, self.t, dtype, masked_grad)
    return wrap_and_track(outs[0])
}

Tensor._to_sparse :: proc(self: *Tensor, layout: i8, blocksize_data: *i64, blocksize_len: i32, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_sparse(&outs[0], self.t, layout, blocksize_data, blocksize_len, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor._to_sparse_bsc :: proc(self: *Tensor, blocksize_data: *i64, blocksize_len: i32, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_sparse_bsc(&outs[0], self.t, blocksize_data, blocksize_len, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor._to_sparse_bsc_out :: proc(self: *Tensor, out: Tensor, blocksize_data: *i64, blocksize_len: i32, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_sparse_bsc_out(&outs[0], out.t, self.t, blocksize_data, blocksize_len, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor._to_sparse_bsr :: proc(self: *Tensor, blocksize_data: *i64, blocksize_len: i32, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_sparse_bsr(&outs[0], self.t, blocksize_data, blocksize_len, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor._to_sparse_bsr_out :: proc(self: *Tensor, out: Tensor, blocksize_data: *i64, blocksize_len: i32, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_sparse_bsr_out(&outs[0], out.t, self.t, blocksize_data, blocksize_len, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor._to_sparse_csc :: proc(self: *Tensor, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_sparse_csc(&outs[0], self.t, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor._to_sparse_csc_out :: proc(self: *Tensor, out: Tensor, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_sparse_csc_out(&outs[0], out.t, self.t, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor._to_sparse_csr :: proc(self: *Tensor, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_sparse_csr(&outs[0], self.t, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor._to_sparse_csr_out :: proc(self: *Tensor, out: Tensor, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_sparse_csr_out(&outs[0], out.t, self.t, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor._to_sparse_out :: proc(self: *Tensor, out: Tensor, layout: i8, blocksize_data: *i64, blocksize_len: i32, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_sparse_out(&outs[0], out.t, self.t, layout, blocksize_data, blocksize_len, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor._to_sparse_semi_structured :: proc(dense: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_sparse_semi_structured(&outs[0], dense.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._to_sparse_sparse_dim :: proc(self: *Tensor, sparse_dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_sparse_sparse_dim(&outs[0], self.t, sparse_dim)
    return wrap_and_track(outs[0])
}

Tensor._to_sparse_sparse_dim_out :: proc(self: *Tensor, out: Tensor, sparse_dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__to_sparse_sparse_dim_out(&outs[0], out.t, self.t, sparse_dim)
    return wrap_and_track(outs[0])
}

Tensor._transform_bias_rescale_qkv :: proc(qkv: Tensor, qkv_bias: Tensor, num_heads: i64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__transform_bias_rescale_qkv(&outs[0], qkv.t, qkv_bias.t, num_heads)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._transform_bias_rescale_qkv_out :: proc(out0: Tensor, out1: Tensor, out2: Tensor, qkv: Tensor, qkv_bias: Tensor, num_heads: i64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__transform_bias_rescale_qkv_out(&outs[0], out0.t, out1.t, out2.t, qkv.t, qkv_bias.t, num_heads)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._transformer_encoder_layer_fwd :: proc(src: Tensor, embed_dim: i64, num_heads: i64, qkv_weight: Tensor, qkv_bias: Tensor, proj_weight: Tensor, proj_bias: Tensor, use_gelu: bool, norm_first: bool, eps: f64, norm_weight_1: Tensor, norm_bias_1: Tensor, norm_weight_2: Tensor, norm_bias_2: Tensor, ffn_weight_1: Tensor, ffn_bias_1: Tensor, ffn_weight_2: Tensor, ffn_bias_2: Tensor, mask: Tensor, mask_type_v: i64, mask_type_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__transformer_encoder_layer_fwd(&outs[0], src.t, embed_dim, num_heads, qkv_weight.t, qkv_bias.t, proj_weight.t, proj_bias.t, use_gelu, norm_first, eps, norm_weight_1.t, norm_bias_1.t, norm_weight_2.t, norm_bias_2.t, ffn_weight_1.t, ffn_bias_1.t, ffn_weight_2.t, ffn_bias_2.t, mask.t, mask_type_v, mask_type_null)
    return wrap_and_track(outs[0])
}

Tensor._transformer_encoder_layer_fwd_out :: proc(out: Tensor, src: Tensor, embed_dim: i64, num_heads: i64, qkv_weight: Tensor, qkv_bias: Tensor, proj_weight: Tensor, proj_bias: Tensor, use_gelu: bool, norm_first: bool, eps: f64, norm_weight_1: Tensor, norm_bias_1: Tensor, norm_weight_2: Tensor, norm_bias_2: Tensor, ffn_weight_1: Tensor, ffn_bias_1: Tensor, ffn_weight_2: Tensor, ffn_bias_2: Tensor, mask: Tensor, mask_type_v: i64, mask_type_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__transformer_encoder_layer_fwd_out(&outs[0], out.t, src.t, embed_dim, num_heads, qkv_weight.t, qkv_bias.t, proj_weight.t, proj_bias.t, use_gelu, norm_first, eps, norm_weight_1.t, norm_bias_1.t, norm_weight_2.t, norm_bias_2.t, ffn_weight_1.t, ffn_bias_1.t, ffn_weight_2.t, ffn_bias_2.t, mask.t, mask_type_v, mask_type_null)
    return wrap_and_track(outs[0])
}

Tensor._trilinear :: proc(i1: Tensor, i2: Tensor, i3: Tensor, expand1_data: *i64, expand1_len: i32, expand2_data: *i64, expand2_len: i32, expand3_data: *i64, expand3_len: i32, sumdim_data: *i64, sumdim_len: i32, unroll_dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__trilinear(&outs[0], i1.t, i2.t, i3.t, expand1_data, expand1_len, expand2_data, expand2_len, expand3_data, expand3_len, sumdim_data, sumdim_len, unroll_dim)
    return wrap_and_track(outs[0])
}

Tensor._trilinear_out :: proc(out: Tensor, i1: Tensor, i2: Tensor, i3: Tensor, expand1_data: *i64, expand1_len: i32, expand2_data: *i64, expand2_len: i32, expand3_data: *i64, expand3_len: i32, sumdim_data: *i64, sumdim_len: i32, unroll_dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__trilinear_out(&outs[0], out.t, i1.t, i2.t, i3.t, expand1_data, expand1_len, expand2_data, expand2_len, expand3_data, expand3_len, sumdim_data, sumdim_len, unroll_dim)
    return wrap_and_track(outs[0])
}

Tensor._triton_multi_head_attention :: proc(query: Tensor, key: Tensor, value: Tensor, embed_dim: i64, num_head: i64, qkv_weight: Tensor, qkv_bias: Tensor, proj_weight: Tensor, proj_bias: Tensor, mask: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__triton_multi_head_attention(&outs[0], query.t, key.t, value.t, embed_dim, num_head, qkv_weight.t, qkv_bias.t, proj_weight.t, proj_bias.t, mask.t)
    return wrap_and_track(outs[0])
}

Tensor._triton_multi_head_attention_out :: proc(out: Tensor, query: Tensor, key: Tensor, value: Tensor, embed_dim: i64, num_head: i64, qkv_weight: Tensor, qkv_bias: Tensor, proj_weight: Tensor, proj_bias: Tensor, mask: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__triton_multi_head_attention_out(&outs[0], out.t, query.t, key.t, value.t, embed_dim, num_head, qkv_weight.t, qkv_bias.t, proj_weight.t, proj_bias.t, mask.t)
    return wrap_and_track(outs[0])
}

Tensor._triton_scaled_dot_attention :: proc(q: Tensor, k: Tensor, v: Tensor, dropout_p: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__triton_scaled_dot_attention(&outs[0], q.t, k.t, v.t, dropout_p)
    return wrap_and_track(outs[0])
}

Tensor._triton_scaled_dot_attention_out :: proc(out: Tensor, q: Tensor, k: Tensor, v: Tensor, dropout_p: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__triton_scaled_dot_attention_out(&outs[0], out.t, q.t, k.t, v.t, dropout_p)
    return wrap_and_track(outs[0])
}

Tensor._unique :: proc(self: *Tensor, sorted: bool, return_inverse: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__unique(&outs[0], self.t, sorted, return_inverse)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._unique2 :: proc(self: *Tensor, sorted: bool, return_inverse: bool, return_counts: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__unique2(&outs[0], self.t, sorted, return_inverse, return_counts)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._unique2_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, sorted: bool, return_inverse: bool, return_counts: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg__unique2_out(&outs[0], out0.t, out1.t, out2.t, self.t, sorted, return_inverse, return_counts)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor._unique_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, sorted: bool, return_inverse: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__unique_out(&outs[0], out0.t, out1.t, self.t, sorted, return_inverse)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._unpack_dual :: proc(dual: Tensor, level: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__unpack_dual(&outs[0], dual.t, level)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._unsafe_index :: proc(self: *Tensor, indices_data: *torch_bindings.Tensor, indices_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__unsafe_index(&outs[0], self.t, indices_data, indices_len)
    return wrap_and_track(outs[0])
}

Tensor._unsafe_index_put :: proc(self: *Tensor, indices_data: *torch_bindings.Tensor, indices_len: i32, values: Tensor, accumulate: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__unsafe_index_put(&outs[0], self.t, indices_data, indices_len, values.t, accumulate)
    return wrap_and_track(outs[0])
}

Tensor._unsafe_masked_index :: proc(self: *Tensor, mask: Tensor, indices_data: *torch_bindings.Tensor, indices_len: i32, fill: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__unsafe_masked_index(&outs[0], self.t, mask.t, indices_data, indices_len, fill)
    return wrap_and_track(outs[0])
}

Tensor._unsafe_masked_index_put_accumulate :: proc(self: *Tensor, mask: Tensor, indices_data: *torch_bindings.Tensor, indices_len: i32, values: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__unsafe_masked_index_put_accumulate(&outs[0], self.t, mask.t, indices_data, indices_len, values.t)
    return wrap_and_track(outs[0])
}

Tensor._unsafe_view :: proc(self: *Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__unsafe_view(&outs[0], self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor._unsafe_view_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__unsafe_view_out(&outs[0], out.t, self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor._upsample_bicubic2d_aa :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_bicubic2d_aa(&outs[0], self.t, output_size_data, output_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_bicubic2d_aa_backward :: proc(grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_bicubic2d_aa_backward(&outs[0], grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_bicubic2d_aa_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_bicubic2d_aa_backward_grad_input(&outs[0], grad_input.t, grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_bicubic2d_aa_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_bicubic2d_aa_out(&outs[0], out.t, self.t, output_size_data, output_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_bicubic2d_aa_vec :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scale_factors_data: *f64, scale_factors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_bicubic2d_aa_vec(&outs[0], self.t, output_size_data, output_size_len, align_corners, scale_factors_data, scale_factors_len)
    return wrap_and_track(outs[0])
}

Tensor._upsample_bilinear2d_aa :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_bilinear2d_aa(&outs[0], self.t, output_size_data, output_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_bilinear2d_aa_backward :: proc(grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_bilinear2d_aa_backward(&outs[0], grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_bilinear2d_aa_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_bilinear2d_aa_backward_grad_input(&outs[0], grad_input.t, grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_bilinear2d_aa_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_bilinear2d_aa_out(&outs[0], out.t, self.t, output_size_data, output_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_bilinear2d_aa_vec :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scale_factors_data: *f64, scale_factors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_bilinear2d_aa_vec(&outs[0], self.t, output_size_data, output_size_len, align_corners, scale_factors_data, scale_factors_len)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact1d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, scales_v: f64, scales_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact1d(&outs[0], self.t, output_size_data, output_size_len, scales_v, scales_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact1d_backward :: proc(grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, scales_v: f64, scales_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact1d_backward(&outs[0], grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, scales_v, scales_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact1d_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, scales_v: f64, scales_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact1d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, scales_v, scales_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact1d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, scales_v: f64, scales_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact1d_out(&outs[0], out.t, self.t, output_size_data, output_size_len, scales_v, scales_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact1d_vec :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, scale_factors_data: *f64, scale_factors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact1d_vec(&outs[0], self.t, output_size_data, output_size_len, scale_factors_data, scale_factors_len)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact2d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact2d(&outs[0], self.t, output_size_data, output_size_len, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact2d_backward :: proc(grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact2d_backward(&outs[0], grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact2d_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact2d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact2d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact2d_out(&outs[0], out.t, self.t, output_size_data, output_size_len, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact2d_vec :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, scale_factors_data: *f64, scale_factors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact2d_vec(&outs[0], self.t, output_size_data, output_size_len, scale_factors_data, scale_factors_len)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact3d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, scales_d_v: f64, scales_d_null: u8, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact3d(&outs[0], self.t, output_size_data, output_size_len, scales_d_v, scales_d_null, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact3d_backward :: proc(grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, scales_d_v: f64, scales_d_null: u8, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact3d_backward(&outs[0], grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, scales_d_v, scales_d_null, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact3d_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, scales_d_v: f64, scales_d_null: u8, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact3d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, scales_d_v, scales_d_null, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact3d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, scales_d_v: f64, scales_d_null: u8, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact3d_out(&outs[0], out.t, self.t, output_size_data, output_size_len, scales_d_v, scales_d_null, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor._upsample_nearest_exact3d_vec :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, scale_factors_data: *f64, scale_factors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__upsample_nearest_exact3d_vec(&outs[0], self.t, output_size_data, output_size_len, scale_factors_data, scale_factors_len)
    return wrap_and_track(outs[0])
}

Tensor._use_cudnn_ctc_loss :: proc(log_probs: Tensor, targets: Tensor, input_lengths_data: *i64, input_lengths_len: i32, target_lengths_data: *i64, target_lengths_len: i32, blank: i64) bool {
    return torch_bindings.atg__use_cudnn_ctc_loss(log_probs.t, targets.t, input_lengths_data, input_lengths_len, target_lengths_data, target_lengths_len, blank)
}

Tensor._use_cudnn_ctc_loss_tensor :: proc(log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor, blank: i64) bool {
    return torch_bindings.atg__use_cudnn_ctc_loss_tensor(log_probs.t, targets.t, input_lengths.t, target_lengths.t, blank)
}

Tensor._use_cudnn_rnn_flatten_weight :: proc() bool {
    return torch_bindings.atg__use_cudnn_rnn_flatten_weight()
}

Tensor._validate_compressed_sparse_indices :: proc(is_crow: bool, compressed_idx: Tensor, plain_idx: Tensor, cdim: i64, dim: i64, nnz: i64) void {
    torch_bindings.atg__validate_compressed_sparse_indices(is_crow, compressed_idx.t, plain_idx.t, cdim, dim, nnz)
}

Tensor._validate_sparse_bsc_tensor_args :: proc(ccol_indices: Tensor, row_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, check_pinning: bool) void {
    torch_bindings.atg__validate_sparse_bsc_tensor_args(ccol_indices.t, row_indices.t, values.t, size_data, size_len, check_pinning)
}

Tensor._validate_sparse_bsr_tensor_args :: proc(crow_indices: Tensor, col_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, check_pinning: bool) void {
    torch_bindings.atg__validate_sparse_bsr_tensor_args(crow_indices.t, col_indices.t, values.t, size_data, size_len, check_pinning)
}

Tensor._validate_sparse_compressed_tensor_args :: proc(compressed_indices: Tensor, plain_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, layout: i8, check_pinning: bool) void {
    torch_bindings.atg__validate_sparse_compressed_tensor_args(compressed_indices.t, plain_indices.t, values.t, size_data, size_len, layout, check_pinning)
}

Tensor._validate_sparse_csc_tensor_args :: proc(ccol_indices: Tensor, row_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, check_pinning: bool) void {
    torch_bindings.atg__validate_sparse_csc_tensor_args(ccol_indices.t, row_indices.t, values.t, size_data, size_len, check_pinning)
}

Tensor._validate_sparse_csr_tensor_args :: proc(crow_indices: Tensor, col_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, check_pinning: bool) void {
    torch_bindings.atg__validate_sparse_csr_tensor_args(crow_indices.t, col_indices.t, values.t, size_data, size_len, check_pinning)
}

Tensor._values :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__values(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._values_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__values_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor._values_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__values_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor._version :: proc(self: *Tensor) i64 {
    return torch_bindings.atg__version(self.t)
}

Tensor._weight_int4pack_mm :: proc(self: *Tensor, mat2: Tensor, qGroupSize: i64, qScaleAndZeros: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__weight_int4pack_mm(&outs[0], self.t, mat2.t, qGroupSize, qScaleAndZeros.t)
    return wrap_and_track(outs[0])
}

Tensor._weight_int4pack_mm_for_cpu :: proc(self: *Tensor, mat2: Tensor, qGroupSize: i64, qScaleAndZeros: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__weight_int4pack_mm_for_cpu(&outs[0], self.t, mat2.t, qGroupSize, qScaleAndZeros.t)
    return wrap_and_track(outs[0])
}

Tensor._weight_int4pack_mm_with_scales_and_zeros :: proc(self: *Tensor, mat2: Tensor, qGroupSize: i64, qScale: Tensor, qZeros: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__weight_int4pack_mm_with_scales_and_zeros(&outs[0], self.t, mat2.t, qGroupSize, qScale.t, qZeros.t)
    return wrap_and_track(outs[0])
}

Tensor._weight_int8pack_mm :: proc(self: *Tensor, mat2: Tensor, scales: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__weight_int8pack_mm(&outs[0], self.t, mat2.t, scales.t)
    return wrap_and_track(outs[0])
}

Tensor._weight_norm :: proc(v: Tensor, g: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__weight_norm(&outs[0], v.t, g.t, dim)
    return wrap_and_track(outs[0])
}

Tensor._weight_norm_differentiable_backward :: proc(grad_w: Tensor, saved_v: Tensor, saved_g: Tensor, saved_norms: Tensor, dim: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__weight_norm_differentiable_backward(&outs[0], grad_w.t, saved_v.t, saved_g.t, saved_norms.t, dim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._weight_norm_interface :: proc(v: Tensor, g: Tensor, dim: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__weight_norm_interface(&outs[0], v.t, g.t, dim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._weight_norm_interface_backward :: proc(grad_w: Tensor, saved_v: Tensor, saved_g: Tensor, saved_norms: Tensor, dim: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__weight_norm_interface_backward(&outs[0], grad_w.t, saved_v.t, saved_g.t, saved_norms.t, dim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._weight_norm_interface_backward_out :: proc(out0: Tensor, out1: Tensor, grad_w: Tensor, saved_v: Tensor, saved_g: Tensor, saved_norms: Tensor, dim: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__weight_norm_interface_backward_out(&outs[0], out0.t, out1.t, grad_w.t, saved_v.t, saved_g.t, saved_norms.t, dim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._weight_norm_interface_out :: proc(out0: Tensor, out1: Tensor, v: Tensor, g: Tensor, dim: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg__weight_norm_interface_out(&outs[0], out0.t, out1.t, v.t, g.t, dim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor._wrapped_linear_prepack :: proc(weight: Tensor, weight_scale: Tensor, weight_zero_point: Tensor, bias: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__wrapped_linear_prepack(&outs[0], weight.t, weight_scale.t, weight_zero_point.t, bias.t)
    return wrap_and_track(outs[0])
}

Tensor._wrapped_quantized_linear_prepacked :: proc(self: *Tensor, input_scale: Tensor, input_zero_point: Tensor, packed_weight: Tensor, output_scale: Tensor, output_zero_point: Tensor, out_channel: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg__wrapped_quantized_linear_prepacked(&outs[0], self.t, input_scale.t, input_zero_point.t, packed_weight.t, output_scale.t, output_zero_point.t, out_channel)
    return wrap_and_track(outs[0])
}

Tensor.abs :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_abs(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.abs_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_abs_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.abs_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_abs_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.absolute :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_absolute(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.absolute_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_absolute_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.absolute_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_absolute_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.acos :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_acos(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.acos_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_acos_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.acos_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_acos_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.acosh :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_acosh(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.acosh_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_acosh_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.acosh_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_acosh_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.adaptive_avg_pool1d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_avg_pool1d(&outs[0], self.t, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor.adaptive_avg_pool1d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_avg_pool1d_out(&outs[0], out.t, self.t, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor.adaptive_avg_pool2d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_avg_pool2d(&outs[0], self.t, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor.adaptive_avg_pool2d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_avg_pool2d_out(&outs[0], out.t, self.t, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor.adaptive_avg_pool3d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_avg_pool3d(&outs[0], self.t, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor.adaptive_avg_pool3d_backward :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_avg_pool3d_backward(&outs[0], grad_input.t, grad_output.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.adaptive_avg_pool3d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_avg_pool3d_out(&outs[0], out.t, self.t, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor.adaptive_max_pool1d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_max_pool1d(&outs[0], self.t, output_size_data, output_size_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.adaptive_max_pool2d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_max_pool2d(&outs[0], self.t, output_size_data, output_size_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.adaptive_max_pool2d_backward :: proc(self: *Tensor, grad_output: Tensor, indices: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_max_pool2d_backward(&outs[0], grad_output.t, self.t, indices.t)
    return wrap_and_track(outs[0])
}

Tensor.adaptive_max_pool2d_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, indices: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_max_pool2d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, indices.t)
    return wrap_and_track(outs[0])
}

Tensor.adaptive_max_pool2d_out :: proc(self: *Tensor, out: Tensor, indices: Tensor, output_size_data: *i64, output_size_len: i32) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_max_pool2d_out(&outs[0], out.t, indices.t, self.t, output_size_data, output_size_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.adaptive_max_pool3d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_max_pool3d(&outs[0], self.t, output_size_data, output_size_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.adaptive_max_pool3d_backward :: proc(self: *Tensor, grad_output: Tensor, indices: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_max_pool3d_backward(&outs[0], grad_output.t, self.t, indices.t)
    return wrap_and_track(outs[0])
}

Tensor.adaptive_max_pool3d_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, indices: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_max_pool3d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, indices.t)
    return wrap_and_track(outs[0])
}

Tensor.adaptive_max_pool3d_out :: proc(self: *Tensor, out: Tensor, indices: Tensor, output_size_data: *i64, output_size_len: i32) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_adaptive_max_pool3d_out(&outs[0], out.t, indices.t, self.t, output_size_data, output_size_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.add :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_add(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.add_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_add_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.add_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_add_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.add_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_add_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.add_scalar_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_add_scalar_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.add_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_add_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.addbmm :: proc(self: *Tensor, batch1: Tensor, batch2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addbmm(&outs[0], self.t, batch1.t, batch2.t)
    return wrap_and_track(outs[0])
}

Tensor.addbmm_ :: proc(self: *Tensor, batch1: Tensor, batch2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addbmm_(&outs[0], self.t, batch1.t, batch2.t)
    return wrap_and_track(outs[0])
}

Tensor.addbmm_out :: proc(self: *Tensor, out: Tensor, batch1: Tensor, batch2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addbmm_out(&outs[0], out.t, self.t, batch1.t, batch2.t)
    return wrap_and_track(outs[0])
}

Tensor.addcdiv :: proc(self: *Tensor, tensor1: Tensor, tensor2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addcdiv(&outs[0], self.t, tensor1.t, tensor2.t)
    return wrap_and_track(outs[0])
}

Tensor.addcdiv_ :: proc(self: *Tensor, tensor1: Tensor, tensor2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addcdiv_(&outs[0], self.t, tensor1.t, tensor2.t)
    return wrap_and_track(outs[0])
}

Tensor.addcdiv_out :: proc(self: *Tensor, out: Tensor, tensor1: Tensor, tensor2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addcdiv_out(&outs[0], out.t, self.t, tensor1.t, tensor2.t)
    return wrap_and_track(outs[0])
}

Tensor.addcmul :: proc(self: *Tensor, tensor1: Tensor, tensor2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addcmul(&outs[0], self.t, tensor1.t, tensor2.t)
    return wrap_and_track(outs[0])
}

Tensor.addcmul_ :: proc(self: *Tensor, tensor1: Tensor, tensor2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addcmul_(&outs[0], self.t, tensor1.t, tensor2.t)
    return wrap_and_track(outs[0])
}

Tensor.addcmul_out :: proc(self: *Tensor, out: Tensor, tensor1: Tensor, tensor2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addcmul_out(&outs[0], out.t, self.t, tensor1.t, tensor2.t)
    return wrap_and_track(outs[0])
}

Tensor.addmm :: proc(self: *Tensor, mat1: Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addmm(&outs[0], self.t, mat1.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor.addmm_ :: proc(self: *Tensor, mat1: Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addmm_(&outs[0], self.t, mat1.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor.addmm_dtype :: proc(self: *Tensor, mat1: Tensor, mat2: Tensor, out_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addmm_dtype(&outs[0], self.t, mat1.t, mat2.t, out_dtype)
    return wrap_and_track(outs[0])
}

Tensor.addmm_dtype_out :: proc(self: *Tensor, out: Tensor, mat1: Tensor, mat2: Tensor, out_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addmm_dtype_out(&outs[0], out.t, self.t, mat1.t, mat2.t, out_dtype)
    return wrap_and_track(outs[0])
}

Tensor.addmm_out :: proc(self: *Tensor, out: Tensor, mat1: Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addmm_out(&outs[0], out.t, self.t, mat1.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor.addmv :: proc(self: *Tensor, mat: Tensor, vec: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addmv(&outs[0], self.t, mat.t, vec.t)
    return wrap_and_track(outs[0])
}

Tensor.addmv_ :: proc(self: *Tensor, mat: Tensor, vec: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addmv_(&outs[0], self.t, mat.t, vec.t)
    return wrap_and_track(outs[0])
}

Tensor.addmv_out :: proc(self: *Tensor, out: Tensor, mat: Tensor, vec: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addmv_out(&outs[0], out.t, self.t, mat.t, vec.t)
    return wrap_and_track(outs[0])
}

Tensor.addr :: proc(self: *Tensor, vec1: Tensor, vec2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addr(&outs[0], self.t, vec1.t, vec2.t)
    return wrap_and_track(outs[0])
}

Tensor.addr_ :: proc(self: *Tensor, vec1: Tensor, vec2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addr_(&outs[0], self.t, vec1.t, vec2.t)
    return wrap_and_track(outs[0])
}

Tensor.addr_out :: proc(self: *Tensor, out: Tensor, vec1: Tensor, vec2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_addr_out(&outs[0], out.t, self.t, vec1.t, vec2.t)
    return wrap_and_track(outs[0])
}

Tensor.adjoint :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_adjoint(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.affine_grid_generator :: proc(theta: Tensor, size_data: *i64, size_len: i32, align_corners: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_affine_grid_generator(&outs[0], theta.t, size_data, size_len, align_corners)
    return wrap_and_track(outs[0])
}

Tensor.affine_grid_generator_backward :: proc(grad: Tensor, size_data: *i64, size_len: i32, align_corners: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_affine_grid_generator_backward(&outs[0], grad.t, size_data, size_len, align_corners)
    return wrap_and_track(outs[0])
}

Tensor.affine_grid_generator_out :: proc(out: Tensor, theta: Tensor, size_data: *i64, size_len: i32, align_corners: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_affine_grid_generator_out(&outs[0], out.t, theta.t, size_data, size_len, align_corners)
    return wrap_and_track(outs[0])
}

Tensor.alias :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_alias(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.alias_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_alias_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.alias_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_alias_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.align_as :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_align_as(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.align_tensors :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) [dyn] Tensor {
    ptr := torch_bindings.atg_align_tensors(tensors_data, tensors_len)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.all :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_all(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.all_all_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_all_all_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.all_dim :: proc(self: *Tensor, dim: i64, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_all_dim(&outs[0], self.t, dim, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.all_dims :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_all_dims(&outs[0], self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.all_dims_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_all_dims_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.all_out :: proc(self: *Tensor, out: Tensor, dim: i64, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_all_out(&outs[0], out.t, self.t, dim, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.allclose :: proc(self: *Tensor, other: Tensor, rtol: f64, atol: f64, equal_nan: bool) bool {
    return torch_bindings.atg_allclose(self.t, other.t, rtol, atol, equal_nan)
}

Tensor.alpha_dropout :: proc(self: *Tensor, p: f64, train: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_alpha_dropout(&outs[0], self.t, p, train)
    return wrap_and_track(outs[0])
}

Tensor.alpha_dropout_ :: proc(self: *Tensor, p: f64, train: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_alpha_dropout_(&outs[0], self.t, p, train)
    return wrap_and_track(outs[0])
}

Tensor.amax :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_amax(&outs[0], self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.amax_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_amax_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.amin :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_amin(&outs[0], self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.amin_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_amin_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.aminmax :: proc(self: *Tensor, dim_v: i64, dim_null: u8, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_aminmax(&outs[0], self.t, dim_v, dim_null, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.aminmax_out :: proc(self: *Tensor, min: Tensor, max: Tensor, dim_v: i64, dim_null: u8, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_aminmax_out(&outs[0], min.t, max.t, self.t, dim_v, dim_null, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.angle :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_angle(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.angle_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_angle_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.r#any :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_any(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.any_all_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_any_all_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.any_dim :: proc(self: *Tensor, dim: i64, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_any_dim(&outs[0], self.t, dim, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.any_dims :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_any_dims(&outs[0], self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.any_dims_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_any_dims_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.any_out :: proc(self: *Tensor, out: Tensor, dim: i64, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_any_out(&outs[0], out.t, self.t, dim, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.arange :: proc(end: torch_bindings.Scalar, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arange(&outs[0], end, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.arange_start :: proc(start: torch_bindings.Scalar, end: torch_bindings.Scalar, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arange_start(&outs[0], start, end, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.arange_start_step :: proc(start: torch_bindings.Scalar, end: torch_bindings.Scalar, step: torch_bindings.Scalar, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arange_start_step(&outs[0], start, end, step, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.arccos :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arccos(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.arccos_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arccos_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.arccos_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arccos_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.arccosh :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arccosh(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.arccosh_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arccosh_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.arccosh_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arccosh_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.arcsin :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arcsin(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.arcsin_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arcsin_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.arcsin_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arcsin_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.arcsinh :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arcsinh(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.arcsinh_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arcsinh_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.arcsinh_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arcsinh_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.arctan :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arctan(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.arctan2 :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arctan2(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.arctan2_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arctan2_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.arctan2_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arctan2_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.arctan_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arctan_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.arctan_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arctan_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.arctanh :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arctanh(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.arctanh_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arctanh_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.arctanh_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_arctanh_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.argmax :: proc(self: *Tensor, dim_v: i64, dim_null: u8, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_argmax(&outs[0], self.t, dim_v, dim_null, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.argmax_out :: proc(self: *Tensor, out: Tensor, dim_v: i64, dim_null: u8, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_argmax_out(&outs[0], out.t, self.t, dim_v, dim_null, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.argmin :: proc(self: *Tensor, dim_v: i64, dim_null: u8, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_argmin(&outs[0], self.t, dim_v, dim_null, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.argmin_out :: proc(self: *Tensor, out: Tensor, dim_v: i64, dim_null: u8, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_argmin_out(&outs[0], out.t, self.t, dim_v, dim_null, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.argsort :: proc(self: *Tensor, dim: i64, descending: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_argsort(&outs[0], self.t, dim, descending)
    return wrap_and_track(outs[0])
}

Tensor.argsort_stable :: proc(self: *Tensor, stable: bool, dim: i64, descending: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_argsort_stable(&outs[0], self.t, stable, dim, descending)
    return wrap_and_track(outs[0])
}

Tensor.argsort_stable_out :: proc(self: *Tensor, out: Tensor, stable: bool, dim: i64, descending: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_argsort_stable_out(&outs[0], out.t, self.t, stable, dim, descending)
    return wrap_and_track(outs[0])
}

Tensor.argwhere :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_argwhere(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.as_strided :: proc(self: *Tensor, size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32, storage_offset_v: i64, storage_offset_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_as_strided(&outs[0], self.t, size_data, size_len, stride_data, stride_len, storage_offset_v, storage_offset_null)
    return wrap_and_track(outs[0])
}

Tensor.as_strided_ :: proc(self: *Tensor, size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32, storage_offset_v: i64, storage_offset_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_as_strided_(&outs[0], self.t, size_data, size_len, stride_data, stride_len, storage_offset_v, storage_offset_null)
    return wrap_and_track(outs[0])
}

Tensor.as_strided_copy :: proc(self: *Tensor, size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32, storage_offset_v: i64, storage_offset_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_as_strided_copy(&outs[0], self.t, size_data, size_len, stride_data, stride_len, storage_offset_v, storage_offset_null)
    return wrap_and_track(outs[0])
}

Tensor.as_strided_copy_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32, storage_offset_v: i64, storage_offset_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_as_strided_copy_out(&outs[0], out.t, self.t, size_data, size_len, stride_data, stride_len, storage_offset_v, storage_offset_null)
    return wrap_and_track(outs[0])
}

Tensor.as_strided_scatter :: proc(self: *Tensor, src: Tensor, size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32, storage_offset_v: i64, storage_offset_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_as_strided_scatter(&outs[0], self.t, src.t, size_data, size_len, stride_data, stride_len, storage_offset_v, storage_offset_null)
    return wrap_and_track(outs[0])
}

Tensor.as_strided_scatter_out :: proc(self: *Tensor, out: Tensor, src: Tensor, size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32, storage_offset_v: i64, storage_offset_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_as_strided_scatter_out(&outs[0], out.t, self.t, src.t, size_data, size_len, stride_data, stride_len, storage_offset_v, storage_offset_null)
    return wrap_and_track(outs[0])
}

Tensor.asin :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_asin(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.asin_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_asin_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.asin_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_asin_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.asinh :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_asinh(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.asinh_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_asinh_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.asinh_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_asinh_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.atan :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_atan(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.atan2 :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_atan2(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.atan2_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_atan2_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.atan2_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_atan2_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.atan_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_atan_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.atan_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_atan_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.atanh :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_atanh(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.atanh_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_atanh_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.atanh_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_atanh_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.atleast_1d :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_atleast_1d(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.atleast_1d_sequence :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) [dyn] Tensor {
    ptr := torch_bindings.atg_atleast_1d_sequence(tensors_data, tensors_len)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.atleast_2d :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_atleast_2d(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.atleast_2d_sequence :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) [dyn] Tensor {
    ptr := torch_bindings.atg_atleast_2d_sequence(tensors_data, tensors_len)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.atleast_3d :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_atleast_3d(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.atleast_3d_sequence :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) [dyn] Tensor {
    ptr := torch_bindings.atg_atleast_3d_sequence(tensors_data, tensors_len)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.avg_pool1d :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, ceil_mode: bool, count_include_pad: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_avg_pool1d(&outs[0], self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, ceil_mode, count_include_pad)
    return wrap_and_track(outs[0])
}

Tensor.avg_pool1d_out :: proc(self: *Tensor, out: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, ceil_mode: bool, count_include_pad: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_avg_pool1d_out(&outs[0], out.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, ceil_mode, count_include_pad)
    return wrap_and_track(outs[0])
}

Tensor.avg_pool2d :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, ceil_mode: bool, count_include_pad: bool, divisor_override_v: i64, divisor_override_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_avg_pool2d(&outs[0], self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, ceil_mode, count_include_pad, divisor_override_v, divisor_override_null)
    return wrap_and_track(outs[0])
}

Tensor.avg_pool2d_backward :: proc(self: *Tensor, grad_output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, ceil_mode: bool, count_include_pad: bool, divisor_override_v: i64, divisor_override_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_avg_pool2d_backward(&outs[0], grad_output.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, ceil_mode, count_include_pad, divisor_override_v, divisor_override_null)
    return wrap_and_track(outs[0])
}

Tensor.avg_pool2d_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, ceil_mode: bool, count_include_pad: bool, divisor_override_v: i64, divisor_override_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_avg_pool2d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, ceil_mode, count_include_pad, divisor_override_v, divisor_override_null)
    return wrap_and_track(outs[0])
}

Tensor.avg_pool2d_out :: proc(self: *Tensor, out: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, ceil_mode: bool, count_include_pad: bool, divisor_override_v: i64, divisor_override_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_avg_pool2d_out(&outs[0], out.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, ceil_mode, count_include_pad, divisor_override_v, divisor_override_null)
    return wrap_and_track(outs[0])
}

Tensor.avg_pool3d :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, ceil_mode: bool, count_include_pad: bool, divisor_override_v: i64, divisor_override_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_avg_pool3d(&outs[0], self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, ceil_mode, count_include_pad, divisor_override_v, divisor_override_null)
    return wrap_and_track(outs[0])
}

Tensor.avg_pool3d_backward :: proc(self: *Tensor, grad_output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, ceil_mode: bool, count_include_pad: bool, divisor_override_v: i64, divisor_override_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_avg_pool3d_backward(&outs[0], grad_output.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, ceil_mode, count_include_pad, divisor_override_v, divisor_override_null)
    return wrap_and_track(outs[0])
}

Tensor.avg_pool3d_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, ceil_mode: bool, count_include_pad: bool, divisor_override_v: i64, divisor_override_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_avg_pool3d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, ceil_mode, count_include_pad, divisor_override_v, divisor_override_null)
    return wrap_and_track(outs[0])
}

Tensor.avg_pool3d_out :: proc(self: *Tensor, out: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, ceil_mode: bool, count_include_pad: bool, divisor_override_v: i64, divisor_override_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_avg_pool3d_out(&outs[0], out.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, ceil_mode, count_include_pad, divisor_override_v, divisor_override_null)
    return wrap_and_track(outs[0])
}

Tensor.baddbmm :: proc(self: *Tensor, batch1: Tensor, batch2: Tensor, beta: torch_bindings.Scalar, alpha: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_baddbmm(&outs[0], self.t, batch1.t, batch2.t, beta, alpha)
    return wrap_and_track(outs[0])
}

Tensor.baddbmm_ :: proc(self: *Tensor, batch1: Tensor, batch2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_baddbmm_(&outs[0], self.t, batch1.t, batch2.t)
    return wrap_and_track(outs[0])
}

Tensor.baddbmm_dtype :: proc(self: *Tensor, batch1: Tensor, batch2: Tensor, out_dtype: i32, beta: torch_bindings.Scalar, alpha: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_baddbmm_dtype(&outs[0], self.t, batch1.t, batch2.t, out_dtype, beta, alpha)
    return wrap_and_track(outs[0])
}

Tensor.baddbmm_dtype_out :: proc(self: *Tensor, out: Tensor, batch1: Tensor, batch2: Tensor, out_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_baddbmm_dtype_out(&outs[0], out.t, self.t, batch1.t, batch2.t, out_dtype)
    return wrap_and_track(outs[0])
}

Tensor.baddbmm_out :: proc(self: *Tensor, out: Tensor, batch1: Tensor, batch2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_baddbmm_out(&outs[0], out.t, self.t, batch1.t, batch2.t)
    return wrap_and_track(outs[0])
}

Tensor.bartlett_window :: proc(window_length: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bartlett_window(&outs[0], window_length, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.bartlett_window_out :: proc(out: Tensor, window_length: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bartlett_window_out(&outs[0], out.t, window_length)
    return wrap_and_track(outs[0])
}

Tensor.bartlett_window_periodic :: proc(window_length: i64, periodic: bool, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bartlett_window_periodic(&outs[0], window_length, periodic, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.bartlett_window_periodic_out :: proc(out: Tensor, window_length: i64, periodic: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bartlett_window_periodic_out(&outs[0], out.t, window_length, periodic)
    return wrap_and_track(outs[0])
}

Tensor.batch_norm :: proc(self: *Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, training: bool, momentum: f64, eps: f64, cudnn_enabled: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm(&outs[0], self.t, weight.t, bias.t, running_mean.t, running_var.t, training, momentum, eps, cudnn_enabled)
    return wrap_and_track(outs[0])
}

Tensor.batch_norm_backward_elemt :: proc(self: *Tensor, grad_out: Tensor, mean: Tensor, invstd: Tensor, weight: Tensor, sum_dy: Tensor, sum_dy_xmu: Tensor, count: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm_backward_elemt(&outs[0], grad_out.t, self.t, mean.t, invstd.t, weight.t, sum_dy.t, sum_dy_xmu.t, count.t)
    return wrap_and_track(outs[0])
}

Tensor.batch_norm_backward_elemt_out :: proc(self: *Tensor, out: Tensor, grad_out: Tensor, mean: Tensor, invstd: Tensor, weight: Tensor, sum_dy: Tensor, sum_dy_xmu: Tensor, count: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm_backward_elemt_out(&outs[0], out.t, grad_out.t, self.t, mean.t, invstd.t, weight.t, sum_dy.t, sum_dy_xmu.t, count.t)
    return wrap_and_track(outs[0])
}

Tensor.batch_norm_backward_reduce :: proc(self: *Tensor, grad_out: Tensor, mean: Tensor, invstd: Tensor, weight: Tensor, input_g: bool, weight_g: bool, bias_g: bool) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm_backward_reduce(&outs[0], grad_out.t, self.t, mean.t, invstd.t, weight.t, input_g, weight_g, bias_g)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor.batch_norm_backward_reduce_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, out3: Tensor, grad_out: Tensor, mean: Tensor, invstd: Tensor, weight: Tensor, input_g: bool, weight_g: bool, bias_g: bool) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm_backward_reduce_out(&outs[0], out0.t, out1.t, out2.t, out3.t, grad_out.t, self.t, mean.t, invstd.t, weight.t, input_g, weight_g, bias_g)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor.batch_norm_elemt :: proc(self: *Tensor, weight: Tensor, bias: Tensor, mean: Tensor, invstd: Tensor, eps: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm_elemt(&outs[0], self.t, weight.t, bias.t, mean.t, invstd.t, eps)
    return wrap_and_track(outs[0])
}

Tensor.batch_norm_elemt_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor, mean: Tensor, invstd: Tensor, eps: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm_elemt_out(&outs[0], out.t, self.t, weight.t, bias.t, mean.t, invstd.t, eps)
    return wrap_and_track(outs[0])
}

Tensor.batch_norm_gather_stats :: proc(self: *Tensor, mean: Tensor, invstd: Tensor, running_mean: Tensor, running_var: Tensor, momentum: f64, eps: f64, count: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm_gather_stats(&outs[0], self.t, mean.t, invstd.t, running_mean.t, running_var.t, momentum, eps, count)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.batch_norm_gather_stats_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, mean: Tensor, invstd: Tensor, running_mean: Tensor, running_var: Tensor, momentum: f64, eps: f64, count: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm_gather_stats_out(&outs[0], out0.t, out1.t, self.t, mean.t, invstd.t, running_mean.t, running_var.t, momentum, eps, count)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.batch_norm_gather_stats_with_counts :: proc(self: *Tensor, mean: Tensor, invstd: Tensor, running_mean: Tensor, running_var: Tensor, momentum: f64, eps: f64, counts: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm_gather_stats_with_counts(&outs[0], self.t, mean.t, invstd.t, running_mean.t, running_var.t, momentum, eps, counts.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.batch_norm_gather_stats_with_counts_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, mean: Tensor, invstd: Tensor, running_mean: Tensor, running_var: Tensor, momentum: f64, eps: f64, counts: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm_gather_stats_with_counts_out(&outs[0], out0.t, out1.t, self.t, mean.t, invstd.t, running_mean.t, running_var.t, momentum, eps, counts.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.batch_norm_stats :: proc(self: *Tensor, eps: f64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm_stats(&outs[0], self.t, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.batch_norm_stats_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, eps: f64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm_stats_out(&outs[0], out0.t, out1.t, self.t, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.batch_norm_update_stats :: proc(self: *Tensor, running_mean: Tensor, running_var: Tensor, momentum: f64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm_update_stats(&outs[0], self.t, running_mean.t, running_var.t, momentum)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.batch_norm_update_stats_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, running_mean: Tensor, running_var: Tensor, momentum: f64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_batch_norm_update_stats_out(&outs[0], out0.t, out1.t, self.t, running_mean.t, running_var.t, momentum)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.bernoulli :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bernoulli(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.bernoulli_ :: proc(self: *Tensor, p: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bernoulli_(&outs[0], self.t, p.t)
    return wrap_and_track(outs[0])
}

Tensor.bernoulli_float_ :: proc(self: *Tensor, p: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bernoulli_float_(&outs[0], self.t, p)
    return wrap_and_track(outs[0])
}

Tensor.bernoulli_p :: proc(self: *Tensor, p: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bernoulli_p(&outs[0], self.t, p)
    return wrap_and_track(outs[0])
}

Tensor.bernoulli_tensor :: proc(self: *Tensor, p: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bernoulli_tensor(&outs[0], self.t, p.t)
    return wrap_and_track(outs[0])
}

Tensor.bilinear :: proc(input1: Tensor, input2: Tensor, weight: Tensor, bias: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bilinear(&outs[0], input1.t, input2.t, weight.t, bias.t)
    return wrap_and_track(outs[0])
}

Tensor.binary_cross_entropy :: proc(self: *Tensor, target: Tensor, weight: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_binary_cross_entropy(&outs[0], self.t, target.t, weight.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.binary_cross_entropy_backward :: proc(self: *Tensor, grad_output: Tensor, target: Tensor, weight: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_binary_cross_entropy_backward(&outs[0], grad_output.t, self.t, target.t, weight.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.binary_cross_entropy_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, target: Tensor, weight: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_binary_cross_entropy_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, target.t, weight.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.binary_cross_entropy_out :: proc(self: *Tensor, out: Tensor, target: Tensor, weight: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_binary_cross_entropy_out(&outs[0], out.t, self.t, target.t, weight.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.binary_cross_entropy_with_logits :: proc(self: *Tensor, target: Tensor, weight: Tensor, pos_weight: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_binary_cross_entropy_with_logits(&outs[0], self.t, target.t, weight.t, pos_weight.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.binary_cross_entropy_with_logits_out :: proc(self: *Tensor, out: Tensor, target: Tensor, weight: Tensor, pos_weight: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_binary_cross_entropy_with_logits_out(&outs[0], out.t, self.t, target.t, weight.t, pos_weight.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.bincount :: proc(self: *Tensor, weights: Tensor, minlength: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bincount(&outs[0], self.t, weights.t, minlength)
    return wrap_and_track(outs[0])
}

Tensor.bincount_out :: proc(self: *Tensor, out: Tensor, weights: Tensor, minlength: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bincount_out(&outs[0], out.t, self.t, weights.t, minlength)
    return wrap_and_track(outs[0])
}

Tensor.binomial :: proc(count: Tensor, prob: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_binomial(&outs[0], count.t, prob.t)
    return wrap_and_track(outs[0])
}

Tensor.binomial_out :: proc(out: Tensor, count: Tensor, prob: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_binomial_out(&outs[0], out.t, count.t, prob.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_and :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_and(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_and_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_and_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_and_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_and_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_and_scalar_tensor :: proc(self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_and_scalar_tensor(&outs[0], self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_and_scalar_tensor_out :: proc(out: Tensor, self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_and_scalar_tensor_out(&outs[0], out.t, self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_and_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_and_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_and_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_and_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_and_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_and_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_left_shift :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_left_shift(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_left_shift_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_left_shift_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_left_shift_scalar_tensor :: proc(self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_left_shift_scalar_tensor(&outs[0], self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_left_shift_scalar_tensor_out :: proc(out: Tensor, self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_left_shift_scalar_tensor_out(&outs[0], out.t, self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_left_shift_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_left_shift_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_left_shift_tensor_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_left_shift_tensor_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_left_shift_tensor_scalar_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_left_shift_tensor_scalar_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_left_shift_tensor_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_left_shift_tensor_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_not :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_not(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_not_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_not_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_not_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_not_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_or :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_or(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_or_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_or_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_or_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_or_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_or_scalar_tensor :: proc(self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_or_scalar_tensor(&outs[0], self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_or_scalar_tensor_out :: proc(out: Tensor, self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_or_scalar_tensor_out(&outs[0], out.t, self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_or_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_or_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_or_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_or_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_or_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_or_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_right_shift :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_right_shift(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_right_shift_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_right_shift_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_right_shift_scalar_tensor :: proc(self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_right_shift_scalar_tensor(&outs[0], self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_right_shift_scalar_tensor_out :: proc(out: Tensor, self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_right_shift_scalar_tensor_out(&outs[0], out.t, self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_right_shift_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_right_shift_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_right_shift_tensor_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_right_shift_tensor_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_right_shift_tensor_scalar_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_right_shift_tensor_scalar_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_right_shift_tensor_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_right_shift_tensor_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_xor :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_xor(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_xor_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_xor_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_xor_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_xor_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_xor_scalar_tensor :: proc(self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_xor_scalar_tensor(&outs[0], self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_xor_scalar_tensor_out :: proc(out: Tensor, self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_xor_scalar_tensor_out(&outs[0], out.t, self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_xor_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_xor_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_xor_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_xor_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.bitwise_xor_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bitwise_xor_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.blackman_window :: proc(window_length: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_blackman_window(&outs[0], window_length, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.blackman_window_out :: proc(out: Tensor, window_length: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_blackman_window_out(&outs[0], out.t, window_length)
    return wrap_and_track(outs[0])
}

Tensor.blackman_window_periodic :: proc(window_length: i64, periodic: bool, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_blackman_window_periodic(&outs[0], window_length, periodic, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.blackman_window_periodic_out :: proc(out: Tensor, window_length: i64, periodic: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_blackman_window_periodic_out(&outs[0], out.t, window_length, periodic)
    return wrap_and_track(outs[0])
}

Tensor.block_diag :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_block_diag(&outs[0], tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.block_diag_out :: proc(out: Tensor, tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_block_diag_out(&outs[0], out.t, tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.bmm :: proc(self: *Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bmm(&outs[0], self.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor.bmm_dtype :: proc(self: *Tensor, mat2: Tensor, out_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bmm_dtype(&outs[0], self.t, mat2.t, out_dtype)
    return wrap_and_track(outs[0])
}

Tensor.bmm_dtype_out :: proc(self: *Tensor, out: Tensor, mat2: Tensor, out_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bmm_dtype_out(&outs[0], out.t, self.t, mat2.t, out_dtype)
    return wrap_and_track(outs[0])
}

Tensor.bmm_out :: proc(self: *Tensor, out: Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bmm_out(&outs[0], out.t, self.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor.broadcast_tensors :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) [dyn] Tensor {
    ptr := torch_bindings.atg_broadcast_tensors(tensors_data, tensors_len)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.broadcast_to :: proc(self: *Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_broadcast_to(&outs[0], self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.bucketize :: proc(self: *Tensor, boundaries: Tensor, out_int32: bool, right: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bucketize(&outs[0], self.t, boundaries.t, out_int32, right)
    return wrap_and_track(outs[0])
}

Tensor.bucketize_scalar :: proc(self_scalar: torch_bindings.Scalar, boundaries: Tensor, out_int32: bool, right: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bucketize_scalar(&outs[0], self_scalar, boundaries.t, out_int32, right)
    return wrap_and_track(outs[0])
}

Tensor.bucketize_scalar_out :: proc(out: Tensor, self_scalar: torch_bindings.Scalar, boundaries: Tensor, out_int32: bool, right: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bucketize_scalar_out(&outs[0], out.t, self_scalar, boundaries.t, out_int32, right)
    return wrap_and_track(outs[0])
}

Tensor.bucketize_tensor_out :: proc(self: *Tensor, out: Tensor, boundaries: Tensor, out_int32: bool, right: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_bucketize_tensor_out(&outs[0], out.t, self.t, boundaries.t, out_int32, right)
    return wrap_and_track(outs[0])
}

Tensor.can_cast :: proc(from_: i32, to: i32) bool {
    return torch_bindings.atg_can_cast(from_, to)
}

Tensor.cartesian_prod :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cartesian_prod(&outs[0], tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.cat :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cat(&outs[0], tensors_data, tensors_len, dim)
    return wrap_and_track(outs[0])
}

Tensor.cat_out :: proc(out: Tensor, tensors_data: *torch_bindings.Tensor, tensors_len: i32, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cat_out(&outs[0], out.t, tensors_data, tensors_len, dim)
    return wrap_and_track(outs[0])
}

Tensor.cauchy :: proc(self: *Tensor, median: f64, sigma: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cauchy(&outs[0], self.t, median, sigma)
    return wrap_and_track(outs[0])
}

Tensor.cauchy_ :: proc(self: *Tensor, median: f64, sigma: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cauchy_(&outs[0], self.t, median, sigma)
    return wrap_and_track(outs[0])
}

Tensor.cauchy_out :: proc(self: *Tensor, out: Tensor, median: f64, sigma: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cauchy_out(&outs[0], out.t, self.t, median, sigma)
    return wrap_and_track(outs[0])
}

Tensor.ccol_indices :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ccol_indices(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.ccol_indices_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ccol_indices_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.ccol_indices_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ccol_indices_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.cdist :: proc(x1: Tensor, x2: Tensor, p: f64, compute_mode_v: i64, compute_mode_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cdist(&outs[0], x1.t, x2.t, p, compute_mode_v, compute_mode_null)
    return wrap_and_track(outs[0])
}

Tensor.ceil :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ceil(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.ceil_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ceil_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.ceil_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ceil_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.celu :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_celu(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.celu_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_celu_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.celu_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_celu_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.chain_matmul :: proc(matrices_data: *torch_bindings.Tensor, matrices_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_chain_matmul(&outs[0], matrices_data, matrices_len)
    return wrap_and_track(outs[0])
}

Tensor.chain_matmul_out :: proc(out: Tensor, matrices_data: *torch_bindings.Tensor, matrices_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_chain_matmul_out(&outs[0], out.t, matrices_data, matrices_len)
    return wrap_and_track(outs[0])
}

Tensor.chalf :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_chalf(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.channel_shuffle :: proc(self: *Tensor, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_channel_shuffle(&outs[0], self.t, groups)
    return wrap_and_track(outs[0])
}

Tensor.channel_shuffle_out :: proc(self: *Tensor, out: Tensor, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_channel_shuffle_out(&outs[0], out.t, self.t, groups)
    return wrap_and_track(outs[0])
}

Tensor.cholesky :: proc(self: *Tensor, upper: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cholesky(&outs[0], self.t, upper)
    return wrap_and_track(outs[0])
}

Tensor.cholesky_inverse :: proc(self: *Tensor, upper: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cholesky_inverse(&outs[0], self.t, upper)
    return wrap_and_track(outs[0])
}

Tensor.cholesky_inverse_out :: proc(self: *Tensor, out: Tensor, upper: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cholesky_inverse_out(&outs[0], out.t, self.t, upper)
    return wrap_and_track(outs[0])
}

Tensor.cholesky_out :: proc(self: *Tensor, out: Tensor, upper: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cholesky_out(&outs[0], out.t, self.t, upper)
    return wrap_and_track(outs[0])
}

Tensor.cholesky_solve :: proc(self: *Tensor, input2: Tensor, upper: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cholesky_solve(&outs[0], self.t, input2.t, upper)
    return wrap_and_track(outs[0])
}

Tensor.cholesky_solve_out :: proc(self: *Tensor, out: Tensor, input2: Tensor, upper: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cholesky_solve_out(&outs[0], out.t, self.t, input2.t, upper)
    return wrap_and_track(outs[0])
}

Tensor.choose_qparams_optimized :: proc(self: *Tensor, numel: i64, n_bins: i64, ratio: f64, bit_width: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_choose_qparams_optimized(&outs[0], self.t, numel, n_bins, ratio, bit_width)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.chunk :: proc(self: *Tensor, chunks: i64, dim: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_chunk(self.t, chunks, dim)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.clamp :: proc(self: *Tensor, min: torch_bindings.Scalar, max: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp(&outs[0], self.t, min, max)
    return wrap_and_track(outs[0])
}

Tensor.clamp_ :: proc(self: *Tensor, min: torch_bindings.Scalar, max: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_(&outs[0], self.t, min, max)
    return wrap_and_track(outs[0])
}

Tensor.clamp_max :: proc(self: *Tensor, max: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_max(&outs[0], self.t, max)
    return wrap_and_track(outs[0])
}

Tensor.clamp_max_ :: proc(self: *Tensor, max: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_max_(&outs[0], self.t, max)
    return wrap_and_track(outs[0])
}

Tensor.clamp_max_out :: proc(self: *Tensor, out: Tensor, max: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_max_out(&outs[0], out.t, self.t, max)
    return wrap_and_track(outs[0])
}

Tensor.clamp_max_tensor :: proc(self: *Tensor, max: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_max_tensor(&outs[0], self.t, max.t)
    return wrap_and_track(outs[0])
}

Tensor.clamp_max_tensor_ :: proc(self: *Tensor, max: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_max_tensor_(&outs[0], self.t, max.t)
    return wrap_and_track(outs[0])
}

Tensor.clamp_max_tensor_out :: proc(self: *Tensor, out: Tensor, max: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_max_tensor_out(&outs[0], out.t, self.t, max.t)
    return wrap_and_track(outs[0])
}

Tensor.clamp_min :: proc(self: *Tensor, min: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_min(&outs[0], self.t, min)
    return wrap_and_track(outs[0])
}

Tensor.clamp_min_ :: proc(self: *Tensor, min: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_min_(&outs[0], self.t, min)
    return wrap_and_track(outs[0])
}

Tensor.clamp_min_out :: proc(self: *Tensor, out: Tensor, min: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_min_out(&outs[0], out.t, self.t, min)
    return wrap_and_track(outs[0])
}

Tensor.clamp_min_tensor :: proc(self: *Tensor, min: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_min_tensor(&outs[0], self.t, min.t)
    return wrap_and_track(outs[0])
}

Tensor.clamp_min_tensor_ :: proc(self: *Tensor, min: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_min_tensor_(&outs[0], self.t, min.t)
    return wrap_and_track(outs[0])
}

Tensor.clamp_min_tensor_out :: proc(self: *Tensor, out: Tensor, min: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_min_tensor_out(&outs[0], out.t, self.t, min.t)
    return wrap_and_track(outs[0])
}

Tensor.clamp_out :: proc(self: *Tensor, out: Tensor, min: torch_bindings.Scalar, max: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_out(&outs[0], out.t, self.t, min, max)
    return wrap_and_track(outs[0])
}

Tensor.clamp_tensor :: proc(self: *Tensor, min: Tensor, max: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_tensor(&outs[0], self.t, min.t, max.t)
    return wrap_and_track(outs[0])
}

Tensor.clamp_tensor_ :: proc(self: *Tensor, min: Tensor, max: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_tensor_(&outs[0], self.t, min.t, max.t)
    return wrap_and_track(outs[0])
}

Tensor.clamp_tensor_out :: proc(self: *Tensor, out: Tensor, min: Tensor, max: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clamp_tensor_out(&outs[0], out.t, self.t, min.t, max.t)
    return wrap_and_track(outs[0])
}

Tensor.clip :: proc(self: *Tensor, min: torch_bindings.Scalar, max: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clip(&outs[0], self.t, min, max)
    return wrap_and_track(outs[0])
}

Tensor.clip_ :: proc(self: *Tensor, min: torch_bindings.Scalar, max: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clip_(&outs[0], self.t, min, max)
    return wrap_and_track(outs[0])
}

Tensor.clip_out :: proc(self: *Tensor, out: Tensor, min: torch_bindings.Scalar, max: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clip_out(&outs[0], out.t, self.t, min, max)
    return wrap_and_track(outs[0])
}

Tensor.clip_tensor :: proc(self: *Tensor, min: Tensor, max: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clip_tensor(&outs[0], self.t, min.t, max.t)
    return wrap_and_track(outs[0])
}

Tensor.clip_tensor_ :: proc(self: *Tensor, min: Tensor, max: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clip_tensor_(&outs[0], self.t, min.t, max.t)
    return wrap_and_track(outs[0])
}

Tensor.clip_tensor_out :: proc(self: *Tensor, out: Tensor, min: Tensor, max: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clip_tensor_out(&outs[0], out.t, self.t, min.t, max.t)
    return wrap_and_track(outs[0])
}

Tensor.clone :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_clone(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.coalesce :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_coalesce(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.col2im :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, kernel_size_data: *i64, kernel_size_len: i32, dilation_data: *i64, dilation_len: i32, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_col2im(&outs[0], self.t, output_size_data, output_size_len, kernel_size_data, kernel_size_len, dilation_data, dilation_len, padding_data, padding_len, stride_data, stride_len)
    return wrap_and_track(outs[0])
}

Tensor.col2im_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, kernel_size_data: *i64, kernel_size_len: i32, dilation_data: *i64, dilation_len: i32, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_col2im_out(&outs[0], out.t, self.t, output_size_data, output_size_len, kernel_size_data, kernel_size_len, dilation_data, dilation_len, padding_data, padding_len, stride_data, stride_len)
    return wrap_and_track(outs[0])
}

Tensor.col_indices :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_col_indices(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.col_indices_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_col_indices_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.col_indices_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_col_indices_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.column_stack :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_column_stack(&outs[0], tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.column_stack_out :: proc(out: Tensor, tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_column_stack_out(&outs[0], out.t, tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.combinations :: proc(self: *Tensor, r: i64, with_replacement: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_combinations(&outs[0], self.t, r, with_replacement)
    return wrap_and_track(outs[0])
}

Tensor.r#complex :: proc(real: Tensor, imag: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_complex(&outs[0], real.t, imag.t)
    return wrap_and_track(outs[0])
}

Tensor.complex_out :: proc(out: Tensor, real: Tensor, imag: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_complex_out(&outs[0], out.t, real.t, imag.t)
    return wrap_and_track(outs[0])
}

Tensor.concat :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_concat(&outs[0], tensors_data, tensors_len, dim)
    return wrap_and_track(outs[0])
}

Tensor.concat_out :: proc(out: Tensor, tensors_data: *torch_bindings.Tensor, tensors_len: i32, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_concat_out(&outs[0], out.t, tensors_data, tensors_len, dim)
    return wrap_and_track(outs[0])
}

Tensor.concatenate :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_concatenate(&outs[0], tensors_data, tensors_len, dim)
    return wrap_and_track(outs[0])
}

Tensor.concatenate_out :: proc(out: Tensor, tensors_data: *torch_bindings.Tensor, tensors_len: i32, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_concatenate_out(&outs[0], out.t, tensors_data, tensors_len, dim)
    return wrap_and_track(outs[0])
}

Tensor.conj :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conj(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.conj_physical :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conj_physical(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.conj_physical_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conj_physical_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.conj_physical_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conj_physical_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.constant_pad_nd :: proc(self: *Tensor, pad_data: *i64, pad_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_constant_pad_nd(&outs[0], self.t, pad_data, pad_len)
    return wrap_and_track(outs[0])
}

Tensor.constant_pad_nd_out :: proc(self: *Tensor, out: Tensor, pad_data: *i64, pad_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_constant_pad_nd_out(&outs[0], out.t, self.t, pad_data, pad_len)
    return wrap_and_track(outs[0])
}

Tensor.contiguous :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_contiguous(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.conv1d :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conv1d(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.conv1d_padding :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_ptr: *u8, padding_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conv1d_padding(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_ptr, padding_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.conv2d :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conv2d(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.conv2d_padding :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_ptr: *u8, padding_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conv2d_padding(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_ptr, padding_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.conv3d :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conv3d(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.conv3d_padding :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_ptr: *u8, padding_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conv3d_padding(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_ptr, padding_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.conv_depthwise3d :: proc(self: *Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conv_depthwise3d(&outs[0], self.t, weight.t, kernel_size_data, kernel_size_len, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor.conv_depthwise3d_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conv_depthwise3d_out(&outs[0], out.t, self.t, weight.t, kernel_size_data, kernel_size_len, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor.conv_tbc :: proc(self: *Tensor, weight: Tensor, bias: Tensor, pad: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conv_tbc(&outs[0], self.t, weight.t, bias.t, pad)
    return wrap_and_track(outs[0])
}

Tensor.conv_tbc_backward :: proc(self: *Tensor, input: Tensor, weight: Tensor, bias: Tensor, pad: i64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_conv_tbc_backward(&outs[0], self.t, input.t, weight.t, bias.t, pad)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.conv_tbc_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor, pad: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conv_tbc_out(&outs[0], out.t, self.t, weight.t, bias.t, pad)
    return wrap_and_track(outs[0])
}

Tensor.conv_transpose1d :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, output_padding_data: *i64, output_padding_len: i32, groups: i64, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conv_transpose1d(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, output_padding_data, output_padding_len, groups, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor.conv_transpose2d :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, output_padding_data: *i64, output_padding_len: i32, groups: i64, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conv_transpose2d(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, output_padding_data, output_padding_len, groups, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor.conv_transpose3d :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, output_padding_data: *i64, output_padding_len: i32, groups: i64, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_conv_transpose3d(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, output_padding_data, output_padding_len, groups, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor.convolution :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, transposed: bool, output_padding_data: *i64, output_padding_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_convolution(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, transposed, output_padding_data, output_padding_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.convolution_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, transposed: bool, output_padding_data: *i64, output_padding_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_convolution_out(&outs[0], out.t, self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, transposed, output_padding_data, output_padding_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.convolution_overrideable :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, transposed: bool, output_padding_data: *i64, output_padding_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_convolution_overrideable(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, transposed, output_padding_data, output_padding_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.convolution_overrideable_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, transposed: bool, output_padding_data: *i64, output_padding_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_convolution_overrideable_out(&outs[0], out.t, self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, transposed, output_padding_data, output_padding_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.copy_sparse_to_sparse :: proc(self: *Tensor, src: Tensor, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_copy_sparse_to_sparse(&outs[0], self.t, src.t, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor.copy_sparse_to_sparse_ :: proc(self: *Tensor, src: Tensor, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_copy_sparse_to_sparse_(&outs[0], self.t, src.t, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor.copy_sparse_to_sparse_out :: proc(self: *Tensor, out: Tensor, src: Tensor, non_blocking: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_copy_sparse_to_sparse_out(&outs[0], out.t, self.t, src.t, non_blocking)
    return wrap_and_track(outs[0])
}

Tensor.copysign :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_copysign(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.copysign_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_copysign_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.copysign_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_copysign_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.copysign_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_copysign_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.copysign_scalar_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_copysign_scalar_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.copysign_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_copysign_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.corrcoef :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_corrcoef(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.cos :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cos(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.cos_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cos_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.cos_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cos_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.cosh :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cosh(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.cosh_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cosh_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.cosh_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cosh_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.cosine_embedding_loss :: proc(input1: Tensor, input2: Tensor, target: Tensor, margin: f64, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cosine_embedding_loss(&outs[0], input1.t, input2.t, target.t, margin, reduction)
    return wrap_and_track(outs[0])
}

Tensor.cosine_similarity :: proc(x1: Tensor, x2: Tensor, dim: i64, eps: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cosine_similarity(&outs[0], x1.t, x2.t, dim, eps)
    return wrap_and_track(outs[0])
}

Tensor.count_nonzero :: proc(self: *Tensor, dim_v: i64, dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_count_nonzero(&outs[0], self.t, dim_v, dim_null)
    return wrap_and_track(outs[0])
}

Tensor.count_nonzero_dim_intlist :: proc(self: *Tensor, dim_data: *i64, dim_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_count_nonzero_dim_intlist(&outs[0], self.t, dim_data, dim_len)
    return wrap_and_track(outs[0])
}

Tensor.count_nonzero_dim_intlist_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_count_nonzero_dim_intlist_out(&outs[0], out.t, self.t, dim_data, dim_len)
    return wrap_and_track(outs[0])
}

Tensor.count_nonzero_out :: proc(self: *Tensor, out: Tensor, dim_v: i64, dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_count_nonzero_out(&outs[0], out.t, self.t, dim_v, dim_null)
    return wrap_and_track(outs[0])
}

Tensor.cov :: proc(self: *Tensor, correction: i64, fweights: Tensor, aweights: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cov(&outs[0], self.t, correction, fweights.t, aweights.t)
    return wrap_and_track(outs[0])
}

Tensor.cross :: proc(self: *Tensor, other: Tensor, dim_v: i64, dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cross(&outs[0], self.t, other.t, dim_v, dim_null)
    return wrap_and_track(outs[0])
}

Tensor.cross_entropy_loss :: proc(self: *Tensor, target: Tensor, weight: Tensor, reduction: i64, ignore_index: i64, label_smoothing: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cross_entropy_loss(&outs[0], self.t, target.t, weight.t, reduction, ignore_index, label_smoothing)
    return wrap_and_track(outs[0])
}

Tensor.cross_out :: proc(self: *Tensor, out: Tensor, other: Tensor, dim_v: i64, dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cross_out(&outs[0], out.t, self.t, other.t, dim_v, dim_null)
    return wrap_and_track(outs[0])
}

Tensor.crow_indices :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_crow_indices(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.crow_indices_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_crow_indices_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.crow_indices_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_crow_indices_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.ctc_loss :: proc(log_probs: Tensor, targets: Tensor, input_lengths_data: *i64, input_lengths_len: i32, target_lengths_data: *i64, target_lengths_len: i32, blank: i64, reduction: i64, zero_infinity: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ctc_loss(&outs[0], log_probs.t, targets.t, input_lengths_data, input_lengths_len, target_lengths_data, target_lengths_len, blank, reduction, zero_infinity)
    return wrap_and_track(outs[0])
}

Tensor.ctc_loss_tensor :: proc(log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor, blank: i64, reduction: i64, zero_infinity: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ctc_loss_tensor(&outs[0], log_probs.t, targets.t, input_lengths.t, target_lengths.t, blank, reduction, zero_infinity)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_affine_grid_generator :: proc(theta: Tensor, n: i64, C: i64, H: i64, W: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_affine_grid_generator(&outs[0], theta.t, n, C, H, W)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_affine_grid_generator_backward :: proc(grad: Tensor, n: i64, C: i64, H: i64, W: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_affine_grid_generator_backward(&outs[0], grad.t, n, C, H, W)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_affine_grid_generator_backward_out :: proc(out: Tensor, grad: Tensor, n: i64, C: i64, H: i64, W: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_affine_grid_generator_backward_out(&outs[0], out.t, grad.t, n, C, H, W)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_affine_grid_generator_out :: proc(out: Tensor, theta: Tensor, n: i64, C: i64, H: i64, W: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_affine_grid_generator_out(&outs[0], out.t, theta.t, n, C, H, W)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_batch_norm :: proc(self: *Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, training: bool, exponential_average_factor: f64, epsilon: f64) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_batch_norm(&outs[0], self.t, weight.t, bias.t, running_mean.t, running_var.t, training, exponential_average_factor, epsilon)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor.cudnn_batch_norm_backward :: proc(self: *Tensor, grad_output: Tensor, weight: Tensor, running_mean: Tensor, running_var: Tensor, save_mean: Tensor, save_var: Tensor, epsilon: f64, reserveSpace: Tensor) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_batch_norm_backward(&outs[0], self.t, grad_output.t, weight.t, running_mean.t, running_var.t, save_mean.t, save_var.t, epsilon, reserveSpace.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.cudnn_batch_norm_backward_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, grad_output: Tensor, weight: Tensor, running_mean: Tensor, running_var: Tensor, save_mean: Tensor, save_var: Tensor, epsilon: f64, reserveSpace: Tensor) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_batch_norm_backward_out(&outs[0], out0.t, out1.t, out2.t, self.t, grad_output.t, weight.t, running_mean.t, running_var.t, save_mean.t, save_var.t, epsilon, reserveSpace.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.cudnn_batch_norm_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, out3: Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, training: bool, exponential_average_factor: f64, epsilon: f64) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_batch_norm_out(&outs[0], out0.t, out1.t, out2.t, out3.t, self.t, weight.t, bias.t, running_mean.t, running_var.t, training, exponential_average_factor, epsilon)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor.cudnn_convolution :: proc(self: *Tensor, weight: Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64, benchmark: bool, deterministic: bool, allow_tf32: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_convolution(&outs[0], self.t, weight.t, padding_data, padding_len, stride_data, stride_len, dilation_data, dilation_len, groups, benchmark, deterministic, allow_tf32)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_convolution_add_relu :: proc(self: *Tensor, weight: Tensor, z: Tensor, alpha: torch_bindings.Scalar, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_convolution_add_relu(&outs[0], self.t, weight.t, z.t, alpha, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_convolution_add_relu_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, z: Tensor, alpha: torch_bindings.Scalar, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_convolution_add_relu_out(&outs[0], out.t, self.t, weight.t, z.t, alpha, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_convolution_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64, benchmark: bool, deterministic: bool, allow_tf32: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_convolution_out(&outs[0], out.t, self.t, weight.t, padding_data, padding_len, stride_data, stride_len, dilation_data, dilation_len, groups, benchmark, deterministic, allow_tf32)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_convolution_relu :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_convolution_relu(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_convolution_relu_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_convolution_relu_out(&outs[0], out.t, self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_convolution_transpose :: proc(self: *Tensor, weight: Tensor, padding_data: *i64, padding_len: i32, output_padding_data: *i64, output_padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64, benchmark: bool, deterministic: bool, allow_tf32: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_convolution_transpose(&outs[0], self.t, weight.t, padding_data, padding_len, output_padding_data, output_padding_len, stride_data, stride_len, dilation_data, dilation_len, groups, benchmark, deterministic, allow_tf32)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_convolution_transpose_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, padding_data: *i64, padding_len: i32, output_padding_data: *i64, output_padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64, benchmark: bool, deterministic: bool, allow_tf32: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_convolution_transpose_out(&outs[0], out.t, self.t, weight.t, padding_data, padding_len, output_padding_data, output_padding_len, stride_data, stride_len, dilation_data, dilation_len, groups, benchmark, deterministic, allow_tf32)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_grid_sampler :: proc(self: *Tensor, grid: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_grid_sampler(&outs[0], self.t, grid.t)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_grid_sampler_backward :: proc(self: *Tensor, grid: Tensor, grad_output: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_grid_sampler_backward(&outs[0], self.t, grid.t, grad_output.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.cudnn_grid_sampler_backward_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, grid: Tensor, grad_output: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_grid_sampler_backward_out(&outs[0], out0.t, out1.t, self.t, grid.t, grad_output.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.cudnn_grid_sampler_out :: proc(self: *Tensor, out: Tensor, grid: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cudnn_grid_sampler_out(&outs[0], out.t, self.t, grid.t)
    return wrap_and_track(outs[0])
}

Tensor.cudnn_is_acceptable :: proc(self: *Tensor) bool {
    return torch_bindings.atg_cudnn_is_acceptable(self.t)
}

Tensor.cummax :: proc(self: *Tensor, dim: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_cummax(&outs[0], self.t, dim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.cummax_out :: proc(self: *Tensor, values: Tensor, indices: Tensor, dim: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_cummax_out(&outs[0], values.t, indices.t, self.t, dim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.cummaxmin_backward :: proc(self: *Tensor, grad: Tensor, indices: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cummaxmin_backward(&outs[0], grad.t, self.t, indices.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.cummin :: proc(self: *Tensor, dim: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_cummin(&outs[0], self.t, dim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.cummin_out :: proc(self: *Tensor, values: Tensor, indices: Tensor, dim: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_cummin_out(&outs[0], values.t, indices.t, self.t, dim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.cumprod :: proc(self: *Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cumprod(&outs[0], self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.cumprod_ :: proc(self: *Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cumprod_(&outs[0], self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.cumprod_backward :: proc(self: *Tensor, grad: Tensor, dim: i64, output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cumprod_backward(&outs[0], grad.t, self.t, dim, output.t)
    return wrap_and_track(outs[0])
}

Tensor.cumprod_out :: proc(self: *Tensor, out: Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cumprod_out(&outs[0], out.t, self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.cumsum :: proc(self: *Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cumsum(&outs[0], self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.cumsum_ :: proc(self: *Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cumsum_(&outs[0], self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.cumsum_out :: proc(self: *Tensor, out: Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cumsum_out(&outs[0], out.t, self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.cumulative_trapezoid :: proc(y: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cumulative_trapezoid(&outs[0], y.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.cumulative_trapezoid_x :: proc(y: Tensor, x: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_cumulative_trapezoid_x(&outs[0], y.t, x.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.data :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_data(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.deg2rad :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_deg2rad(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.deg2rad_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_deg2rad_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.deg2rad_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_deg2rad_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.dense_dim :: proc(self: *Tensor) i64 {
    return torch_bindings.atg_dense_dim(self.t)
}

Tensor.dequantize :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_dequantize(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.dequantize_self_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_dequantize_self_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.dequantize_tensors :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) [dyn] Tensor {
    ptr := torch_bindings.atg_dequantize_tensors(tensors_data, tensors_len)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.dequantize_tensors_out :: proc(out_data: *torch_bindings.Tensor, out_len: i32, tensors_data: *torch_bindings.Tensor, tensors_len: i32) void {
    torch_bindings.atg_dequantize_tensors_out(out_data, out_len, tensors_data, tensors_len)
}

Tensor.det :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_det(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.detach :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_detach(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.detach_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_detach_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.detach_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_detach_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.detach_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_detach_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.diag :: proc(self: *Tensor, diagonal: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_diag(&outs[0], self.t, diagonal)
    return wrap_and_track(outs[0])
}

Tensor.diag_embed :: proc(self: *Tensor, offset: i64, dim1: i64, dim2: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_diag_embed(&outs[0], self.t, offset, dim1, dim2)
    return wrap_and_track(outs[0])
}

Tensor.diag_embed_out :: proc(self: *Tensor, out: Tensor, offset: i64, dim1: i64, dim2: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_diag_embed_out(&outs[0], out.t, self.t, offset, dim1, dim2)
    return wrap_and_track(outs[0])
}

Tensor.diag_out :: proc(self: *Tensor, out: Tensor, diagonal: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_diag_out(&outs[0], out.t, self.t, diagonal)
    return wrap_and_track(outs[0])
}

Tensor.diagflat :: proc(self: *Tensor, offset: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_diagflat(&outs[0], self.t, offset)
    return wrap_and_track(outs[0])
}

Tensor.diagonal :: proc(self: *Tensor, offset: i64, dim1: i64, dim2: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_diagonal(&outs[0], self.t, offset, dim1, dim2)
    return wrap_and_track(outs[0])
}

Tensor.diagonal_backward :: proc(grad_output: Tensor, input_sizes_data: *i64, input_sizes_len: i32, offset: i64, dim1: i64, dim2: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_diagonal_backward(&outs[0], grad_output.t, input_sizes_data, input_sizes_len, offset, dim1, dim2)
    return wrap_and_track(outs[0])
}

Tensor.diagonal_backward_out :: proc(out: Tensor, grad_output: Tensor, input_sizes_data: *i64, input_sizes_len: i32, offset: i64, dim1: i64, dim2: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_diagonal_backward_out(&outs[0], out.t, grad_output.t, input_sizes_data, input_sizes_len, offset, dim1, dim2)
    return wrap_and_track(outs[0])
}

Tensor.diagonal_copy :: proc(self: *Tensor, offset: i64, dim1: i64, dim2: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_diagonal_copy(&outs[0], self.t, offset, dim1, dim2)
    return wrap_and_track(outs[0])
}

Tensor.diagonal_copy_out :: proc(self: *Tensor, out: Tensor, offset: i64, dim1: i64, dim2: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_diagonal_copy_out(&outs[0], out.t, self.t, offset, dim1, dim2)
    return wrap_and_track(outs[0])
}

Tensor.diagonal_scatter :: proc(self: *Tensor, src: Tensor, offset: i64, dim1: i64, dim2: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_diagonal_scatter(&outs[0], self.t, src.t, offset, dim1, dim2)
    return wrap_and_track(outs[0])
}

Tensor.diagonal_scatter_out :: proc(self: *Tensor, out: Tensor, src: Tensor, offset: i64, dim1: i64, dim2: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_diagonal_scatter_out(&outs[0], out.t, self.t, src.t, offset, dim1, dim2)
    return wrap_and_track(outs[0])
}

Tensor.diff :: proc(self: *Tensor, n: i64, dim: i64, prepend: Tensor, append: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_diff(&outs[0], self.t, n, dim, prepend.t, append.t)
    return wrap_and_track(outs[0])
}

Tensor.diff_out :: proc(self: *Tensor, out: Tensor, n: i64, dim: i64, prepend: Tensor, append: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_diff_out(&outs[0], out.t, self.t, n, dim, prepend.t, append.t)
    return wrap_and_track(outs[0])
}

Tensor.digamma :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_digamma(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.digamma_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_digamma_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.digamma_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_digamma_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.dist :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_dist(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.dist_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_dist_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.div :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_div(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.div_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_div_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.div_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_div_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.div_out_mode :: proc(self: *Tensor, out: Tensor, other: Tensor, rounding_mode_ptr: *u8, rounding_mode_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_div_out_mode(&outs[0], out.t, self.t, other.t, rounding_mode_ptr, rounding_mode_len)
    return wrap_and_track(outs[0])
}

Tensor.div_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_div_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.div_scalar_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_div_scalar_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.div_scalar_mode :: proc(self: *Tensor, other: torch_bindings.Scalar, rounding_mode_ptr: *u8, rounding_mode_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_div_scalar_mode(&outs[0], self.t, other, rounding_mode_ptr, rounding_mode_len)
    return wrap_and_track(outs[0])
}

Tensor.div_scalar_mode_ :: proc(self: *Tensor, other: torch_bindings.Scalar, rounding_mode_ptr: *u8, rounding_mode_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_div_scalar_mode_(&outs[0], self.t, other, rounding_mode_ptr, rounding_mode_len)
    return wrap_and_track(outs[0])
}

Tensor.div_scalar_mode_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar, rounding_mode_ptr: *u8, rounding_mode_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_div_scalar_mode_out(&outs[0], out.t, self.t, other, rounding_mode_ptr, rounding_mode_len)
    return wrap_and_track(outs[0])
}

Tensor.div_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_div_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.div_tensor_mode :: proc(self: *Tensor, other: Tensor, rounding_mode_ptr: *u8, rounding_mode_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_div_tensor_mode(&outs[0], self.t, other.t, rounding_mode_ptr, rounding_mode_len)
    return wrap_and_track(outs[0])
}

Tensor.div_tensor_mode_ :: proc(self: *Tensor, other: Tensor, rounding_mode_ptr: *u8, rounding_mode_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_div_tensor_mode_(&outs[0], self.t, other.t, rounding_mode_ptr, rounding_mode_len)
    return wrap_and_track(outs[0])
}

Tensor.divide :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_divide(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.divide_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_divide_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.divide_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_divide_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.divide_out_mode :: proc(self: *Tensor, out: Tensor, other: Tensor, rounding_mode_ptr: *u8, rounding_mode_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_divide_out_mode(&outs[0], out.t, self.t, other.t, rounding_mode_ptr, rounding_mode_len)
    return wrap_and_track(outs[0])
}

Tensor.divide_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_divide_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.divide_scalar_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_divide_scalar_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.divide_scalar_mode :: proc(self: *Tensor, other: torch_bindings.Scalar, rounding_mode_ptr: *u8, rounding_mode_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_divide_scalar_mode(&outs[0], self.t, other, rounding_mode_ptr, rounding_mode_len)
    return wrap_and_track(outs[0])
}

Tensor.divide_scalar_mode_ :: proc(self: *Tensor, other: torch_bindings.Scalar, rounding_mode_ptr: *u8, rounding_mode_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_divide_scalar_mode_(&outs[0], self.t, other, rounding_mode_ptr, rounding_mode_len)
    return wrap_and_track(outs[0])
}

Tensor.divide_tensor_mode :: proc(self: *Tensor, other: Tensor, rounding_mode_ptr: *u8, rounding_mode_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_divide_tensor_mode(&outs[0], self.t, other.t, rounding_mode_ptr, rounding_mode_len)
    return wrap_and_track(outs[0])
}

Tensor.divide_tensor_mode_ :: proc(self: *Tensor, other: Tensor, rounding_mode_ptr: *u8, rounding_mode_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_divide_tensor_mode_(&outs[0], self.t, other.t, rounding_mode_ptr, rounding_mode_len)
    return wrap_and_track(outs[0])
}

Tensor.dot :: proc(self: *Tensor, tensor_: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_dot(&outs[0], self.t, tensor_.t)
    return wrap_and_track(outs[0])
}

Tensor.dot_out :: proc(self: *Tensor, out: Tensor, tensor_: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_dot_out(&outs[0], out.t, self.t, tensor_.t)
    return wrap_and_track(outs[0])
}

Tensor.dropout :: proc(self: *Tensor, p: f64, train: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_dropout(&outs[0], self.t, p, train)
    return wrap_and_track(outs[0])
}

Tensor.dropout_ :: proc(self: *Tensor, p: f64, train: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_dropout_(&outs[0], self.t, p, train)
    return wrap_and_track(outs[0])
}

Tensor.dsplit :: proc(self: *Tensor, sections: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_dsplit(self.t, sections)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.dsplit_array :: proc(self: *Tensor, indices_data: *i64, indices_len: i32) [dyn] Tensor {
    ptr := torch_bindings.atg_dsplit_array(self.t, indices_data, indices_len)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.dstack :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_dstack(&outs[0], tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.dstack_out :: proc(out: Tensor, tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_dstack_out(&outs[0], out.t, tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.einsum :: proc(equation_ptr: *u8, equation_len: i32, tensors_data: *torch_bindings.Tensor, tensors_len: i32, path_data: *i64, path_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_einsum(&outs[0], equation_ptr, equation_len, tensors_data, tensors_len, path_data, path_len)
    return wrap_and_track(outs[0])
}

Tensor.elu :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_elu(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.elu_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_elu_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.elu_backward :: proc(grad_output: Tensor, alpha: torch_bindings.Scalar, scale: torch_bindings.Scalar, input_scale: torch_bindings.Scalar, is_result: bool, self_or_result: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_elu_backward(&outs[0], grad_output.t, alpha, scale, input_scale, is_result, self_or_result.t)
    return wrap_and_track(outs[0])
}

Tensor.elu_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, alpha: torch_bindings.Scalar, scale: torch_bindings.Scalar, input_scale: torch_bindings.Scalar, is_result: bool, self_or_result: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_elu_backward_grad_input(&outs[0], grad_input.t, grad_output.t, alpha, scale, input_scale, is_result, self_or_result.t)
    return wrap_and_track(outs[0])
}

Tensor.elu_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_elu_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.embedding :: proc(weight: Tensor, indices: Tensor, padding_idx: i64, scale_grad_by_freq: bool, sparse: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_embedding(&outs[0], weight.t, indices.t, padding_idx, scale_grad_by_freq, sparse)
    return wrap_and_track(outs[0])
}

Tensor.embedding_backward :: proc(grad: Tensor, indices: Tensor, num_weights: i64, padding_idx: i64, scale_grad_by_freq: bool, sparse: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_embedding_backward(&outs[0], grad.t, indices.t, num_weights, padding_idx, scale_grad_by_freq, sparse)
    return wrap_and_track(outs[0])
}

Tensor.embedding_bag :: proc(weight: Tensor, indices: Tensor, offsets: Tensor, scale_grad_by_freq: bool, mode: i64, sparse: bool, per_sample_weights: Tensor, include_last_offset: bool) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg_embedding_bag(&outs[0], weight.t, indices.t, offsets.t, scale_grad_by_freq, mode, sparse, per_sample_weights.t, include_last_offset)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor.embedding_bag_padding_idx :: proc(weight: Tensor, indices: Tensor, offsets: Tensor, scale_grad_by_freq: bool, mode: i64, sparse: bool, per_sample_weights: Tensor, include_last_offset: bool, padding_idx_v: i64, padding_idx_null: u8) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg_embedding_bag_padding_idx(&outs[0], weight.t, indices.t, offsets.t, scale_grad_by_freq, mode, sparse, per_sample_weights.t, include_last_offset, padding_idx_v, padding_idx_null)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor.embedding_dense_backward :: proc(grad_output: Tensor, indices: Tensor, num_weights: i64, padding_idx: i64, scale_grad_by_freq: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_embedding_dense_backward(&outs[0], grad_output.t, indices.t, num_weights, padding_idx, scale_grad_by_freq)
    return wrap_and_track(outs[0])
}

Tensor.embedding_dense_backward_out :: proc(out: Tensor, grad_output: Tensor, indices: Tensor, num_weights: i64, padding_idx: i64, scale_grad_by_freq: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_embedding_dense_backward_out(&outs[0], out.t, grad_output.t, indices.t, num_weights, padding_idx, scale_grad_by_freq)
    return wrap_and_track(outs[0])
}

Tensor.embedding_out :: proc(out: Tensor, weight: Tensor, indices: Tensor, padding_idx: i64, scale_grad_by_freq: bool, sparse: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_embedding_out(&outs[0], out.t, weight.t, indices.t, padding_idx, scale_grad_by_freq, sparse)
    return wrap_and_track(outs[0])
}

Tensor.embedding_renorm :: proc(self: *Tensor, indices: Tensor, max_norm: f64, norm_type: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_embedding_renorm(&outs[0], self.t, indices.t, max_norm, norm_type)
    return wrap_and_track(outs[0])
}

Tensor.embedding_renorm_ :: proc(self: *Tensor, indices: Tensor, max_norm: f64, norm_type: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_embedding_renorm_(&outs[0], self.t, indices.t, max_norm, norm_type)
    return wrap_and_track(outs[0])
}

Tensor.embedding_renorm_out :: proc(self: *Tensor, out: Tensor, indices: Tensor, max_norm: f64, norm_type: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_embedding_renorm_out(&outs[0], out.t, self.t, indices.t, max_norm, norm_type)
    return wrap_and_track(outs[0])
}

Tensor.embedding_sparse_backward :: proc(grad: Tensor, indices: Tensor, num_weights: i64, padding_idx: i64, scale_grad_by_freq: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_embedding_sparse_backward(&outs[0], grad.t, indices.t, num_weights, padding_idx, scale_grad_by_freq)
    return wrap_and_track(outs[0])
}

Tensor.empty :: proc(size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_empty(&outs[0], size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.empty_like :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_empty_like(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.empty_like_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_empty_like_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.empty_out :: proc(out: Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_empty_out(&outs[0], out.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.empty_permuted :: proc(size_data: *i64, size_len: i32, physical_layout_data: *i64, physical_layout_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_empty_permuted(&outs[0], size_data, size_len, physical_layout_data, physical_layout_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.empty_permuted_out :: proc(out: Tensor, size_data: *i64, size_len: i32, physical_layout_data: *i64, physical_layout_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_empty_permuted_out(&outs[0], out.t, size_data, size_len, physical_layout_data, physical_layout_len)
    return wrap_and_track(outs[0])
}

Tensor.empty_quantized :: proc(size_data: *i64, size_len: i32, qtensor: Tensor, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_empty_quantized(&outs[0], size_data, size_len, qtensor.t, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.empty_quantized_out :: proc(out: Tensor, size_data: *i64, size_len: i32, qtensor: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_empty_quantized_out(&outs[0], out.t, size_data, size_len, qtensor.t)
    return wrap_and_track(outs[0])
}

Tensor.empty_strided :: proc(size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_empty_strided(&outs[0], size_data, size_len, stride_data, stride_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.empty_strided_out :: proc(out: Tensor, size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_empty_strided_out(&outs[0], out.t, size_data, size_len, stride_data, stride_len)
    return wrap_and_track(outs[0])
}

Tensor.eq :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_eq(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.eq_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_eq_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.eq_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_eq_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.eq_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_eq_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.eq_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_eq_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.eq_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_eq_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.equal :: proc(self: *Tensor, other: Tensor) bool {
    return torch_bindings.atg_equal(self.t, other.t)
}

Tensor.erf :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_erf(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.erf_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_erf_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.erf_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_erf_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.erfc :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_erfc(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.erfc_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_erfc_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.erfc_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_erfc_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.erfinv :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_erfinv(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.erfinv_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_erfinv_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.erfinv_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_erfinv_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.exp :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_exp(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.exp2 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_exp2(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.exp2_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_exp2_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.exp2_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_exp2_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.exp_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_exp_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.exp_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_exp_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.expand :: proc(self: *Tensor, size_data: *i64, size_len: i32, implicit: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_expand(&outs[0], self.t, size_data, size_len, implicit)
    return wrap_and_track(outs[0])
}

Tensor.expand_as :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_expand_as(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.expand_copy :: proc(self: *Tensor, size_data: *i64, size_len: i32, implicit: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_expand_copy(&outs[0], self.t, size_data, size_len, implicit)
    return wrap_and_track(outs[0])
}

Tensor.expand_copy_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32, implicit: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_expand_copy_out(&outs[0], out.t, self.t, size_data, size_len, implicit)
    return wrap_and_track(outs[0])
}

Tensor.expm1 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_expm1(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.expm1_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_expm1_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.expm1_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_expm1_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.exponential :: proc(self: *Tensor, lambd: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_exponential(&outs[0], self.t, lambd)
    return wrap_and_track(outs[0])
}

Tensor.exponential_ :: proc(self: *Tensor, lambd: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_exponential_(&outs[0], self.t, lambd)
    return wrap_and_track(outs[0])
}

Tensor.exponential_out :: proc(self: *Tensor, out: Tensor, lambd: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_exponential_out(&outs[0], out.t, self.t, lambd)
    return wrap_and_track(outs[0])
}

Tensor.eye :: proc(n: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_eye(&outs[0], n, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.eye_m :: proc(n: i64, m: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_eye_m(&outs[0], n, m, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.eye_m_out :: proc(out: Tensor, n: i64, m: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_eye_m_out(&outs[0], out.t, n, m)
    return wrap_and_track(outs[0])
}

Tensor.eye_out :: proc(out: Tensor, n: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_eye_out(&outs[0], out.t, n)
    return wrap_and_track(outs[0])
}

Tensor.fake_quantize_per_channel_affine :: proc(self: *Tensor, scale: Tensor, zero_point: Tensor, axis: i64, quant_min: i64, quant_max: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fake_quantize_per_channel_affine(&outs[0], self.t, scale.t, zero_point.t, axis, quant_min, quant_max)
    return wrap_and_track(outs[0])
}

Tensor.fake_quantize_per_channel_affine_cachemask :: proc(self: *Tensor, scale: Tensor, zero_point: Tensor, axis: i64, quant_min: i64, quant_max: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_fake_quantize_per_channel_affine_cachemask(&outs[0], self.t, scale.t, zero_point.t, axis, quant_min, quant_max)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.fake_quantize_per_channel_affine_cachemask_backward :: proc(grad: Tensor, mask: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fake_quantize_per_channel_affine_cachemask_backward(&outs[0], grad.t, mask.t)
    return wrap_and_track(outs[0])
}

Tensor.fake_quantize_per_channel_affine_cachemask_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, scale: Tensor, zero_point: Tensor, axis: i64, quant_min: i64, quant_max: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_fake_quantize_per_channel_affine_cachemask_out(&outs[0], out0.t, out1.t, self.t, scale.t, zero_point.t, axis, quant_min, quant_max)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.fake_quantize_per_tensor_affine :: proc(self: *Tensor, scale: f64, zero_point: i64, quant_min: i64, quant_max: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fake_quantize_per_tensor_affine(&outs[0], self.t, scale, zero_point, quant_min, quant_max)
    return wrap_and_track(outs[0])
}

Tensor.fake_quantize_per_tensor_affine_cachemask :: proc(self: *Tensor, scale: f64, zero_point: i64, quant_min: i64, quant_max: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_fake_quantize_per_tensor_affine_cachemask(&outs[0], self.t, scale, zero_point, quant_min, quant_max)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.fake_quantize_per_tensor_affine_cachemask_backward :: proc(grad: Tensor, mask: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fake_quantize_per_tensor_affine_cachemask_backward(&outs[0], grad.t, mask.t)
    return wrap_and_track(outs[0])
}

Tensor.fake_quantize_per_tensor_affine_cachemask_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, scale: f64, zero_point: i64, quant_min: i64, quant_max: i64) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_fake_quantize_per_tensor_affine_cachemask_out(&outs[0], out0.t, out1.t, self.t, scale, zero_point, quant_min, quant_max)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.fake_quantize_per_tensor_affine_tensor_qparams :: proc(self: *Tensor, scale: Tensor, zero_point: Tensor, quant_min: i64, quant_max: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fake_quantize_per_tensor_affine_tensor_qparams(&outs[0], self.t, scale.t, zero_point.t, quant_min, quant_max)
    return wrap_and_track(outs[0])
}

Tensor.feature_alpha_dropout :: proc(self: *Tensor, p: f64, train: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_feature_alpha_dropout(&outs[0], self.t, p, train)
    return wrap_and_track(outs[0])
}

Tensor.feature_alpha_dropout_ :: proc(self: *Tensor, p: f64, train: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_feature_alpha_dropout_(&outs[0], self.t, p, train)
    return wrap_and_track(outs[0])
}

Tensor.feature_dropout :: proc(self: *Tensor, p: f64, train: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_feature_dropout(&outs[0], self.t, p, train)
    return wrap_and_track(outs[0])
}

Tensor.feature_dropout_ :: proc(self: *Tensor, p: f64, train: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_feature_dropout_(&outs[0], self.t, p, train)
    return wrap_and_track(outs[0])
}

Tensor.fft_fft :: proc(self: *Tensor, n_v: i64, n_null: u8, dim: i64, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_fft(&outs[0], self.t, n_v, n_null, dim, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_fft2 :: proc(self: *Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_fft2(&outs[0], self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_fft2_out :: proc(self: *Tensor, out: Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_fft2_out(&outs[0], out.t, self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_fft_out :: proc(self: *Tensor, out: Tensor, n_v: i64, n_null: u8, dim: i64, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_fft_out(&outs[0], out.t, self.t, n_v, n_null, dim, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_fftfreq :: proc(n: i64, d: f64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_fftfreq(&outs[0], n, d, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.fft_fftfreq_out :: proc(out: Tensor, n: i64, d: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_fftfreq_out(&outs[0], out.t, n, d)
    return wrap_and_track(outs[0])
}

Tensor.fft_fftn :: proc(self: *Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_fftn(&outs[0], self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_fftn_out :: proc(self: *Tensor, out: Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_fftn_out(&outs[0], out.t, self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_fftshift :: proc(self: *Tensor, dim_data: *i64, dim_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_fftshift(&outs[0], self.t, dim_data, dim_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_hfft :: proc(self: *Tensor, n_v: i64, n_null: u8, dim: i64, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_hfft(&outs[0], self.t, n_v, n_null, dim, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_hfft2 :: proc(self: *Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_hfft2(&outs[0], self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_hfft2_out :: proc(self: *Tensor, out: Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_hfft2_out(&outs[0], out.t, self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_hfft_out :: proc(self: *Tensor, out: Tensor, n_v: i64, n_null: u8, dim: i64, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_hfft_out(&outs[0], out.t, self.t, n_v, n_null, dim, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_hfftn :: proc(self: *Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_hfftn(&outs[0], self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_hfftn_out :: proc(self: *Tensor, out: Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_hfftn_out(&outs[0], out.t, self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_ifft :: proc(self: *Tensor, n_v: i64, n_null: u8, dim: i64, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_ifft(&outs[0], self.t, n_v, n_null, dim, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_ifft2 :: proc(self: *Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_ifft2(&outs[0], self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_ifft2_out :: proc(self: *Tensor, out: Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_ifft2_out(&outs[0], out.t, self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_ifft_out :: proc(self: *Tensor, out: Tensor, n_v: i64, n_null: u8, dim: i64, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_ifft_out(&outs[0], out.t, self.t, n_v, n_null, dim, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_ifftn :: proc(self: *Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_ifftn(&outs[0], self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_ifftn_out :: proc(self: *Tensor, out: Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_ifftn_out(&outs[0], out.t, self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_ifftshift :: proc(self: *Tensor, dim_data: *i64, dim_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_ifftshift(&outs[0], self.t, dim_data, dim_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_ihfft :: proc(self: *Tensor, n_v: i64, n_null: u8, dim: i64, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_ihfft(&outs[0], self.t, n_v, n_null, dim, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_ihfft2 :: proc(self: *Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_ihfft2(&outs[0], self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_ihfft2_out :: proc(self: *Tensor, out: Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_ihfft2_out(&outs[0], out.t, self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_ihfft_out :: proc(self: *Tensor, out: Tensor, n_v: i64, n_null: u8, dim: i64, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_ihfft_out(&outs[0], out.t, self.t, n_v, n_null, dim, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_ihfftn :: proc(self: *Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_ihfftn(&outs[0], self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_ihfftn_out :: proc(self: *Tensor, out: Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_ihfftn_out(&outs[0], out.t, self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_irfft :: proc(self: *Tensor, n_v: i64, n_null: u8, dim: i64, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_irfft(&outs[0], self.t, n_v, n_null, dim, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_irfft2 :: proc(self: *Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_irfft2(&outs[0], self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_irfft2_out :: proc(self: *Tensor, out: Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_irfft2_out(&outs[0], out.t, self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_irfft_out :: proc(self: *Tensor, out: Tensor, n_v: i64, n_null: u8, dim: i64, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_irfft_out(&outs[0], out.t, self.t, n_v, n_null, dim, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_irfftn :: proc(self: *Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_irfftn(&outs[0], self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_irfftn_out :: proc(self: *Tensor, out: Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_irfftn_out(&outs[0], out.t, self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_rfft :: proc(self: *Tensor, n_v: i64, n_null: u8, dim: i64, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_rfft(&outs[0], self.t, n_v, n_null, dim, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_rfft2 :: proc(self: *Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_rfft2(&outs[0], self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_rfft2_out :: proc(self: *Tensor, out: Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_rfft2_out(&outs[0], out.t, self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_rfft_out :: proc(self: *Tensor, out: Tensor, n_v: i64, n_null: u8, dim: i64, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_rfft_out(&outs[0], out.t, self.t, n_v, n_null, dim, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_rfftfreq :: proc(n: i64, d: f64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_rfftfreq(&outs[0], n, d, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.fft_rfftfreq_out :: proc(out: Tensor, n: i64, d: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_rfftfreq_out(&outs[0], out.t, n, d)
    return wrap_and_track(outs[0])
}

Tensor.fft_rfftn :: proc(self: *Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_rfftn(&outs[0], self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fft_rfftn_out :: proc(self: *Tensor, out: Tensor, s_data: *i64, s_len: i32, dim_data: *i64, dim_len: i32, norm_ptr: *u8, norm_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fft_rfftn_out(&outs[0], out.t, self.t, s_data, s_len, dim_data, dim_len, norm_ptr, norm_len)
    return wrap_and_track(outs[0])
}

Tensor.fill :: proc(self: *Tensor, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fill(&outs[0], self.t, value)
    return wrap_and_track(outs[0])
}

Tensor.fill_ :: proc(self: *Tensor, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fill_(&outs[0], self.t, value)
    return wrap_and_track(outs[0])
}

Tensor.fill_diagonal_ :: proc(self: *Tensor, fill_value: torch_bindings.Scalar, wrap: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fill_diagonal_(&outs[0], self.t, fill_value, wrap)
    return wrap_and_track(outs[0])
}

Tensor.fill_scalar_out :: proc(self: *Tensor, out: Tensor, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fill_scalar_out(&outs[0], out.t, self.t, value)
    return wrap_and_track(outs[0])
}

Tensor.fill_tensor :: proc(self: *Tensor, value: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fill_tensor(&outs[0], self.t, value.t)
    return wrap_and_track(outs[0])
}

Tensor.fill_tensor_ :: proc(self: *Tensor, value: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fill_tensor_(&outs[0], self.t, value.t)
    return wrap_and_track(outs[0])
}

Tensor.fill_tensor_out :: proc(self: *Tensor, out: Tensor, value: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fill_tensor_out(&outs[0], out.t, self.t, value.t)
    return wrap_and_track(outs[0])
}

Tensor.fix :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fix(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.fix_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fix_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.fix_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fix_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.flatten :: proc(self: *Tensor, start_dim: i64, end_dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_flatten(&outs[0], self.t, start_dim, end_dim)
    return wrap_and_track(outs[0])
}

Tensor.flatten_dense_tensors :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_flatten_dense_tensors(&outs[0], tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.flip :: proc(self: *Tensor, dims_data: *i64, dims_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_flip(&outs[0], self.t, dims_data, dims_len)
    return wrap_and_track(outs[0])
}

Tensor.flip_out :: proc(self: *Tensor, out: Tensor, dims_data: *i64, dims_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_flip_out(&outs[0], out.t, self.t, dims_data, dims_len)
    return wrap_and_track(outs[0])
}

Tensor.fliplr :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fliplr(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.flipud :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_flipud(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.float_power :: proc(self: *Tensor, exponent: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_float_power(&outs[0], self.t, exponent.t)
    return wrap_and_track(outs[0])
}

Tensor.float_power_ :: proc(self: *Tensor, exponent: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_float_power_(&outs[0], self.t, exponent)
    return wrap_and_track(outs[0])
}

Tensor.float_power_scalar :: proc(self_scalar: torch_bindings.Scalar, exponent: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_float_power_scalar(&outs[0], self_scalar, exponent.t)
    return wrap_and_track(outs[0])
}

Tensor.float_power_scalar_out :: proc(out: Tensor, self_scalar: torch_bindings.Scalar, exponent: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_float_power_scalar_out(&outs[0], out.t, self_scalar, exponent.t)
    return wrap_and_track(outs[0])
}

Tensor.float_power_tensor_ :: proc(self: *Tensor, exponent: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_float_power_tensor_(&outs[0], self.t, exponent.t)
    return wrap_and_track(outs[0])
}

Tensor.float_power_tensor_scalar :: proc(self: *Tensor, exponent: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_float_power_tensor_scalar(&outs[0], self.t, exponent)
    return wrap_and_track(outs[0])
}

Tensor.float_power_tensor_scalar_out :: proc(self: *Tensor, out: Tensor, exponent: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_float_power_tensor_scalar_out(&outs[0], out.t, self.t, exponent)
    return wrap_and_track(outs[0])
}

Tensor.float_power_tensor_tensor_out :: proc(self: *Tensor, out: Tensor, exponent: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_float_power_tensor_tensor_out(&outs[0], out.t, self.t, exponent.t)
    return wrap_and_track(outs[0])
}

Tensor.floor :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_floor(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.floor_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_floor_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.floor_divide :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_floor_divide(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.floor_divide_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_floor_divide_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.floor_divide_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_floor_divide_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.floor_divide_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_floor_divide_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.floor_divide_scalar_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_floor_divide_scalar_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.floor_divide_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_floor_divide_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.floor_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_floor_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.fmax :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fmax(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.fmax_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fmax_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.fmin :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fmin(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.fmin_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fmin_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.fmod :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fmod(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.fmod_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fmod_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.fmod_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fmod_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.fmod_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fmod_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.fmod_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fmod_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.fmod_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fmod_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.frac :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_frac(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.frac_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_frac_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.frac_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_frac_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.fractional_max_pool2d :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, output_size_data: *i64, output_size_len: i32, random_samples: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_fractional_max_pool2d(&outs[0], self.t, kernel_size_data, kernel_size_len, output_size_data, output_size_len, random_samples.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.fractional_max_pool2d_backward :: proc(self: *Tensor, grad_output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, output_size_data: *i64, output_size_len: i32, indices: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fractional_max_pool2d_backward(&outs[0], grad_output.t, self.t, kernel_size_data, kernel_size_len, output_size_data, output_size_len, indices.t)
    return wrap_and_track(outs[0])
}

Tensor.fractional_max_pool2d_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, output_size_data: *i64, output_size_len: i32, indices: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fractional_max_pool2d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, kernel_size_data, kernel_size_len, output_size_data, output_size_len, indices.t)
    return wrap_and_track(outs[0])
}

Tensor.fractional_max_pool2d_output :: proc(self: *Tensor, output: Tensor, indices: Tensor, kernel_size_data: *i64, kernel_size_len: i32, output_size_data: *i64, output_size_len: i32, random_samples: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_fractional_max_pool2d_output(&outs[0], output.t, indices.t, self.t, kernel_size_data, kernel_size_len, output_size_data, output_size_len, random_samples.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.fractional_max_pool3d :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, output_size_data: *i64, output_size_len: i32, random_samples: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_fractional_max_pool3d(&outs[0], self.t, kernel_size_data, kernel_size_len, output_size_data, output_size_len, random_samples.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.fractional_max_pool3d_backward :: proc(self: *Tensor, grad_output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, output_size_data: *i64, output_size_len: i32, indices: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fractional_max_pool3d_backward(&outs[0], grad_output.t, self.t, kernel_size_data, kernel_size_len, output_size_data, output_size_len, indices.t)
    return wrap_and_track(outs[0])
}

Tensor.fractional_max_pool3d_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, output_size_data: *i64, output_size_len: i32, indices: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fractional_max_pool3d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, kernel_size_data, kernel_size_len, output_size_data, output_size_len, indices.t)
    return wrap_and_track(outs[0])
}

Tensor.fractional_max_pool3d_output :: proc(self: *Tensor, output: Tensor, indices: Tensor, kernel_size_data: *i64, kernel_size_len: i32, output_size_data: *i64, output_size_len: i32, random_samples: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_fractional_max_pool3d_output(&outs[0], output.t, indices.t, self.t, kernel_size_data, kernel_size_len, output_size_data, output_size_len, random_samples.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.frexp :: proc(self: *Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_frexp(&outs[0], self.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.frexp_tensor_out :: proc(self: *Tensor, mantissa: Tensor, exponent: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_frexp_tensor_out(&outs[0], mantissa.t, exponent.t, self.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.frobenius_norm :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_frobenius_norm(&outs[0], self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.frobenius_norm_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_frobenius_norm_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.from_file :: proc(filename_ptr: *u8, filename_len: i32, shared: bool, size_v: i64, size_null: u8, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_from_file(&outs[0], filename_ptr, filename_len, shared, size_v, size_null, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.from_file_out :: proc(out: Tensor, filename_ptr: *u8, filename_len: i32, shared: bool, size_v: i64, size_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_from_file_out(&outs[0], out.t, filename_ptr, filename_len, shared, size_v, size_null)
    return wrap_and_track(outs[0])
}

Tensor.full :: proc(size_data: *i64, size_len: i32, fill_value: torch_bindings.Scalar, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_full(&outs[0], size_data, size_len, fill_value, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.full_like :: proc(self: *Tensor, fill_value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_full_like(&outs[0], self.t, fill_value)
    return wrap_and_track(outs[0])
}

Tensor.full_like_out :: proc(self: *Tensor, out: Tensor, fill_value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_full_like_out(&outs[0], out.t, self.t, fill_value)
    return wrap_and_track(outs[0])
}

Tensor.full_out :: proc(out: Tensor, size_data: *i64, size_len: i32, fill_value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_full_out(&outs[0], out.t, size_data, size_len, fill_value)
    return wrap_and_track(outs[0])
}

Tensor.fused_moving_avg_obs_fake_quant :: proc(self: *Tensor, observer_on: Tensor, fake_quant_on: Tensor, running_min: Tensor, running_max: Tensor, scale: Tensor, zero_point: Tensor, averaging_const: f64, quant_min: i64, quant_max: i64, ch_axis: i64, per_row_fake_quant: bool, symmetric_quant: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_fused_moving_avg_obs_fake_quant(&outs[0], self.t, observer_on.t, fake_quant_on.t, running_min.t, running_max.t, scale.t, zero_point.t, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant)
    return wrap_and_track(outs[0])
}

Tensor.gather :: proc(self: *Tensor, dim: i64, index: Tensor, sparse_grad: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gather(&outs[0], self.t, dim, index.t, sparse_grad)
    return wrap_and_track(outs[0])
}

Tensor.gather_backward :: proc(self: *Tensor, grad: Tensor, dim: i64, index: Tensor, sparse_grad: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gather_backward(&outs[0], grad.t, self.t, dim, index.t, sparse_grad)
    return wrap_and_track(outs[0])
}

Tensor.gather_out :: proc(self: *Tensor, out: Tensor, dim: i64, index: Tensor, sparse_grad: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gather_out(&outs[0], out.t, self.t, dim, index.t, sparse_grad)
    return wrap_and_track(outs[0])
}

Tensor.gcd :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gcd(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.gcd_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gcd_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.gcd_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gcd_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.ge :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ge(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.ge_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ge_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.ge_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ge_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.ge_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ge_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.ge_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ge_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.ge_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ge_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.gelu :: proc(self: *Tensor, approximate_ptr: *u8, approximate_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gelu(&outs[0], self.t, approximate_ptr, approximate_len)
    return wrap_and_track(outs[0])
}

Tensor.gelu_ :: proc(self: *Tensor, approximate_ptr: *u8, approximate_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gelu_(&outs[0], self.t, approximate_ptr, approximate_len)
    return wrap_and_track(outs[0])
}

Tensor.gelu_backward :: proc(self: *Tensor, grad_output: Tensor, approximate_ptr: *u8, approximate_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gelu_backward(&outs[0], grad_output.t, self.t, approximate_ptr, approximate_len)
    return wrap_and_track(outs[0])
}

Tensor.gelu_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, approximate_ptr: *u8, approximate_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gelu_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, approximate_ptr, approximate_len)
    return wrap_and_track(outs[0])
}

Tensor.gelu_out :: proc(self: *Tensor, out: Tensor, approximate_ptr: *u8, approximate_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gelu_out(&outs[0], out.t, self.t, approximate_ptr, approximate_len)
    return wrap_and_track(outs[0])
}

Tensor.geometric :: proc(self: *Tensor, p: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_geometric(&outs[0], self.t, p)
    return wrap_and_track(outs[0])
}

Tensor.geometric_ :: proc(self: *Tensor, p: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_geometric_(&outs[0], self.t, p)
    return wrap_and_track(outs[0])
}

Tensor.geometric_out :: proc(self: *Tensor, out: Tensor, p: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_geometric_out(&outs[0], out.t, self.t, p)
    return wrap_and_track(outs[0])
}

Tensor.geqrf :: proc(self: *Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_geqrf(&outs[0], self.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.geqrf_a :: proc(self: *Tensor, a: Tensor, tau: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_geqrf_a(&outs[0], a.t, tau.t, self.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.ger :: proc(self: *Tensor, vec2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ger(&outs[0], self.t, vec2.t)
    return wrap_and_track(outs[0])
}

Tensor.ger_out :: proc(self: *Tensor, out: Tensor, vec2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ger_out(&outs[0], out.t, self.t, vec2.t)
    return wrap_and_track(outs[0])
}

Tensor.glu :: proc(self: *Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_glu(&outs[0], self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.glu_backward :: proc(self: *Tensor, grad_output: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_glu_backward(&outs[0], grad_output.t, self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.glu_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_glu_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.glu_backward_jvp :: proc(grad_x: Tensor, grad_glu: Tensor, x: Tensor, dgrad_glu: Tensor, dx: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_glu_backward_jvp(&outs[0], grad_x.t, grad_glu.t, x.t, dgrad_glu.t, dx.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.glu_backward_jvp_out :: proc(out: Tensor, grad_x: Tensor, grad_glu: Tensor, x: Tensor, dgrad_glu: Tensor, dx: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_glu_backward_jvp_out(&outs[0], out.t, grad_x.t, grad_glu.t, x.t, dgrad_glu.t, dx.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.glu_jvp :: proc(glu: Tensor, x: Tensor, dx: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_glu_jvp(&outs[0], glu.t, x.t, dx.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.glu_jvp_out :: proc(out: Tensor, glu: Tensor, x: Tensor, dx: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_glu_jvp_out(&outs[0], out.t, glu.t, x.t, dx.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.glu_out :: proc(self: *Tensor, out: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_glu_out(&outs[0], out.t, self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.grad :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_grad(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.greater :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_greater(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.greater_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_greater_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.greater_equal :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_greater_equal(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.greater_equal_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_greater_equal_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.greater_equal_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_greater_equal_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.greater_equal_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_greater_equal_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.greater_equal_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_greater_equal_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.greater_equal_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_greater_equal_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.greater_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_greater_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.greater_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_greater_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.greater_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_greater_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.greater_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_greater_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.grid_sampler :: proc(self: *Tensor, grid: Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_grid_sampler(&outs[0], self.t, grid.t, interpolation_mode, padding_mode, align_corners)
    return wrap_and_track(outs[0])
}

Tensor.grid_sampler_2d :: proc(self: *Tensor, grid: Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_grid_sampler_2d(&outs[0], self.t, grid.t, interpolation_mode, padding_mode, align_corners)
    return wrap_and_track(outs[0])
}

Tensor.grid_sampler_2d_out :: proc(self: *Tensor, out: Tensor, grid: Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_grid_sampler_2d_out(&outs[0], out.t, self.t, grid.t, interpolation_mode, padding_mode, align_corners)
    return wrap_and_track(outs[0])
}

Tensor.grid_sampler_3d :: proc(self: *Tensor, grid: Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_grid_sampler_3d(&outs[0], self.t, grid.t, interpolation_mode, padding_mode, align_corners)
    return wrap_and_track(outs[0])
}

Tensor.grid_sampler_3d_out :: proc(self: *Tensor, out: Tensor, grid: Tensor, interpolation_mode: i64, padding_mode: i64, align_corners: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_grid_sampler_3d_out(&outs[0], out.t, self.t, grid.t, interpolation_mode, padding_mode, align_corners)
    return wrap_and_track(outs[0])
}

Tensor.group_norm :: proc(self: *Tensor, num_groups: i64, weight: Tensor, bias: Tensor, eps: f64, cudnn_enabled: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_group_norm(&outs[0], self.t, num_groups, weight.t, bias.t, eps, cudnn_enabled)
    return wrap_and_track(outs[0])
}

Tensor.gru :: proc(self: *Tensor, hx: Tensor, params_data: *torch_bindings.Tensor, params_len: i32, has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool, batch_first: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_gru(&outs[0], self.t, hx.t, params_data, params_len, has_biases, num_layers, dropout, train, bidirectional, batch_first)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.gru_cell :: proc(self: *Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gru_cell(&outs[0], self.t, hx.t, w_ih.t, w_hh.t, b_ih.t, b_hh.t)
    return wrap_and_track(outs[0])
}

Tensor.gru_data :: proc(data: Tensor, batch_sizes: Tensor, hx: Tensor, params_data: *torch_bindings.Tensor, params_len: i32, has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_gru_data(&outs[0], data.t, batch_sizes.t, hx.t, params_data, params_len, has_biases, num_layers, dropout, train, bidirectional)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.gt :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gt(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.gt_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gt_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.gt_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gt_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.gt_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gt_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.gt_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gt_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.gt_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_gt_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.hamming_window :: proc(window_length: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hamming_window(&outs[0], window_length, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.hamming_window_out :: proc(out: Tensor, window_length: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hamming_window_out(&outs[0], out.t, window_length)
    return wrap_and_track(outs[0])
}

Tensor.hamming_window_periodic :: proc(window_length: i64, periodic: bool, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hamming_window_periodic(&outs[0], window_length, periodic, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.hamming_window_periodic_alpha :: proc(window_length: i64, periodic: bool, alpha: f64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hamming_window_periodic_alpha(&outs[0], window_length, periodic, alpha, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.hamming_window_periodic_alpha_beta :: proc(window_length: i64, periodic: bool, alpha: f64, beta: f64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hamming_window_periodic_alpha_beta(&outs[0], window_length, periodic, alpha, beta, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.hamming_window_periodic_alpha_beta_out :: proc(out: Tensor, window_length: i64, periodic: bool, alpha: f64, beta: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hamming_window_periodic_alpha_beta_out(&outs[0], out.t, window_length, periodic, alpha, beta)
    return wrap_and_track(outs[0])
}

Tensor.hamming_window_periodic_alpha_out :: proc(out: Tensor, window_length: i64, periodic: bool, alpha: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hamming_window_periodic_alpha_out(&outs[0], out.t, window_length, periodic, alpha)
    return wrap_and_track(outs[0])
}

Tensor.hamming_window_periodic_out :: proc(out: Tensor, window_length: i64, periodic: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hamming_window_periodic_out(&outs[0], out.t, window_length, periodic)
    return wrap_and_track(outs[0])
}

Tensor.hann_window :: proc(window_length: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hann_window(&outs[0], window_length, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.hann_window_out :: proc(out: Tensor, window_length: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hann_window_out(&outs[0], out.t, window_length)
    return wrap_and_track(outs[0])
}

Tensor.hann_window_periodic :: proc(window_length: i64, periodic: bool, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hann_window_periodic(&outs[0], window_length, periodic, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.hann_window_periodic_out :: proc(out: Tensor, window_length: i64, periodic: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hann_window_periodic_out(&outs[0], out.t, window_length, periodic)
    return wrap_and_track(outs[0])
}

Tensor.hardshrink :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardshrink(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.hardshrink_backward :: proc(self: *Tensor, grad_out: Tensor, lambd: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardshrink_backward(&outs[0], grad_out.t, self.t, lambd)
    return wrap_and_track(outs[0])
}

Tensor.hardshrink_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_out: Tensor, lambd: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardshrink_backward_grad_input(&outs[0], grad_input.t, grad_out.t, self.t, lambd)
    return wrap_and_track(outs[0])
}

Tensor.hardshrink_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardshrink_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.hardsigmoid :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardsigmoid(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.hardsigmoid_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardsigmoid_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.hardsigmoid_backward :: proc(self: *Tensor, grad_output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardsigmoid_backward(&outs[0], grad_output.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.hardsigmoid_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardsigmoid_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.hardsigmoid_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardsigmoid_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.hardswish :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardswish(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.hardswish_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardswish_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.hardswish_backward :: proc(self: *Tensor, grad_output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardswish_backward(&outs[0], grad_output.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.hardswish_backward_out :: proc(self: *Tensor, out: Tensor, grad_output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardswish_backward_out(&outs[0], out.t, grad_output.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.hardswish_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardswish_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.hardtanh :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardtanh(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.hardtanh_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardtanh_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.hardtanh_backward :: proc(self: *Tensor, grad_output: Tensor, min_val: torch_bindings.Scalar, max_val: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardtanh_backward(&outs[0], grad_output.t, self.t, min_val, max_val)
    return wrap_and_track(outs[0])
}

Tensor.hardtanh_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, min_val: torch_bindings.Scalar, max_val: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardtanh_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, min_val, max_val)
    return wrap_and_track(outs[0])
}

Tensor.hardtanh_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hardtanh_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.hash_tensor :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool, mode: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hash_tensor(&outs[0], self.t, dim_data, dim_len, keepdim, mode)
    return wrap_and_track(outs[0])
}

Tensor.hash_tensor_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool, mode: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hash_tensor_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim, mode)
    return wrap_and_track(outs[0])
}

Tensor.heaviside :: proc(self: *Tensor, values: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_heaviside(&outs[0], self.t, values.t)
    return wrap_and_track(outs[0])
}

Tensor.heaviside_ :: proc(self: *Tensor, values: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_heaviside_(&outs[0], self.t, values.t)
    return wrap_and_track(outs[0])
}

Tensor.heaviside_out :: proc(self: *Tensor, out: Tensor, values: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_heaviside_out(&outs[0], out.t, self.t, values.t)
    return wrap_and_track(outs[0])
}

Tensor.hinge_embedding_loss :: proc(self: *Tensor, target: Tensor, margin: f64, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hinge_embedding_loss(&outs[0], self.t, target.t, margin, reduction)
    return wrap_and_track(outs[0])
}

Tensor.histc :: proc(self: *Tensor, bins: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_histc(&outs[0], self.t, bins)
    return wrap_and_track(outs[0])
}

Tensor.histc_out :: proc(self: *Tensor, out: Tensor, bins: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_histc_out(&outs[0], out.t, self.t, bins)
    return wrap_and_track(outs[0])
}

Tensor.histogram :: proc(self: *Tensor, bins: Tensor, weight: Tensor, density: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_histogram(&outs[0], self.t, bins.t, weight.t, density)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.histogram_bin_ct :: proc(self: *Tensor, bins: i64, range_data: *f64, range_len: i32, weight: Tensor, density: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_histogram_bin_ct(&outs[0], self.t, bins, range_data, range_len, weight.t, density)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.histogram_bin_ct_out :: proc(self: *Tensor, hist: Tensor, bin_edges: Tensor, bins: i64, range_data: *f64, range_len: i32, weight: Tensor, density: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_histogram_bin_ct_out(&outs[0], hist.t, bin_edges.t, self.t, bins, range_data, range_len, weight.t, density)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.histogram_bins_tensor_out :: proc(self: *Tensor, hist: Tensor, bin_edges: Tensor, bins: Tensor, weight: Tensor, density: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_histogram_bins_tensor_out(&outs[0], hist.t, bin_edges.t, self.t, bins.t, weight.t, density)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.hsplit :: proc(self: *Tensor, sections: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_hsplit(self.t, sections)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.hsplit_array :: proc(self: *Tensor, indices_data: *i64, indices_len: i32) [dyn] Tensor {
    ptr := torch_bindings.atg_hsplit_array(self.t, indices_data, indices_len)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.hspmm :: proc(mat1: Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hspmm(&outs[0], mat1.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor.hspmm_out :: proc(out: Tensor, mat1: Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hspmm_out(&outs[0], out.t, mat1.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor.hstack :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hstack(&outs[0], tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.hstack_out :: proc(out: Tensor, tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hstack_out(&outs[0], out.t, tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.huber_loss :: proc(self: *Tensor, target: Tensor, reduction: i64, delta: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_huber_loss(&outs[0], self.t, target.t, reduction, delta)
    return wrap_and_track(outs[0])
}

Tensor.huber_loss_backward :: proc(self: *Tensor, grad_output: Tensor, target: Tensor, reduction: i64, delta: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_huber_loss_backward(&outs[0], grad_output.t, self.t, target.t, reduction, delta)
    return wrap_and_track(outs[0])
}

Tensor.huber_loss_backward_out :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, target: Tensor, reduction: i64, delta: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_huber_loss_backward_out(&outs[0], grad_input.t, grad_output.t, self.t, target.t, reduction, delta)
    return wrap_and_track(outs[0])
}

Tensor.huber_loss_out :: proc(self: *Tensor, out: Tensor, target: Tensor, reduction: i64, delta: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_huber_loss_out(&outs[0], out.t, self.t, target.t, reduction, delta)
    return wrap_and_track(outs[0])
}

Tensor.hypot :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hypot(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.hypot_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hypot_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.hypot_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_hypot_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.i0 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_i0(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.i0_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_i0_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.i0_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_i0_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.igamma :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_igamma(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.igamma_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_igamma_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.igamma_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_igamma_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.igammac :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_igammac(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.igammac_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_igammac_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.igammac_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_igammac_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.im2col :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, dilation_data: *i64, dilation_len: i32, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_im2col(&outs[0], self.t, kernel_size_data, kernel_size_len, dilation_data, dilation_len, padding_data, padding_len, stride_data, stride_len)
    return wrap_and_track(outs[0])
}

Tensor.im2col_out :: proc(self: *Tensor, out: Tensor, kernel_size_data: *i64, kernel_size_len: i32, dilation_data: *i64, dilation_len: i32, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_im2col_out(&outs[0], out.t, self.t, kernel_size_data, kernel_size_len, dilation_data, dilation_len, padding_data, padding_len, stride_data, stride_len)
    return wrap_and_track(outs[0])
}

Tensor.imag :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_imag(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.index :: proc(self: *Tensor, indices_data: *torch_bindings.Tensor, indices_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index(&outs[0], self.t, indices_data, indices_len)
    return wrap_and_track(outs[0])
}

Tensor.index_add :: proc(self: *Tensor, dim: i64, index: Tensor, source: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_add(&outs[0], self.t, dim, index.t, source.t)
    return wrap_and_track(outs[0])
}

Tensor.index_add_ :: proc(self: *Tensor, dim: i64, index: Tensor, source: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_add_(&outs[0], self.t, dim, index.t, source.t)
    return wrap_and_track(outs[0])
}

Tensor.index_add_out :: proc(self: *Tensor, out: Tensor, dim: i64, index: Tensor, source: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_add_out(&outs[0], out.t, self.t, dim, index.t, source.t)
    return wrap_and_track(outs[0])
}

Tensor.index_copy :: proc(self: *Tensor, dim: i64, index: Tensor, source: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_copy(&outs[0], self.t, dim, index.t, source.t)
    return wrap_and_track(outs[0])
}

Tensor.index_copy_ :: proc(self: *Tensor, dim: i64, index: Tensor, source: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_copy_(&outs[0], self.t, dim, index.t, source.t)
    return wrap_and_track(outs[0])
}

Tensor.index_copy_out :: proc(self: *Tensor, out: Tensor, dim: i64, index: Tensor, source: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_copy_out(&outs[0], out.t, self.t, dim, index.t, source.t)
    return wrap_and_track(outs[0])
}

Tensor.index_fill :: proc(self: *Tensor, dim: i64, index: Tensor, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_fill(&outs[0], self.t, dim, index.t, value)
    return wrap_and_track(outs[0])
}

Tensor.index_fill_ :: proc(self: *Tensor, dim: i64, index: Tensor, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_fill_(&outs[0], self.t, dim, index.t, value)
    return wrap_and_track(outs[0])
}

Tensor.index_fill_int_scalar_out :: proc(self: *Tensor, out: Tensor, dim: i64, index: Tensor, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_fill_int_scalar_out(&outs[0], out.t, self.t, dim, index.t, value)
    return wrap_and_track(outs[0])
}

Tensor.index_fill_int_tensor :: proc(self: *Tensor, dim: i64, index: Tensor, value: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_fill_int_tensor(&outs[0], self.t, dim, index.t, value.t)
    return wrap_and_track(outs[0])
}

Tensor.index_fill_int_tensor_ :: proc(self: *Tensor, dim: i64, index: Tensor, value: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_fill_int_tensor_(&outs[0], self.t, dim, index.t, value.t)
    return wrap_and_track(outs[0])
}

Tensor.index_fill_int_tensor_out :: proc(self: *Tensor, out: Tensor, dim: i64, index: Tensor, value: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_fill_int_tensor_out(&outs[0], out.t, self.t, dim, index.t, value.t)
    return wrap_and_track(outs[0])
}

Tensor.index_put :: proc(self: *Tensor, indices_data: *torch_bindings.Tensor, indices_len: i32, values: Tensor, accumulate: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_put(&outs[0], self.t, indices_data, indices_len, values.t, accumulate)
    return wrap_and_track(outs[0])
}

Tensor.index_put_ :: proc(self: *Tensor, indices_data: *torch_bindings.Tensor, indices_len: i32, values: Tensor, accumulate: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_put_(&outs[0], self.t, indices_data, indices_len, values.t, accumulate)
    return wrap_and_track(outs[0])
}

Tensor.index_put_out :: proc(self: *Tensor, out: Tensor, indices_data: *torch_bindings.Tensor, indices_len: i32, values: Tensor, accumulate: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_put_out(&outs[0], out.t, self.t, indices_data, indices_len, values.t, accumulate)
    return wrap_and_track(outs[0])
}

Tensor.index_reduce :: proc(self: *Tensor, dim: i64, index: Tensor, source: Tensor, reduce_ptr: *u8, reduce_len: i32, include_self: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_reduce(&outs[0], self.t, dim, index.t, source.t, reduce_ptr, reduce_len, include_self)
    return wrap_and_track(outs[0])
}

Tensor.index_reduce_ :: proc(self: *Tensor, dim: i64, index: Tensor, source: Tensor, reduce_ptr: *u8, reduce_len: i32, include_self: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_reduce_(&outs[0], self.t, dim, index.t, source.t, reduce_ptr, reduce_len, include_self)
    return wrap_and_track(outs[0])
}

Tensor.index_reduce_out :: proc(self: *Tensor, out: Tensor, dim: i64, index: Tensor, source: Tensor, reduce_ptr: *u8, reduce_len: i32, include_self: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_reduce_out(&outs[0], out.t, self.t, dim, index.t, source.t, reduce_ptr, reduce_len, include_self)
    return wrap_and_track(outs[0])
}

Tensor.index_select :: proc(self: *Tensor, dim: i64, index: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_select(&outs[0], self.t, dim, index.t)
    return wrap_and_track(outs[0])
}

Tensor.index_select_backward :: proc(grad: Tensor, self_sizes_data: *i64, self_sizes_len: i32, dim: i64, index: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_select_backward(&outs[0], grad.t, self_sizes_data, self_sizes_len, dim, index.t)
    return wrap_and_track(outs[0])
}

Tensor.index_select_out :: proc(self: *Tensor, out: Tensor, dim: i64, index: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_select_out(&outs[0], out.t, self.t, dim, index.t)
    return wrap_and_track(outs[0])
}

Tensor.index_tensor_out :: proc(self: *Tensor, out: Tensor, indices_data: *torch_bindings.Tensor, indices_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_index_tensor_out(&outs[0], out.t, self.t, indices_data, indices_len)
    return wrap_and_track(outs[0])
}

Tensor.indices :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_indices(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.indices_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_indices_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.indices_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_indices_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.infinitely_differentiable_gelu_backward :: proc(self: *Tensor, grad: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_infinitely_differentiable_gelu_backward(&outs[0], grad.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.inner :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_inner(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.inner_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_inner_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.instance_norm :: proc(self: *Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, use_input_stats: bool, momentum: f64, eps: f64, cudnn_enabled: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_instance_norm(&outs[0], self.t, weight.t, bias.t, running_mean.t, running_var.t, use_input_stats, momentum, eps, cudnn_enabled)
    return wrap_and_track(outs[0])
}

Tensor.int_repr :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_int_repr(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.int_repr_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_int_repr_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.inverse :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_inverse(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.inverse_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_inverse_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.is_coalesced :: proc(self: *Tensor) bool {
    return torch_bindings.atg_is_coalesced(self.t)
}

Tensor.is_complex :: proc(self: *Tensor) bool {
    return torch_bindings.atg_is_complex(self.t)
}

Tensor.is_conj :: proc(self: *Tensor) bool {
    return torch_bindings.atg_is_conj(self.t)
}

Tensor.is_distributed :: proc(self: *Tensor) bool {
    return torch_bindings.atg_is_distributed(self.t)
}

Tensor.is_floating_point :: proc(self: *Tensor) bool {
    return torch_bindings.atg_is_floating_point(self.t)
}

Tensor.is_inference :: proc(self: *Tensor) bool {
    return torch_bindings.atg_is_inference(self.t)
}

Tensor.is_leaf :: proc(self: *Tensor) bool {
    return torch_bindings.atg_is_leaf(self.t)
}

Tensor.is_neg :: proc(self: *Tensor) bool {
    return torch_bindings.atg_is_neg(self.t)
}

Tensor.is_nonzero :: proc(self: *Tensor) bool {
    return torch_bindings.atg_is_nonzero(self.t)
}

Tensor.is_pinned :: proc(self: *Tensor, device: i32) bool {
    return torch_bindings.atg_is_pinned(self.t, device)
}

Tensor.is_same_size :: proc(self: *Tensor, other: Tensor) bool {
    return torch_bindings.atg_is_same_size(self.t, other.t)
}

Tensor.is_set_to :: proc(self: *Tensor, tensor_: Tensor) bool {
    return torch_bindings.atg_is_set_to(self.t, tensor_.t)
}

Tensor.is_signed :: proc(self: *Tensor) bool {
    return torch_bindings.atg_is_signed(self.t)
}

Tensor.is_vulkan_available :: proc() bool {
    return torch_bindings.atg_is_vulkan_available()
}

Tensor.isclose :: proc(self: *Tensor, other: Tensor, rtol: f64, atol: f64, equal_nan: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isclose(&outs[0], self.t, other.t, rtol, atol, equal_nan)
    return wrap_and_track(outs[0])
}

Tensor.isfinite :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isfinite(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.isin :: proc(elements: Tensor, test_elements: Tensor, assume_unique: bool, invert: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isin(&outs[0], elements.t, test_elements.t, assume_unique, invert)
    return wrap_and_track(outs[0])
}

Tensor.isin_scalar_tensor :: proc(element: torch_bindings.Scalar, test_elements: Tensor, assume_unique: bool, invert: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isin_scalar_tensor(&outs[0], element, test_elements.t, assume_unique, invert)
    return wrap_and_track(outs[0])
}

Tensor.isin_scalar_tensor_out :: proc(out: Tensor, element: torch_bindings.Scalar, test_elements: Tensor, assume_unique: bool, invert: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isin_scalar_tensor_out(&outs[0], out.t, element, test_elements.t, assume_unique, invert)
    return wrap_and_track(outs[0])
}

Tensor.isin_tensor_scalar :: proc(elements: Tensor, test_element: torch_bindings.Scalar, assume_unique: bool, invert: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isin_tensor_scalar(&outs[0], elements.t, test_element, assume_unique, invert)
    return wrap_and_track(outs[0])
}

Tensor.isin_tensor_scalar_out :: proc(out: Tensor, elements: Tensor, test_element: torch_bindings.Scalar, assume_unique: bool, invert: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isin_tensor_scalar_out(&outs[0], out.t, elements.t, test_element, assume_unique, invert)
    return wrap_and_track(outs[0])
}

Tensor.isin_tensor_tensor_out :: proc(out: Tensor, elements: Tensor, test_elements: Tensor, assume_unique: bool, invert: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isin_tensor_tensor_out(&outs[0], out.t, elements.t, test_elements.t, assume_unique, invert)
    return wrap_and_track(outs[0])
}

Tensor.isinf :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isinf(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.isinf_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isinf_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.isnan :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isnan(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.isnan_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isnan_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.isneginf :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isneginf(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.isneginf_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isneginf_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.isposinf :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isposinf(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.isposinf_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isposinf_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.isreal :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_isreal(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.istft :: proc(self: *Tensor, n_fft: i64, hop_length_v: i64, hop_length_null: u8, win_length_v: i64, win_length_null: u8, window: Tensor, center: bool, normalized: bool, onesided: bool, length_v: i64, length_null: u8, return_complex: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_istft(&outs[0], self.t, n_fft, hop_length_v, hop_length_null, win_length_v, win_length_null, window.t, center, normalized, onesided, length_v, length_null, return_complex)
    return wrap_and_track(outs[0])
}

Tensor.kaiser_window :: proc(window_length: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_kaiser_window(&outs[0], window_length, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.kaiser_window_beta :: proc(window_length: i64, periodic: bool, beta: f64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_kaiser_window_beta(&outs[0], window_length, periodic, beta, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.kaiser_window_beta_out :: proc(out: Tensor, window_length: i64, periodic: bool, beta: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_kaiser_window_beta_out(&outs[0], out.t, window_length, periodic, beta)
    return wrap_and_track(outs[0])
}

Tensor.kaiser_window_out :: proc(out: Tensor, window_length: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_kaiser_window_out(&outs[0], out.t, window_length)
    return wrap_and_track(outs[0])
}

Tensor.kaiser_window_periodic :: proc(window_length: i64, periodic: bool, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_kaiser_window_periodic(&outs[0], window_length, periodic, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.kaiser_window_periodic_out :: proc(out: Tensor, window_length: i64, periodic: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_kaiser_window_periodic_out(&outs[0], out.t, window_length, periodic)
    return wrap_and_track(outs[0])
}

Tensor.kl_div :: proc(self: *Tensor, target: Tensor, reduction: i64, log_target: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_kl_div(&outs[0], self.t, target.t, reduction, log_target)
    return wrap_and_track(outs[0])
}

Tensor.kron :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_kron(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.kron_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_kron_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.kthvalue :: proc(self: *Tensor, k: i64, dim: i64, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_kthvalue(&outs[0], self.t, k, dim, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.kthvalue_values :: proc(self: *Tensor, values: Tensor, indices: Tensor, k: i64, dim: i64, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_kthvalue_values(&outs[0], values.t, indices.t, self.t, k, dim, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.l1_loss :: proc(self: *Tensor, target: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_l1_loss(&outs[0], self.t, target.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.layer_norm :: proc(self: *Tensor, normalized_shape_data: *i64, normalized_shape_len: i32, weight: Tensor, bias: Tensor, eps: f64, cudnn_enable: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_layer_norm(&outs[0], self.t, normalized_shape_data, normalized_shape_len, weight.t, bias.t, eps, cudnn_enable)
    return wrap_and_track(outs[0])
}

Tensor.lcm :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lcm(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.lcm_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lcm_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.lcm_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lcm_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.ldexp :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ldexp(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.ldexp_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ldexp_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.ldexp_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ldexp_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.le :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_le(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.le_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_le_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.le_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_le_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.le_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_le_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.le_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_le_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.le_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_le_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.leaky_relu :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_leaky_relu(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.leaky_relu_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_leaky_relu_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.leaky_relu_backward :: proc(self: *Tensor, grad_output: Tensor, negative_slope: torch_bindings.Scalar, self_is_result: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_leaky_relu_backward(&outs[0], grad_output.t, self.t, negative_slope, self_is_result)
    return wrap_and_track(outs[0])
}

Tensor.leaky_relu_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, negative_slope: torch_bindings.Scalar, self_is_result: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_leaky_relu_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, negative_slope, self_is_result)
    return wrap_and_track(outs[0])
}

Tensor.leaky_relu_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_leaky_relu_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.lerp :: proc(self: *Tensor, end: Tensor, weight: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lerp(&outs[0], self.t, end.t, weight)
    return wrap_and_track(outs[0])
}

Tensor.lerp_ :: proc(self: *Tensor, end: Tensor, weight: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lerp_(&outs[0], self.t, end.t, weight)
    return wrap_and_track(outs[0])
}

Tensor.lerp_scalar_out :: proc(self: *Tensor, out: Tensor, end: Tensor, weight: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lerp_scalar_out(&outs[0], out.t, self.t, end.t, weight)
    return wrap_and_track(outs[0])
}

Tensor.lerp_tensor :: proc(self: *Tensor, end: Tensor, weight: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lerp_tensor(&outs[0], self.t, end.t, weight.t)
    return wrap_and_track(outs[0])
}

Tensor.lerp_tensor_ :: proc(self: *Tensor, end: Tensor, weight: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lerp_tensor_(&outs[0], self.t, end.t, weight.t)
    return wrap_and_track(outs[0])
}

Tensor.lerp_tensor_out :: proc(self: *Tensor, out: Tensor, end: Tensor, weight: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lerp_tensor_out(&outs[0], out.t, self.t, end.t, weight.t)
    return wrap_and_track(outs[0])
}

Tensor.less :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_less(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.less_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_less_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.less_equal :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_less_equal(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.less_equal_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_less_equal_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.less_equal_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_less_equal_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.less_equal_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_less_equal_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.less_equal_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_less_equal_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.less_equal_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_less_equal_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.less_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_less_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.less_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_less_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.less_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_less_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.less_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_less_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.lgamma :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lgamma(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.lgamma_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lgamma_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.lgamma_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lgamma_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.lift :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lift(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.lift_fresh :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lift_fresh(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.lift_fresh_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lift_fresh_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.lift_fresh_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lift_fresh_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.lift_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lift_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.linalg_cholesky :: proc(self: *Tensor, upper: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_cholesky(&outs[0], self.t, upper)
    return wrap_and_track(outs[0])
}

Tensor.linalg_cholesky_ex :: proc(self: *Tensor, upper: bool, check_errors: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_cholesky_ex(&outs[0], self.t, upper, check_errors)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_cholesky_ex_l :: proc(self: *Tensor, L: Tensor, info: Tensor, upper: bool, check_errors: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_cholesky_ex_l(&outs[0], L.t, info.t, self.t, upper, check_errors)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_cholesky_out :: proc(self: *Tensor, out: Tensor, upper: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_cholesky_out(&outs[0], out.t, self.t, upper)
    return wrap_and_track(outs[0])
}

Tensor.linalg_cond :: proc(self: *Tensor, p: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_cond(&outs[0], self.t, p)
    return wrap_and_track(outs[0])
}

Tensor.linalg_cond_out :: proc(self: *Tensor, out: Tensor, p: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_cond_out(&outs[0], out.t, self.t, p)
    return wrap_and_track(outs[0])
}

Tensor.linalg_cond_p_str :: proc(self: *Tensor, p_ptr: *u8, p_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_cond_p_str(&outs[0], self.t, p_ptr, p_len)
    return wrap_and_track(outs[0])
}

Tensor.linalg_cond_p_str_out :: proc(self: *Tensor, out: Tensor, p_ptr: *u8, p_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_cond_p_str_out(&outs[0], out.t, self.t, p_ptr, p_len)
    return wrap_and_track(outs[0])
}

Tensor.linalg_cross :: proc(self: *Tensor, other: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_cross(&outs[0], self.t, other.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.linalg_cross_out :: proc(self: *Tensor, out: Tensor, other: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_cross_out(&outs[0], out.t, self.t, other.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.linalg_det :: proc(A: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_det(&outs[0], A.t)
    return wrap_and_track(outs[0])
}

Tensor.linalg_det_out :: proc(out: Tensor, A: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_det_out(&outs[0], out.t, A.t)
    return wrap_and_track(outs[0])
}

Tensor.linalg_diagonal :: proc(A: Tensor, offset: i64, dim1: i64, dim2: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_diagonal(&outs[0], A.t, offset, dim1, dim2)
    return wrap_and_track(outs[0])
}

Tensor.linalg_eig :: proc(self: *Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_eig(&outs[0], self.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_eig_out :: proc(self: *Tensor, eigenvalues: Tensor, eigenvectors: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_eig_out(&outs[0], eigenvalues.t, eigenvectors.t, self.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_eigh :: proc(self: *Tensor, UPLO_ptr: *u8, UPLO_len: i32) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_eigh(&outs[0], self.t, UPLO_ptr, UPLO_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_eigh_eigvals :: proc(self: *Tensor, eigvals: Tensor, eigvecs: Tensor, UPLO_ptr: *u8, UPLO_len: i32) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_eigh_eigvals(&outs[0], eigvals.t, eigvecs.t, self.t, UPLO_ptr, UPLO_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_eigvals :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_eigvals(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.linalg_eigvals_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_eigvals_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.linalg_eigvalsh :: proc(self: *Tensor, UPLO_ptr: *u8, UPLO_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_eigvalsh(&outs[0], self.t, UPLO_ptr, UPLO_len)
    return wrap_and_track(outs[0])
}

Tensor.linalg_eigvalsh_out :: proc(self: *Tensor, out: Tensor, UPLO_ptr: *u8, UPLO_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_eigvalsh_out(&outs[0], out.t, self.t, UPLO_ptr, UPLO_len)
    return wrap_and_track(outs[0])
}

Tensor.linalg_householder_product :: proc(self: *Tensor, tau: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_householder_product(&outs[0], self.t, tau.t)
    return wrap_and_track(outs[0])
}

Tensor.linalg_householder_product_out :: proc(self: *Tensor, out: Tensor, tau: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_householder_product_out(&outs[0], out.t, self.t, tau.t)
    return wrap_and_track(outs[0])
}

Tensor.linalg_inv :: proc(A: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_inv(&outs[0], A.t)
    return wrap_and_track(outs[0])
}

Tensor.linalg_inv_ex :: proc(A: Tensor, check_errors: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_inv_ex(&outs[0], A.t, check_errors)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_inv_ex_inverse :: proc(inverse: Tensor, info: Tensor, A: Tensor, check_errors: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_inv_ex_inverse(&outs[0], inverse.t, info.t, A.t, check_errors)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_inv_out :: proc(out: Tensor, A: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_inv_out(&outs[0], out.t, A.t)
    return wrap_and_track(outs[0])
}

Tensor.linalg_ldl_factor :: proc(self: *Tensor, hermitian: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_ldl_factor(&outs[0], self.t, hermitian)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_ldl_factor_ex :: proc(self: *Tensor, hermitian: bool, check_errors: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_ldl_factor_ex(&outs[0], self.t, hermitian, check_errors)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.linalg_ldl_factor_ex_out :: proc(self: *Tensor, LD: Tensor, pivots: Tensor, info: Tensor, hermitian: bool, check_errors: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_ldl_factor_ex_out(&outs[0], LD.t, pivots.t, info.t, self.t, hermitian, check_errors)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.linalg_ldl_factor_out :: proc(self: *Tensor, LD: Tensor, pivots: Tensor, hermitian: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_ldl_factor_out(&outs[0], LD.t, pivots.t, self.t, hermitian)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_ldl_solve :: proc(LD: Tensor, pivots: Tensor, B: Tensor, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_ldl_solve(&outs[0], LD.t, pivots.t, B.t, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_ldl_solve_out :: proc(out: Tensor, LD: Tensor, pivots: Tensor, B: Tensor, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_ldl_solve_out(&outs[0], out.t, LD.t, pivots.t, B.t, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_lstsq :: proc(self: *Tensor, b: Tensor, rcond_v: f64, rcond_null: u8, driver_ptr: *u8, driver_len: i32) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_lstsq(&outs[0], self.t, b.t, rcond_v, rcond_null, driver_ptr, driver_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor.linalg_lstsq_out :: proc(self: *Tensor, solution: Tensor, residuals: Tensor, rank: Tensor, singular_values: Tensor, b: Tensor, rcond_v: f64, rcond_null: u8, driver_ptr: *u8, driver_len: i32) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_lstsq_out(&outs[0], solution.t, residuals.t, rank.t, singular_values.t, self.t, b.t, rcond_v, rcond_null, driver_ptr, driver_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor.linalg_lu :: proc(A: Tensor, pivot: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_lu(&outs[0], A.t, pivot)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.linalg_lu_factor :: proc(A: Tensor, pivot: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_lu_factor(&outs[0], A.t, pivot)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_lu_factor_ex :: proc(A: Tensor, pivot: bool, check_errors: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_lu_factor_ex(&outs[0], A.t, pivot, check_errors)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.linalg_lu_factor_ex_out :: proc(LU: Tensor, pivots: Tensor, info: Tensor, A: Tensor, pivot: bool, check_errors: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_lu_factor_ex_out(&outs[0], LU.t, pivots.t, info.t, A.t, pivot, check_errors)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.linalg_lu_factor_out :: proc(LU: Tensor, pivots: Tensor, A: Tensor, pivot: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_lu_factor_out(&outs[0], LU.t, pivots.t, A.t, pivot)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_lu_out :: proc(P: Tensor, L: Tensor, U: Tensor, A: Tensor, pivot: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_lu_out(&outs[0], P.t, L.t, U.t, A.t, pivot)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.linalg_lu_solve :: proc(LU: Tensor, pivots: Tensor, B: Tensor, left: bool, adjoint: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_lu_solve(&outs[0], LU.t, pivots.t, B.t, left, adjoint)
    return wrap_and_track(outs[0])
}

Tensor.linalg_lu_solve_out :: proc(out: Tensor, LU: Tensor, pivots: Tensor, B: Tensor, left: bool, adjoint: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_lu_solve_out(&outs[0], out.t, LU.t, pivots.t, B.t, left, adjoint)
    return wrap_and_track(outs[0])
}

Tensor.linalg_matmul :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_matmul(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.linalg_matmul_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_matmul_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.linalg_matrix_exp :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_matrix_exp(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.linalg_matrix_exp_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_matrix_exp_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.linalg_matrix_power :: proc(self: *Tensor, n: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_matrix_power(&outs[0], self.t, n)
    return wrap_and_track(outs[0])
}

Tensor.linalg_matrix_power_out :: proc(self: *Tensor, out: Tensor, n: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_matrix_power_out(&outs[0], out.t, self.t, n)
    return wrap_and_track(outs[0])
}

Tensor.linalg_matrix_rank :: proc(self: *Tensor, tol: f64, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_matrix_rank(&outs[0], self.t, tol, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_matrix_rank_atol_rtol_float :: proc(self: *Tensor, atol_v: f64, atol_null: u8, rtol_v: f64, rtol_null: u8, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_matrix_rank_atol_rtol_float(&outs[0], self.t, atol_v, atol_null, rtol_v, rtol_null, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_matrix_rank_atol_rtol_float_out :: proc(self: *Tensor, out: Tensor, atol_v: f64, atol_null: u8, rtol_v: f64, rtol_null: u8, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_matrix_rank_atol_rtol_float_out(&outs[0], out.t, self.t, atol_v, atol_null, rtol_v, rtol_null, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_matrix_rank_atol_rtol_tensor :: proc(self: *Tensor, atol: Tensor, rtol: Tensor, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_matrix_rank_atol_rtol_tensor(&outs[0], self.t, atol.t, rtol.t, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_matrix_rank_atol_rtol_tensor_out :: proc(self: *Tensor, out: Tensor, atol: Tensor, rtol: Tensor, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_matrix_rank_atol_rtol_tensor_out(&outs[0], out.t, self.t, atol.t, rtol.t, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_matrix_rank_out :: proc(self: *Tensor, out: Tensor, tol: f64, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_matrix_rank_out(&outs[0], out.t, self.t, tol, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_matrix_rank_out_tol_tensor :: proc(self: *Tensor, out: Tensor, tol: Tensor, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_matrix_rank_out_tol_tensor(&outs[0], out.t, self.t, tol.t, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_matrix_rank_tol_tensor :: proc(self: *Tensor, tol: Tensor, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_matrix_rank_tol_tensor(&outs[0], self.t, tol.t, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_multi_dot :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_multi_dot(&outs[0], tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.linalg_multi_dot_out :: proc(out: Tensor, tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_multi_dot_out(&outs[0], out.t, tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.linalg_norm :: proc(self: *Tensor, ord: torch_bindings.Scalar, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_norm(&outs[0], self.t, ord, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.linalg_norm_ord_str :: proc(self: *Tensor, ord_ptr: *u8, ord_len: i32, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_norm_ord_str(&outs[0], self.t, ord_ptr, ord_len, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.linalg_norm_ord_str_out :: proc(self: *Tensor, out: Tensor, ord_ptr: *u8, ord_len: i32, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_norm_ord_str_out(&outs[0], out.t, self.t, ord_ptr, ord_len, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.linalg_norm_out :: proc(self: *Tensor, out: Tensor, ord: torch_bindings.Scalar, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_norm_out(&outs[0], out.t, self.t, ord, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.linalg_pinv :: proc(self: *Tensor, rcond: f64, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_pinv(&outs[0], self.t, rcond, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_pinv_atol_rtol_float :: proc(self: *Tensor, atol_v: f64, atol_null: u8, rtol_v: f64, rtol_null: u8, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_pinv_atol_rtol_float(&outs[0], self.t, atol_v, atol_null, rtol_v, rtol_null, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_pinv_atol_rtol_float_out :: proc(self: *Tensor, out: Tensor, atol_v: f64, atol_null: u8, rtol_v: f64, rtol_null: u8, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_pinv_atol_rtol_float_out(&outs[0], out.t, self.t, atol_v, atol_null, rtol_v, rtol_null, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_pinv_atol_rtol_tensor :: proc(self: *Tensor, atol: Tensor, rtol: Tensor, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_pinv_atol_rtol_tensor(&outs[0], self.t, atol.t, rtol.t, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_pinv_atol_rtol_tensor_out :: proc(self: *Tensor, out: Tensor, atol: Tensor, rtol: Tensor, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_pinv_atol_rtol_tensor_out(&outs[0], out.t, self.t, atol.t, rtol.t, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_pinv_out :: proc(self: *Tensor, out: Tensor, rcond: f64, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_pinv_out(&outs[0], out.t, self.t, rcond, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_pinv_out_rcond_tensor :: proc(self: *Tensor, out: Tensor, rcond: Tensor, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_pinv_out_rcond_tensor(&outs[0], out.t, self.t, rcond.t, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_pinv_rcond_tensor :: proc(self: *Tensor, rcond: Tensor, hermitian: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_pinv_rcond_tensor(&outs[0], self.t, rcond.t, hermitian)
    return wrap_and_track(outs[0])
}

Tensor.linalg_qr :: proc(A: Tensor, mode_ptr: *u8, mode_len: i32) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_qr(&outs[0], A.t, mode_ptr, mode_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_qr_out :: proc(Q: Tensor, R: Tensor, A: Tensor, mode_ptr: *u8, mode_len: i32) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_qr_out(&outs[0], Q.t, R.t, A.t, mode_ptr, mode_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_slogdet :: proc(A: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_slogdet(&outs[0], A.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_slogdet_out :: proc(sign: Tensor, logabsdet: Tensor, A: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_slogdet_out(&outs[0], sign.t, logabsdet.t, A.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_solve :: proc(A: Tensor, B: Tensor, left: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_solve(&outs[0], A.t, B.t, left)
    return wrap_and_track(outs[0])
}

Tensor.linalg_solve_ex :: proc(A: Tensor, B: Tensor, left: bool, check_errors: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_solve_ex(&outs[0], A.t, B.t, left, check_errors)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_solve_ex_out :: proc(result: Tensor, info: Tensor, A: Tensor, B: Tensor, left: bool, check_errors: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_solve_ex_out(&outs[0], result.t, info.t, A.t, B.t, left, check_errors)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.linalg_solve_out :: proc(out: Tensor, A: Tensor, B: Tensor, left: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_solve_out(&outs[0], out.t, A.t, B.t, left)
    return wrap_and_track(outs[0])
}

Tensor.linalg_solve_triangular :: proc(self: *Tensor, B: Tensor, upper: bool, left: bool, unitriangular: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_solve_triangular(&outs[0], self.t, B.t, upper, left, unitriangular)
    return wrap_and_track(outs[0])
}

Tensor.linalg_solve_triangular_out :: proc(self: *Tensor, out: Tensor, B: Tensor, upper: bool, left: bool, unitriangular: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_solve_triangular_out(&outs[0], out.t, self.t, B.t, upper, left, unitriangular)
    return wrap_and_track(outs[0])
}

Tensor.linalg_svd :: proc(A: Tensor, full_matrices: bool, driver_ptr: *u8, driver_len: i32) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_svd(&outs[0], A.t, full_matrices, driver_ptr, driver_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.linalg_svd_u :: proc(U: Tensor, S: Tensor, Vh: Tensor, A: Tensor, full_matrices: bool, driver_ptr: *u8, driver_len: i32) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_svd_u(&outs[0], U.t, S.t, Vh.t, A.t, full_matrices, driver_ptr, driver_len)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.linalg_svdvals :: proc(A: Tensor, driver_ptr: *u8, driver_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_svdvals(&outs[0], A.t, driver_ptr, driver_len)
    return wrap_and_track(outs[0])
}

Tensor.linalg_svdvals_out :: proc(out: Tensor, A: Tensor, driver_ptr: *u8, driver_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_svdvals_out(&outs[0], out.t, A.t, driver_ptr, driver_len)
    return wrap_and_track(outs[0])
}

Tensor.linalg_tensorinv :: proc(self: *Tensor, ind: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_tensorinv(&outs[0], self.t, ind)
    return wrap_and_track(outs[0])
}

Tensor.linalg_tensorinv_out :: proc(self: *Tensor, out: Tensor, ind: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_tensorinv_out(&outs[0], out.t, self.t, ind)
    return wrap_and_track(outs[0])
}

Tensor.linalg_tensorsolve :: proc(self: *Tensor, other: Tensor, dims_data: *i64, dims_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_tensorsolve(&outs[0], self.t, other.t, dims_data, dims_len)
    return wrap_and_track(outs[0])
}

Tensor.linalg_tensorsolve_out :: proc(self: *Tensor, out: Tensor, other: Tensor, dims_data: *i64, dims_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_tensorsolve_out(&outs[0], out.t, self.t, other.t, dims_data, dims_len)
    return wrap_and_track(outs[0])
}

Tensor.linalg_vander :: proc(x: Tensor, n_v: i64, n_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_vander(&outs[0], x.t, n_v, n_null)
    return wrap_and_track(outs[0])
}

Tensor.linalg_vecdot :: proc(x: Tensor, y: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_vecdot(&outs[0], x.t, y.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.linalg_vecdot_out :: proc(out: Tensor, x: Tensor, y: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linalg_vecdot_out(&outs[0], out.t, x.t, y.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.linear :: proc(self: *Tensor, weight: Tensor, bias: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linear(&outs[0], self.t, weight.t, bias.t)
    return wrap_and_track(outs[0])
}

Tensor.linear_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linear_out(&outs[0], out.t, self.t, weight.t, bias.t)
    return wrap_and_track(outs[0])
}

Tensor.linspace :: proc(start: torch_bindings.Scalar, end: torch_bindings.Scalar, steps: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linspace(&outs[0], start, end, steps, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.linspace_out :: proc(out: Tensor, start: torch_bindings.Scalar, end: torch_bindings.Scalar, steps: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linspace_out(&outs[0], out.t, start, end, steps)
    return wrap_and_track(outs[0])
}

Tensor.linspace_scalar_tensor :: proc(start: torch_bindings.Scalar, end: Tensor, steps: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linspace_scalar_tensor(&outs[0], start, end.t, steps, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.linspace_scalar_tensor_out :: proc(out: Tensor, start: torch_bindings.Scalar, end: Tensor, steps: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linspace_scalar_tensor_out(&outs[0], out.t, start, end.t, steps)
    return wrap_and_track(outs[0])
}

Tensor.linspace_tensor_scalar :: proc(start: Tensor, end: torch_bindings.Scalar, steps: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linspace_tensor_scalar(&outs[0], start.t, end, steps, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.linspace_tensor_scalar_out :: proc(out: Tensor, start: Tensor, end: torch_bindings.Scalar, steps: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linspace_tensor_scalar_out(&outs[0], out.t, start.t, end, steps)
    return wrap_and_track(outs[0])
}

Tensor.linspace_tensor_tensor :: proc(start: Tensor, end: Tensor, steps: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linspace_tensor_tensor(&outs[0], start.t, end.t, steps, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.linspace_tensor_tensor_out :: proc(out: Tensor, start: Tensor, end: Tensor, steps: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_linspace_tensor_tensor_out(&outs[0], out.t, start.t, end.t, steps)
    return wrap_and_track(outs[0])
}

Tensor.log :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.log10 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log10(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.log10_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log10_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.log10_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log10_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.log1p :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log1p(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.log1p_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log1p_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.log1p_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log1p_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.log2 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log2(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.log2_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log2_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.log2_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log2_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.log_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.log_normal :: proc(self: *Tensor, mean: f64, std: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log_normal(&outs[0], self.t, mean, std)
    return wrap_and_track(outs[0])
}

Tensor.log_normal_ :: proc(self: *Tensor, mean: f64, std: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log_normal_(&outs[0], self.t, mean, std)
    return wrap_and_track(outs[0])
}

Tensor.log_normal_out :: proc(self: *Tensor, out: Tensor, mean: f64, std: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log_normal_out(&outs[0], out.t, self.t, mean, std)
    return wrap_and_track(outs[0])
}

Tensor.log_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.log_sigmoid :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log_sigmoid(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.log_sigmoid_backward :: proc(self: *Tensor, grad_output: Tensor, buffer: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log_sigmoid_backward(&outs[0], grad_output.t, self.t, buffer.t)
    return wrap_and_track(outs[0])
}

Tensor.log_sigmoid_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, buffer: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log_sigmoid_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, buffer.t)
    return wrap_and_track(outs[0])
}

Tensor.log_sigmoid_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log_sigmoid_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.log_softmax :: proc(self: *Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log_softmax(&outs[0], self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.log_softmax_int_out :: proc(self: *Tensor, out: Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_log_softmax_int_out(&outs[0], out.t, self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.logaddexp :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logaddexp(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.logaddexp2 :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logaddexp2(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.logaddexp2_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logaddexp2_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.logaddexp_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logaddexp_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.logcumsumexp :: proc(self: *Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logcumsumexp(&outs[0], self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.logcumsumexp_out :: proc(self: *Tensor, out: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logcumsumexp_out(&outs[0], out.t, self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.logdet :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logdet(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.logical_and :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logical_and(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.logical_and_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logical_and_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.logical_and_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logical_and_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.logical_not :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logical_not(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.logical_not_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logical_not_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.logical_not_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logical_not_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.logical_or :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logical_or(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.logical_or_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logical_or_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.logical_or_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logical_or_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.logical_xor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logical_xor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.logical_xor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logical_xor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.logical_xor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logical_xor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.logit :: proc(self: *Tensor, eps_v: f64, eps_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logit(&outs[0], self.t, eps_v, eps_null)
    return wrap_and_track(outs[0])
}

Tensor.logit_ :: proc(self: *Tensor, eps_v: f64, eps_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logit_(&outs[0], self.t, eps_v, eps_null)
    return wrap_and_track(outs[0])
}

Tensor.logit_backward :: proc(self: *Tensor, grad_output: Tensor, eps_v: f64, eps_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logit_backward(&outs[0], grad_output.t, self.t, eps_v, eps_null)
    return wrap_and_track(outs[0])
}

Tensor.logit_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, eps_v: f64, eps_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logit_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, eps_v, eps_null)
    return wrap_and_track(outs[0])
}

Tensor.logit_out :: proc(self: *Tensor, out: Tensor, eps_v: f64, eps_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logit_out(&outs[0], out.t, self.t, eps_v, eps_null)
    return wrap_and_track(outs[0])
}

Tensor.logspace :: proc(start: torch_bindings.Scalar, end: torch_bindings.Scalar, steps: i64, base: f64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logspace(&outs[0], start, end, steps, base, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.logspace_out :: proc(out: Tensor, start: torch_bindings.Scalar, end: torch_bindings.Scalar, steps: i64, base: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logspace_out(&outs[0], out.t, start, end, steps, base)
    return wrap_and_track(outs[0])
}

Tensor.logspace_scalar_tensor :: proc(start: torch_bindings.Scalar, end: Tensor, steps: i64, base: f64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logspace_scalar_tensor(&outs[0], start, end.t, steps, base, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.logspace_scalar_tensor_out :: proc(out: Tensor, start: torch_bindings.Scalar, end: Tensor, steps: i64, base: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logspace_scalar_tensor_out(&outs[0], out.t, start, end.t, steps, base)
    return wrap_and_track(outs[0])
}

Tensor.logspace_tensor_scalar :: proc(start: Tensor, end: torch_bindings.Scalar, steps: i64, base: f64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logspace_tensor_scalar(&outs[0], start.t, end, steps, base, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.logspace_tensor_scalar_out :: proc(out: Tensor, start: Tensor, end: torch_bindings.Scalar, steps: i64, base: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logspace_tensor_scalar_out(&outs[0], out.t, start.t, end, steps, base)
    return wrap_and_track(outs[0])
}

Tensor.logspace_tensor_tensor :: proc(start: Tensor, end: Tensor, steps: i64, base: f64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logspace_tensor_tensor(&outs[0], start.t, end.t, steps, base, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.logspace_tensor_tensor_out :: proc(out: Tensor, start: Tensor, end: Tensor, steps: i64, base: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logspace_tensor_tensor_out(&outs[0], out.t, start.t, end.t, steps, base)
    return wrap_and_track(outs[0])
}

Tensor.logsumexp :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logsumexp(&outs[0], self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.logsumexp_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_logsumexp_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.lstm :: proc(self: *Tensor, hx_data: *torch_bindings.Tensor, hx_len: i32, params_data: *torch_bindings.Tensor, params_len: i32, has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool, batch_first: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_lstm(&outs[0], self.t, hx_data, hx_len, params_data, params_len, has_biases, num_layers, dropout, train, bidirectional, batch_first)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.lstm_cell :: proc(self: *Tensor, hx_data: *torch_bindings.Tensor, hx_len: i32, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_lstm_cell(&outs[0], self.t, hx_data, hx_len, w_ih.t, w_hh.t, b_ih.t, b_hh.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.lstm_data :: proc(data: Tensor, batch_sizes: Tensor, hx_data: *torch_bindings.Tensor, hx_len: i32, params_data: *torch_bindings.Tensor, params_len: i32, has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_lstm_data(&outs[0], data.t, batch_sizes.t, hx_data, hx_len, params_data, params_len, has_biases, num_layers, dropout, train, bidirectional)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.lstm_mps_backward :: proc(self: *Tensor, out0: Tensor, out1_data: *torch_bindings.Tensor, out1_len: i32, out2_data: *torch_bindings.Tensor, out2_len: i32, grad_y: Tensor, grad_hy: Tensor, grad_cy: Tensor, z_state: Tensor, cell_state_fwd: Tensor, layersOutputs: Tensor, hx_data: *torch_bindings.Tensor, hx_len: i32, params_data: *torch_bindings.Tensor, params_len: i32, has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool, batch_first: bool) void {
    torch_bindings.atg_lstm_mps_backward(out0.t, out1_data, out1_len, out2_data, out2_len, grad_y.t, grad_hy.t, grad_cy.t, z_state.t, cell_state_fwd.t, self.t, layersOutputs.t, hx_data, hx_len, params_data, params_len, has_biases, num_layers, dropout, train, bidirectional, batch_first)
}

Tensor.lt :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lt(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.lt_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lt_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.lt_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lt_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.lt_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lt_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.lt_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lt_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.lt_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lt_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.lu_solve :: proc(self: *Tensor, LU_data: Tensor, LU_pivots: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lu_solve(&outs[0], self.t, LU_data.t, LU_pivots.t)
    return wrap_and_track(outs[0])
}

Tensor.lu_solve_out :: proc(self: *Tensor, out: Tensor, LU_data: Tensor, LU_pivots: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_lu_solve_out(&outs[0], out.t, self.t, LU_data.t, LU_pivots.t)
    return wrap_and_track(outs[0])
}

Tensor.lu_unpack :: proc(LU_data: Tensor, LU_pivots: Tensor, unpack_data: bool, unpack_pivots: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_lu_unpack(&outs[0], LU_data.t, LU_pivots.t, unpack_data, unpack_pivots)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.lu_unpack_out :: proc(P: Tensor, L: Tensor, U: Tensor, LU_data: Tensor, LU_pivots: Tensor, unpack_data: bool, unpack_pivots: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_lu_unpack_out(&outs[0], P.t, L.t, U.t, LU_data.t, LU_pivots.t, unpack_data, unpack_pivots)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.margin_ranking_loss :: proc(input1: Tensor, input2: Tensor, target: Tensor, margin: f64, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_margin_ranking_loss(&outs[0], input1.t, input2.t, target.t, margin, reduction)
    return wrap_and_track(outs[0])
}

Tensor.masked_fill :: proc(self: *Tensor, mask: Tensor, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_masked_fill(&outs[0], self.t, mask.t, value)
    return wrap_and_track(outs[0])
}

Tensor.masked_fill_ :: proc(self: *Tensor, mask: Tensor, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_masked_fill_(&outs[0], self.t, mask.t, value)
    return wrap_and_track(outs[0])
}

Tensor.masked_fill_scalar_out :: proc(self: *Tensor, out: Tensor, mask: Tensor, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_masked_fill_scalar_out(&outs[0], out.t, self.t, mask.t, value)
    return wrap_and_track(outs[0])
}

Tensor.masked_fill_tensor :: proc(self: *Tensor, mask: Tensor, value: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_masked_fill_tensor(&outs[0], self.t, mask.t, value.t)
    return wrap_and_track(outs[0])
}

Tensor.masked_fill_tensor_ :: proc(self: *Tensor, mask: Tensor, value: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_masked_fill_tensor_(&outs[0], self.t, mask.t, value.t)
    return wrap_and_track(outs[0])
}

Tensor.masked_fill_tensor_out :: proc(self: *Tensor, out: Tensor, mask: Tensor, value: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_masked_fill_tensor_out(&outs[0], out.t, self.t, mask.t, value.t)
    return wrap_and_track(outs[0])
}

Tensor.masked_scatter :: proc(self: *Tensor, mask: Tensor, source: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_masked_scatter(&outs[0], self.t, mask.t, source.t)
    return wrap_and_track(outs[0])
}

Tensor.masked_scatter_ :: proc(self: *Tensor, mask: Tensor, source: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_masked_scatter_(&outs[0], self.t, mask.t, source.t)
    return wrap_and_track(outs[0])
}

Tensor.masked_scatter_backward :: proc(grad_output: Tensor, mask: Tensor, sizes_data: *i64, sizes_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_masked_scatter_backward(&outs[0], grad_output.t, mask.t, sizes_data, sizes_len)
    return wrap_and_track(outs[0])
}

Tensor.masked_scatter_out :: proc(self: *Tensor, out: Tensor, mask: Tensor, source: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_masked_scatter_out(&outs[0], out.t, self.t, mask.t, source.t)
    return wrap_and_track(outs[0])
}

Tensor.masked_select :: proc(self: *Tensor, mask: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_masked_select(&outs[0], self.t, mask.t)
    return wrap_and_track(outs[0])
}

Tensor.masked_select_backward :: proc(self: *Tensor, grad: Tensor, mask: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_masked_select_backward(&outs[0], grad.t, self.t, mask.t)
    return wrap_and_track(outs[0])
}

Tensor.masked_select_out :: proc(self: *Tensor, out: Tensor, mask: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_masked_select_out(&outs[0], out.t, self.t, mask.t)
    return wrap_and_track(outs[0])
}

Tensor.matmul :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_matmul(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.matmul_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_matmul_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.matrix_exp :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_matrix_exp(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.matrix_exp_backward :: proc(self: *Tensor, grad: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_matrix_exp_backward(&outs[0], self.t, grad.t)
    return wrap_and_track(outs[0])
}

Tensor.matrix_h :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_matrix_h(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.matrix_power :: proc(self: *Tensor, n: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_matrix_power(&outs[0], self.t, n)
    return wrap_and_track(outs[0])
}

Tensor.matrix_power_out :: proc(self: *Tensor, out: Tensor, n: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_matrix_power_out(&outs[0], out.t, self.t, n)
    return wrap_and_track(outs[0])
}

Tensor.max :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.max_dim :: proc(self: *Tensor, dim: i64, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_dim(&outs[0], self.t, dim, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.max_dim_max :: proc(self: *Tensor, max: Tensor, max_values: Tensor, dim: i64, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_dim_max(&outs[0], max.t, max_values.t, self.t, dim, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.max_other :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_other(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.max_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.max_pool1d :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_pool1d(&outs[0], self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.max_pool1d_with_indices :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_pool1d_with_indices(&outs[0], self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.max_pool2d :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_pool2d(&outs[0], self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.max_pool2d_backward :: proc(self: *Tensor, grad_output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_pool2d_backward(&outs[0], grad_output.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.max_pool2d_backward_out :: proc(self: *Tensor, out: Tensor, grad_output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_pool2d_backward_out(&outs[0], out.t, grad_output.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.max_pool2d_with_indices :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_pool2d_with_indices(&outs[0], self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.max_pool2d_with_indices_backward :: proc(self: *Tensor, grad_output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool, indices: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_pool2d_with_indices_backward(&outs[0], grad_output.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode, indices.t)
    return wrap_and_track(outs[0])
}

Tensor.max_pool2d_with_indices_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool, indices: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_pool2d_with_indices_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode, indices.t)
    return wrap_and_track(outs[0])
}

Tensor.max_pool2d_with_indices_out :: proc(self: *Tensor, out: Tensor, indices: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_pool2d_with_indices_out(&outs[0], out.t, indices.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.max_pool3d :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_pool3d(&outs[0], self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.max_pool3d_with_indices :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_pool3d_with_indices(&outs[0], self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.max_pool3d_with_indices_backward :: proc(self: *Tensor, grad_output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool, indices: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_pool3d_with_indices_backward(&outs[0], grad_output.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode, indices.t)
    return wrap_and_track(outs[0])
}

Tensor.max_pool3d_with_indices_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool, indices: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_pool3d_with_indices_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode, indices.t)
    return wrap_and_track(outs[0])
}

Tensor.max_pool3d_with_indices_out :: proc(self: *Tensor, out: Tensor, indices: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_pool3d_with_indices_out(&outs[0], out.t, indices.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.max_unary_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_unary_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.max_unpool2d :: proc(self: *Tensor, indices: Tensor, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_unpool2d(&outs[0], self.t, indices.t, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor.max_unpool2d_out :: proc(self: *Tensor, out: Tensor, indices: Tensor, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_unpool2d_out(&outs[0], out.t, self.t, indices.t, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor.max_unpool3d :: proc(self: *Tensor, indices: Tensor, output_size_data: *i64, output_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_unpool3d(&outs[0], self.t, indices.t, output_size_data, output_size_len, stride_data, stride_len, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.max_unpool3d_out :: proc(self: *Tensor, out: Tensor, indices: Tensor, output_size_data: *i64, output_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_max_unpool3d_out(&outs[0], out.t, self.t, indices.t, output_size_data, output_size_len, stride_data, stride_len, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.maximum :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_maximum(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.maximum_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_maximum_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.mean :: proc(self: *Tensor, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mean(&outs[0], self.t, dtype)
    return wrap_and_track(outs[0])
}

Tensor.mean_dim :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mean_dim(&outs[0], self.t, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.mean_dtype_out :: proc(self: *Tensor, out: Tensor, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mean_dtype_out(&outs[0], out.t, self.t, dtype)
    return wrap_and_track(outs[0])
}

Tensor.mean_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mean_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.median :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_median(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.median_dim :: proc(self: *Tensor, dim: i64, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_median_dim(&outs[0], self.t, dim, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.median_dim_values :: proc(self: *Tensor, values: Tensor, indices: Tensor, dim: i64, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_median_dim_values(&outs[0], values.t, indices.t, self.t, dim, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.median_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_median_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.meshgrid :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) [dyn] Tensor {
    ptr := torch_bindings.atg_meshgrid(tensors_data, tensors_len)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.meshgrid_indexing :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32, indexing_ptr: *u8, indexing_len: i32) [dyn] Tensor {
    ptr := torch_bindings.atg_meshgrid_indexing(tensors_data, tensors_len, indexing_ptr, indexing_len)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.mh :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mh(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.min :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_min(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.min_dim :: proc(self: *Tensor, dim: i64, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_min_dim(&outs[0], self.t, dim, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.min_dim_min :: proc(self: *Tensor, min: Tensor, min_indices: Tensor, dim: i64, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_min_dim_min(&outs[0], min.t, min_indices.t, self.t, dim, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.min_other :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_min_other(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.min_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_min_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.min_unary_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_min_unary_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.minimum :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_minimum(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.minimum_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_minimum_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.miopen_batch_norm :: proc(self: *Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, training: bool, exponential_average_factor: f64, epsilon: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_miopen_batch_norm(&outs[0], self.t, weight.t, bias.t, running_mean.t, running_var.t, training, exponential_average_factor, epsilon)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.miopen_batch_norm_backward :: proc(self: *Tensor, grad_output: Tensor, weight: Tensor, running_mean: Tensor, running_var: Tensor, save_mean: Tensor, save_var: Tensor, epsilon: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_miopen_batch_norm_backward(&outs[0], self.t, grad_output.t, weight.t, running_mean.t, running_var.t, save_mean.t, save_var.t, epsilon)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.miopen_batch_norm_backward_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, grad_output: Tensor, weight: Tensor, running_mean: Tensor, running_var: Tensor, save_mean: Tensor, save_var: Tensor, epsilon: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_miopen_batch_norm_backward_out(&outs[0], out0.t, out1.t, out2.t, self.t, grad_output.t, weight.t, running_mean.t, running_var.t, save_mean.t, save_var.t, epsilon)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.miopen_batch_norm_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, training: bool, exponential_average_factor: f64, epsilon: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_miopen_batch_norm_out(&outs[0], out0.t, out1.t, out2.t, self.t, weight.t, bias.t, running_mean.t, running_var.t, training, exponential_average_factor, epsilon)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.miopen_convolution :: proc(self: *Tensor, weight: Tensor, bias: Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64, benchmark: bool, deterministic: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_miopen_convolution(&outs[0], self.t, weight.t, bias.t, padding_data, padding_len, stride_data, stride_len, dilation_data, dilation_len, groups, benchmark, deterministic)
    return wrap_and_track(outs[0])
}

Tensor.miopen_convolution_add_relu :: proc(self: *Tensor, weight: Tensor, z: Tensor, alpha: torch_bindings.Scalar, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_miopen_convolution_add_relu(&outs[0], self.t, weight.t, z.t, alpha, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.miopen_convolution_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64, benchmark: bool, deterministic: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_miopen_convolution_out(&outs[0], out.t, self.t, weight.t, bias.t, padding_data, padding_len, stride_data, stride_len, dilation_data, dilation_len, groups, benchmark, deterministic)
    return wrap_and_track(outs[0])
}

Tensor.miopen_convolution_relu :: proc(self: *Tensor, weight: Tensor, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_miopen_convolution_relu(&outs[0], self.t, weight.t, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.miopen_convolution_transpose :: proc(self: *Tensor, weight: Tensor, bias: Tensor, padding_data: *i64, padding_len: i32, output_padding_data: *i64, output_padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64, benchmark: bool, deterministic: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_miopen_convolution_transpose(&outs[0], self.t, weight.t, bias.t, padding_data, padding_len, output_padding_data, output_padding_len, stride_data, stride_len, dilation_data, dilation_len, groups, benchmark, deterministic)
    return wrap_and_track(outs[0])
}

Tensor.miopen_convolution_transpose_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor, padding_data: *i64, padding_len: i32, output_padding_data: *i64, output_padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64, benchmark: bool, deterministic: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_miopen_convolution_transpose_out(&outs[0], out.t, self.t, weight.t, bias.t, padding_data, padding_len, output_padding_data, output_padding_len, stride_data, stride_len, dilation_data, dilation_len, groups, benchmark, deterministic)
    return wrap_and_track(outs[0])
}

Tensor.miopen_depthwise_convolution :: proc(self: *Tensor, weight: Tensor, bias: Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64, benchmark: bool, deterministic: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_miopen_depthwise_convolution(&outs[0], self.t, weight.t, bias.t, padding_data, padding_len, stride_data, stride_len, dilation_data, dilation_len, groups, benchmark, deterministic)
    return wrap_and_track(outs[0])
}

Tensor.miopen_depthwise_convolution_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64, benchmark: bool, deterministic: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_miopen_depthwise_convolution_out(&outs[0], out.t, self.t, weight.t, bias.t, padding_data, padding_len, stride_data, stride_len, dilation_data, dilation_len, groups, benchmark, deterministic)
    return wrap_and_track(outs[0])
}

Tensor.miopen_rnn :: proc(self: *Tensor, weight_data: *torch_bindings.Tensor, weight_len: i32, weight_stride0: i64, hx: Tensor, cx: Tensor, mode: i64, hidden_size: i64, num_layers: i64, batch_first: bool, dropout: f64, train: bool, bidirectional: bool, batch_sizes_data: *i64, batch_sizes_len: i32, dropout_state: Tensor) (Tensor, Tensor, Tensor, Tensor, Tensor) {
    outs: [5] torch_bindings.Tensor = undefined
    torch_bindings.atg_miopen_rnn(&outs[0], self.t, weight_data, weight_len, weight_stride0, hx.t, cx.t, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes_data, batch_sizes_len, dropout_state.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]), wrap_and_track(outs[4]))
}

Tensor.miopen_rnn_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, out3: Tensor, out4: Tensor, weight_data: *torch_bindings.Tensor, weight_len: i32, weight_stride0: i64, hx: Tensor, cx: Tensor, mode: i64, hidden_size: i64, num_layers: i64, batch_first: bool, dropout: f64, train: bool, bidirectional: bool, batch_sizes_data: *i64, batch_sizes_len: i32, dropout_state: Tensor) (Tensor, Tensor, Tensor, Tensor, Tensor) {
    outs: [5] torch_bindings.Tensor = undefined
    torch_bindings.atg_miopen_rnn_out(&outs[0], out0.t, out1.t, out2.t, out3.t, out4.t, self.t, weight_data, weight_len, weight_stride0, hx.t, cx.t, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes_data, batch_sizes_len, dropout_state.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]), wrap_and_track(outs[4]))
}

Tensor.mish :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mish(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.mish_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mish_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.mish_backward :: proc(self: *Tensor, grad_output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mish_backward(&outs[0], grad_output.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.mish_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mish_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_adaptive_avg_pool2d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_adaptive_avg_pool2d(&outs[0], self.t, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_adaptive_avg_pool2d_backward :: proc(self: *Tensor, grad_output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_adaptive_avg_pool2d_backward(&outs[0], grad_output.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_adaptive_avg_pool2d_backward_out :: proc(self: *Tensor, out: Tensor, grad_output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_adaptive_avg_pool2d_backward_out(&outs[0], out.t, grad_output.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_adaptive_avg_pool2d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_adaptive_avg_pool2d_out(&outs[0], out.t, self.t, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_convolution :: proc(self: *Tensor, weight: Tensor, bias: Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_convolution(&outs[0], self.t, weight.t, bias.t, padding_data, padding_len, stride_data, stride_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_convolution_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_convolution_out(&outs[0], out.t, self.t, weight.t, bias.t, padding_data, padding_len, stride_data, stride_len, dilation_data, dilation_len, groups)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_linear :: proc(self: *Tensor, weight: Tensor, bias: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_linear(&outs[0], self.t, weight.t, bias.t)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_linear_backward_input :: proc(input_size_data: *i64, input_size_len: i32, grad_output: Tensor, weight: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_linear_backward_input(&outs[0], input_size_data, input_size_len, grad_output.t, weight.t)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_linear_backward_input_out :: proc(out: Tensor, input_size_data: *i64, input_size_len: i32, grad_output: Tensor, weight: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_linear_backward_input_out(&outs[0], out.t, input_size_data, input_size_len, grad_output.t, weight.t)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_linear_backward_weights :: proc(self: *Tensor, grad_output: Tensor, weight: Tensor, bias_defined: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_linear_backward_weights(&outs[0], grad_output.t, self.t, weight.t, bias_defined)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.mkldnn_linear_backward_weights_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, grad_output: Tensor, weight: Tensor, bias_defined: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_linear_backward_weights_out(&outs[0], out0.t, out1.t, grad_output.t, self.t, weight.t, bias_defined)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.mkldnn_linear_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_linear_out(&outs[0], out.t, self.t, weight.t, bias.t)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_max_pool2d :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_max_pool2d(&outs[0], self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_max_pool2d_backward :: proc(self: *Tensor, grad_output: Tensor, output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_max_pool2d_backward(&outs[0], grad_output.t, output.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_max_pool2d_backward_out :: proc(self: *Tensor, out: Tensor, grad_output: Tensor, output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_max_pool2d_backward_out(&outs[0], out.t, grad_output.t, output.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_max_pool2d_out :: proc(self: *Tensor, out: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_max_pool2d_out(&outs[0], out.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_max_pool3d :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_max_pool3d(&outs[0], self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_max_pool3d_backward :: proc(self: *Tensor, grad_output: Tensor, output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_max_pool3d_backward(&outs[0], grad_output.t, output.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_max_pool3d_backward_out :: proc(self: *Tensor, out: Tensor, grad_output: Tensor, output: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_max_pool3d_backward_out(&outs[0], out.t, grad_output.t, output.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_max_pool3d_out :: proc(self: *Tensor, out: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_max_pool3d_out(&outs[0], out.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_reorder_conv2d_weight :: proc(self: *Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64, input_size_data: *i64, input_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_reorder_conv2d_weight(&outs[0], self.t, padding_data, padding_len, stride_data, stride_len, dilation_data, dilation_len, groups, input_size_data, input_size_len)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_reorder_conv2d_weight_out :: proc(self: *Tensor, out: Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64, input_size_data: *i64, input_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_reorder_conv2d_weight_out(&outs[0], out.t, self.t, padding_data, padding_len, stride_data, stride_len, dilation_data, dilation_len, groups, input_size_data, input_size_len)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_reorder_conv3d_weight :: proc(self: *Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64, input_size_data: *i64, input_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_reorder_conv3d_weight(&outs[0], self.t, padding_data, padding_len, stride_data, stride_len, dilation_data, dilation_len, groups, input_size_data, input_size_len)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_reorder_conv3d_weight_out :: proc(self: *Tensor, out: Tensor, padding_data: *i64, padding_len: i32, stride_data: *i64, stride_len: i32, dilation_data: *i64, dilation_len: i32, groups: i64, input_size_data: *i64, input_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_reorder_conv3d_weight_out(&outs[0], out.t, self.t, padding_data, padding_len, stride_data, stride_len, dilation_data, dilation_len, groups, input_size_data, input_size_len)
    return wrap_and_track(outs[0])
}

Tensor.mkldnn_rnn_layer :: proc(self: *Tensor, weight0: Tensor, weight1: Tensor, weight2: Tensor, weight3: Tensor, hx_: Tensor, cx_: Tensor, reverse: bool, batch_sizes_data: *i64, batch_sizes_len: i32, mode: i64, hidden_size: i64, num_layers: i64, has_biases: bool, bidirectional: bool, batch_first: bool, train: bool) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_rnn_layer(&outs[0], self.t, weight0.t, weight1.t, weight2.t, weight3.t, hx_.t, cx_.t, reverse, batch_sizes_data, batch_sizes_len, mode, hidden_size, num_layers, has_biases, bidirectional, batch_first, train)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor.mkldnn_rnn_layer_backward :: proc(self: *Tensor, weight1: Tensor, weight2: Tensor, weight3: Tensor, weight4: Tensor, hx_: Tensor, cx_tmp: Tensor, output: Tensor, hy_: Tensor, cy_: Tensor, grad_output: Tensor, grad_hy: Tensor, grad_cy: Tensor, reverse: bool, mode: i64, hidden_size: i64, num_layers: i64, has_biases: bool, train: bool, bidirectional: bool, batch_sizes_data: *i64, batch_sizes_len: i32, batch_first: bool, workspace: Tensor) (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor) {
    outs: [7] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_rnn_layer_backward(&outs[0], self.t, weight1.t, weight2.t, weight3.t, weight4.t, hx_.t, cx_tmp.t, output.t, hy_.t, cy_.t, grad_output.t, grad_hy.t, grad_cy.t, reverse, mode, hidden_size, num_layers, has_biases, train, bidirectional, batch_sizes_data, batch_sizes_len, batch_first, workspace.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]), wrap_and_track(outs[4]), wrap_and_track(outs[5]), wrap_and_track(outs[6]))
}

Tensor.mkldnn_rnn_layer_backward_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, out3: Tensor, out4: Tensor, out5: Tensor, out6: Tensor, weight1: Tensor, weight2: Tensor, weight3: Tensor, weight4: Tensor, hx_: Tensor, cx_tmp: Tensor, output: Tensor, hy_: Tensor, cy_: Tensor, grad_output: Tensor, grad_hy: Tensor, grad_cy: Tensor, reverse: bool, mode: i64, hidden_size: i64, num_layers: i64, has_biases: bool, train: bool, bidirectional: bool, batch_sizes_data: *i64, batch_sizes_len: i32, batch_first: bool, workspace: Tensor) (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor) {
    outs: [7] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_rnn_layer_backward_out(&outs[0], out0.t, out1.t, out2.t, out3.t, out4.t, out5.t, out6.t, self.t, weight1.t, weight2.t, weight3.t, weight4.t, hx_.t, cx_tmp.t, output.t, hy_.t, cy_.t, grad_output.t, grad_hy.t, grad_cy.t, reverse, mode, hidden_size, num_layers, has_biases, train, bidirectional, batch_sizes_data, batch_sizes_len, batch_first, workspace.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]), wrap_and_track(outs[4]), wrap_and_track(outs[5]), wrap_and_track(outs[6]))
}

Tensor.mkldnn_rnn_layer_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, out3: Tensor, weight0: Tensor, weight1: Tensor, weight2: Tensor, weight3: Tensor, hx_: Tensor, cx_: Tensor, reverse: bool, batch_sizes_data: *i64, batch_sizes_len: i32, mode: i64, hidden_size: i64, num_layers: i64, has_biases: bool, bidirectional: bool, batch_first: bool, train: bool) (Tensor, Tensor, Tensor, Tensor) {
    outs: [4] torch_bindings.Tensor = undefined
    torch_bindings.atg_mkldnn_rnn_layer_out(&outs[0], out0.t, out1.t, out2.t, out3.t, self.t, weight0.t, weight1.t, weight2.t, weight3.t, hx_.t, cx_.t, reverse, batch_sizes_data, batch_sizes_len, mode, hidden_size, num_layers, has_biases, bidirectional, batch_first, train)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]), wrap_and_track(outs[3]))
}

Tensor.mm :: proc(self: *Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mm(&outs[0], self.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor.mm_dtype :: proc(self: *Tensor, mat2: Tensor, out_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mm_dtype(&outs[0], self.t, mat2.t, out_dtype)
    return wrap_and_track(outs[0])
}

Tensor.mm_dtype_out :: proc(self: *Tensor, out: Tensor, mat2: Tensor, out_dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mm_dtype_out(&outs[0], out.t, self.t, mat2.t, out_dtype)
    return wrap_and_track(outs[0])
}

Tensor.mm_out :: proc(self: *Tensor, out: Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mm_out(&outs[0], out.t, self.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor.mode :: proc(self: *Tensor, dim: i64, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_mode(&outs[0], self.t, dim, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.mode_values :: proc(self: *Tensor, values: Tensor, indices: Tensor, dim: i64, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_mode_values(&outs[0], values.t, indices.t, self.t, dim, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.moveaxis :: proc(self: *Tensor, source_data: *i64, source_len: i32, destination_data: *i64, destination_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_moveaxis(&outs[0], self.t, source_data, source_len, destination_data, destination_len)
    return wrap_and_track(outs[0])
}

Tensor.moveaxis_int :: proc(self: *Tensor, source: i64, destination: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_moveaxis_int(&outs[0], self.t, source, destination)
    return wrap_and_track(outs[0])
}

Tensor.movedim :: proc(self: *Tensor, source_data: *i64, source_len: i32, destination_data: *i64, destination_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_movedim(&outs[0], self.t, source_data, source_len, destination_data, destination_len)
    return wrap_and_track(outs[0])
}

Tensor.movedim_int :: proc(self: *Tensor, source: i64, destination: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_movedim_int(&outs[0], self.t, source, destination)
    return wrap_and_track(outs[0])
}

Tensor.mse_loss :: proc(self: *Tensor, target: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mse_loss(&outs[0], self.t, target.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.mse_loss_backward :: proc(self: *Tensor, grad_output: Tensor, target: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mse_loss_backward(&outs[0], grad_output.t, self.t, target.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.mse_loss_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, target: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mse_loss_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, target.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.mse_loss_out :: proc(self: *Tensor, out: Tensor, target: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mse_loss_out(&outs[0], out.t, self.t, target.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.msort :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_msort(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.msort_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_msort_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.mt :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mt(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.mul :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mul(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.mul_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mul_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.mul_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mul_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.mul_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mul_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.mul_scalar_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mul_scalar_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.mul_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mul_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.multi_margin_loss_backward :: proc(self: *Tensor, grad_output: Tensor, target: Tensor, p: torch_bindings.Scalar, margin: torch_bindings.Scalar, weight: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_multi_margin_loss_backward(&outs[0], grad_output.t, self.t, target.t, p, margin, weight.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.multi_margin_loss_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, target: Tensor, p: torch_bindings.Scalar, margin: torch_bindings.Scalar, weight: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_multi_margin_loss_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, target.t, p, margin, weight.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.multilabel_margin_loss :: proc(self: *Tensor, target: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_multilabel_margin_loss(&outs[0], self.t, target.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.multilabel_margin_loss_backward :: proc(self: *Tensor, grad_output: Tensor, target: Tensor, reduction: i64, is_target: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_multilabel_margin_loss_backward(&outs[0], grad_output.t, self.t, target.t, reduction, is_target.t)
    return wrap_and_track(outs[0])
}

Tensor.multilabel_margin_loss_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, target: Tensor, reduction: i64, is_target: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_multilabel_margin_loss_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, target.t, reduction, is_target.t)
    return wrap_and_track(outs[0])
}

Tensor.multilabel_margin_loss_out :: proc(self: *Tensor, out: Tensor, target: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_multilabel_margin_loss_out(&outs[0], out.t, self.t, target.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.multinomial :: proc(self: *Tensor, num_samples: i64, replacement: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_multinomial(&outs[0], self.t, num_samples, replacement)
    return wrap_and_track(outs[0])
}

Tensor.multinomial_out :: proc(self: *Tensor, out: Tensor, num_samples: i64, replacement: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_multinomial_out(&outs[0], out.t, self.t, num_samples, replacement)
    return wrap_and_track(outs[0])
}

Tensor.multiply :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_multiply(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.multiply_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_multiply_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.multiply_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_multiply_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.multiply_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_multiply_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.multiply_scalar_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_multiply_scalar_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.mv :: proc(self: *Tensor, vec: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mv(&outs[0], self.t, vec.t)
    return wrap_and_track(outs[0])
}

Tensor.mv_out :: proc(self: *Tensor, out: Tensor, vec: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mv_out(&outs[0], out.t, self.t, vec.t)
    return wrap_and_track(outs[0])
}

Tensor.mvlgamma :: proc(self: *Tensor, p: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mvlgamma(&outs[0], self.t, p)
    return wrap_and_track(outs[0])
}

Tensor.mvlgamma_ :: proc(self: *Tensor, p: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mvlgamma_(&outs[0], self.t, p)
    return wrap_and_track(outs[0])
}

Tensor.mvlgamma_out :: proc(self: *Tensor, out: Tensor, p: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_mvlgamma_out(&outs[0], out.t, self.t, p)
    return wrap_and_track(outs[0])
}

Tensor.nan_to_num :: proc(self: *Tensor, nan_v: f64, nan_null: u8, posinf_v: f64, posinf_null: u8, neginf_v: f64, neginf_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nan_to_num(&outs[0], self.t, nan_v, nan_null, posinf_v, posinf_null, neginf_v, neginf_null)
    return wrap_and_track(outs[0])
}

Tensor.nan_to_num_ :: proc(self: *Tensor, nan_v: f64, nan_null: u8, posinf_v: f64, posinf_null: u8, neginf_v: f64, neginf_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nan_to_num_(&outs[0], self.t, nan_v, nan_null, posinf_v, posinf_null, neginf_v, neginf_null)
    return wrap_and_track(outs[0])
}

Tensor.nan_to_num_out :: proc(self: *Tensor, out: Tensor, nan_v: f64, nan_null: u8, posinf_v: f64, posinf_null: u8, neginf_v: f64, neginf_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nan_to_num_out(&outs[0], out.t, self.t, nan_v, nan_null, posinf_v, posinf_null, neginf_v, neginf_null)
    return wrap_and_track(outs[0])
}

Tensor.nanmean :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nanmean(&outs[0], self.t, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.nanmean_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nanmean_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.nanmedian :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nanmedian(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.nanmedian_dim :: proc(self: *Tensor, dim: i64, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_nanmedian_dim(&outs[0], self.t, dim, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.nanmedian_dim_values :: proc(self: *Tensor, values: Tensor, indices: Tensor, dim: i64, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_nanmedian_dim_values(&outs[0], values.t, indices.t, self.t, dim, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.nanmedian_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nanmedian_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.nanquantile :: proc(self: *Tensor, q: Tensor, dim_v: i64, dim_null: u8, keepdim: bool, interpolation_ptr: *u8, interpolation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nanquantile(&outs[0], self.t, q.t, dim_v, dim_null, keepdim, interpolation_ptr, interpolation_len)
    return wrap_and_track(outs[0])
}

Tensor.nanquantile_out :: proc(self: *Tensor, out: Tensor, q: Tensor, dim_v: i64, dim_null: u8, keepdim: bool, interpolation_ptr: *u8, interpolation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nanquantile_out(&outs[0], out.t, self.t, q.t, dim_v, dim_null, keepdim, interpolation_ptr, interpolation_len)
    return wrap_and_track(outs[0])
}

Tensor.nanquantile_scalar :: proc(self: *Tensor, q: f64, dim_v: i64, dim_null: u8, keepdim: bool, interpolation_ptr: *u8, interpolation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nanquantile_scalar(&outs[0], self.t, q, dim_v, dim_null, keepdim, interpolation_ptr, interpolation_len)
    return wrap_and_track(outs[0])
}

Tensor.nanquantile_scalar_out :: proc(self: *Tensor, out: Tensor, q: f64, dim_v: i64, dim_null: u8, keepdim: bool, interpolation_ptr: *u8, interpolation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nanquantile_scalar_out(&outs[0], out.t, self.t, q, dim_v, dim_null, keepdim, interpolation_ptr, interpolation_len)
    return wrap_and_track(outs[0])
}

Tensor.nansum :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nansum(&outs[0], self.t, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.nansum_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nansum_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.narrow :: proc(self: *Tensor, dim: i64, start: i64, length: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_narrow(&outs[0], self.t, dim, start, length)
    return wrap_and_track(outs[0])
}

Tensor.narrow_copy :: proc(self: *Tensor, dim: i64, start: i64, length: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_narrow_copy(&outs[0], self.t, dim, start, length)
    return wrap_and_track(outs[0])
}

Tensor.narrow_copy_out :: proc(self: *Tensor, out: Tensor, dim: i64, start: i64, length: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_narrow_copy_out(&outs[0], out.t, self.t, dim, start, length)
    return wrap_and_track(outs[0])
}

Tensor.narrow_tensor :: proc(self: *Tensor, dim: i64, start: Tensor, length: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_narrow_tensor(&outs[0], self.t, dim, start.t, length)
    return wrap_and_track(outs[0])
}

Tensor.native_batch_norm :: proc(self: *Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, training: bool, momentum: f64, eps: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_batch_norm(&outs[0], self.t, weight.t, bias.t, running_mean.t, running_var.t, training, momentum, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.native_batch_norm_out :: proc(self: *Tensor, out: Tensor, save_mean: Tensor, save_invstd: Tensor, weight: Tensor, bias: Tensor, running_mean: Tensor, running_var: Tensor, training: bool, momentum: f64, eps: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_batch_norm_out(&outs[0], out.t, save_mean.t, save_invstd.t, self.t, weight.t, bias.t, running_mean.t, running_var.t, training, momentum, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.native_channel_shuffle :: proc(self: *Tensor, groups: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_channel_shuffle(&outs[0], self.t, groups)
    return wrap_and_track(outs[0])
}

Tensor.native_dropout :: proc(self: *Tensor, p: f64, train: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_dropout(&outs[0], self.t, p, train)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.native_dropout_backward :: proc(grad_output: Tensor, mask: Tensor, scale: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_dropout_backward(&outs[0], grad_output.t, mask.t, scale)
    return wrap_and_track(outs[0])
}

Tensor.native_dropout_backward_out :: proc(out: Tensor, grad_output: Tensor, mask: Tensor, scale: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_dropout_backward_out(&outs[0], out.t, grad_output.t, mask.t, scale)
    return wrap_and_track(outs[0])
}

Tensor.native_dropout_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, p: f64, train: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_dropout_out(&outs[0], out0.t, out1.t, self.t, p, train)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.native_group_norm :: proc(self: *Tensor, weight: Tensor, bias: Tensor, n: i64, C: i64, HxW: i64, group: i64, eps: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_group_norm(&outs[0], self.t, weight.t, bias.t, n, C, HxW, group, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.native_group_norm_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, weight: Tensor, bias: Tensor, n: i64, C: i64, HxW: i64, group: i64, eps: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_group_norm_out(&outs[0], out0.t, out1.t, out2.t, self.t, weight.t, bias.t, n, C, HxW, group, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.native_layer_norm :: proc(self: *Tensor, normalized_shape_data: *i64, normalized_shape_len: i32, weight: Tensor, bias: Tensor, eps: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_layer_norm(&outs[0], self.t, normalized_shape_data, normalized_shape_len, weight.t, bias.t, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.native_layer_norm_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, normalized_shape_data: *i64, normalized_shape_len: i32, weight: Tensor, bias: Tensor, eps: f64) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_layer_norm_out(&outs[0], out0.t, out1.t, out2.t, self.t, normalized_shape_data, normalized_shape_len, weight.t, bias.t, eps)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.native_norm :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_norm(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.native_norm_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_norm_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.native_norm_scalaropt_dim_dtype :: proc(self: *Tensor, p: torch_bindings.Scalar, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_norm_scalaropt_dim_dtype(&outs[0], self.t, p, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.native_norm_scalaropt_dim_dtype_out :: proc(self: *Tensor, out: Tensor, p: torch_bindings.Scalar, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_native_norm_scalaropt_dim_dtype_out(&outs[0], out.t, self.t, p, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.ne :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ne(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.ne_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ne_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.ne_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ne_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.ne_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ne_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.ne_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ne_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.ne_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ne_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.neg :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_neg(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.neg_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_neg_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.neg_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_neg_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.negative :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_negative(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.negative_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_negative_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.negative_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_negative_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.nested_to_padded_tensor :: proc(self: *Tensor, padding: f64, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nested_to_padded_tensor(&outs[0], self.t, padding, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor.new_empty :: proc(self: *Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_new_empty(&outs[0], self.t, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.new_empty_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_new_empty_out(&outs[0], out.t, self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.new_empty_strided :: proc(self: *Tensor, size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_new_empty_strided(&outs[0], self.t, size_data, size_len, stride_data, stride_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.new_empty_strided_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_new_empty_strided_out(&outs[0], out.t, self.t, size_data, size_len, stride_data, stride_len)
    return wrap_and_track(outs[0])
}

Tensor.new_full :: proc(self: *Tensor, size_data: *i64, size_len: i32, fill_value: torch_bindings.Scalar, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_new_full(&outs[0], self.t, size_data, size_len, fill_value, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.new_full_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32, fill_value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_new_full_out(&outs[0], out.t, self.t, size_data, size_len, fill_value)
    return wrap_and_track(outs[0])
}

Tensor.new_ones :: proc(self: *Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_new_ones(&outs[0], self.t, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.new_ones_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_new_ones_out(&outs[0], out.t, self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.new_zeros :: proc(self: *Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_new_zeros(&outs[0], self.t, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.new_zeros_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_new_zeros_out(&outs[0], out.t, self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.nextafter :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nextafter(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.nextafter_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nextafter_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.nextafter_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nextafter_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.nll_loss :: proc(self: *Tensor, target: Tensor, weight: Tensor, reduction: i64, ignore_index: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nll_loss(&outs[0], self.t, target.t, weight.t, reduction, ignore_index)
    return wrap_and_track(outs[0])
}

Tensor.nll_loss2d :: proc(self: *Tensor, target: Tensor, weight: Tensor, reduction: i64, ignore_index: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nll_loss2d(&outs[0], self.t, target.t, weight.t, reduction, ignore_index)
    return wrap_and_track(outs[0])
}

Tensor.nll_loss2d_backward :: proc(self: *Tensor, grad_output: Tensor, target: Tensor, weight: Tensor, reduction: i64, ignore_index: i64, total_weight: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nll_loss2d_backward(&outs[0], grad_output.t, self.t, target.t, weight.t, reduction, ignore_index, total_weight.t)
    return wrap_and_track(outs[0])
}

Tensor.nll_loss2d_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, target: Tensor, weight: Tensor, reduction: i64, ignore_index: i64, total_weight: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nll_loss2d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, target.t, weight.t, reduction, ignore_index, total_weight.t)
    return wrap_and_track(outs[0])
}

Tensor.nll_loss2d_out :: proc(self: *Tensor, out: Tensor, target: Tensor, weight: Tensor, reduction: i64, ignore_index: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nll_loss2d_out(&outs[0], out.t, self.t, target.t, weight.t, reduction, ignore_index)
    return wrap_and_track(outs[0])
}

Tensor.nll_loss_backward :: proc(self: *Tensor, grad_output: Tensor, target: Tensor, weight: Tensor, reduction: i64, ignore_index: i64, total_weight: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nll_loss_backward(&outs[0], grad_output.t, self.t, target.t, weight.t, reduction, ignore_index, total_weight.t)
    return wrap_and_track(outs[0])
}

Tensor.nll_loss_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, target: Tensor, weight: Tensor, reduction: i64, ignore_index: i64, total_weight: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nll_loss_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, target.t, weight.t, reduction, ignore_index, total_weight.t)
    return wrap_and_track(outs[0])
}

Tensor.nll_loss_nd :: proc(self: *Tensor, target: Tensor, weight: Tensor, reduction: i64, ignore_index: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nll_loss_nd(&outs[0], self.t, target.t, weight.t, reduction, ignore_index)
    return wrap_and_track(outs[0])
}

Tensor.nll_loss_out :: proc(self: *Tensor, out: Tensor, target: Tensor, weight: Tensor, reduction: i64, ignore_index: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nll_loss_out(&outs[0], out.t, self.t, target.t, weight.t, reduction, ignore_index)
    return wrap_and_track(outs[0])
}

Tensor.nonzero :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nonzero(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.nonzero_numpy :: proc(self: *Tensor) [dyn] Tensor {
    ptr := torch_bindings.atg_nonzero_numpy(self.t)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.nonzero_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nonzero_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.nonzero_static :: proc(self: *Tensor, size: i64, fill_value: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nonzero_static(&outs[0], self.t, size, fill_value)
    return wrap_and_track(outs[0])
}

Tensor.nonzero_static_out :: proc(self: *Tensor, out: Tensor, size: i64, fill_value: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nonzero_static_out(&outs[0], out.t, self.t, size, fill_value)
    return wrap_and_track(outs[0])
}

Tensor.norm :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_norm(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.norm_dtype_out :: proc(self: *Tensor, out: Tensor, p: torch_bindings.Scalar, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_norm_dtype_out(&outs[0], out.t, self.t, p, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.norm_except_dim :: proc(v: Tensor, pow: i64, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_norm_except_dim(&outs[0], v.t, pow, dim)
    return wrap_and_track(outs[0])
}

Tensor.norm_out :: proc(self: *Tensor, out: Tensor, p: torch_bindings.Scalar, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_norm_out(&outs[0], out.t, self.t, p, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.norm_scalar_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_norm_scalar_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.norm_scalaropt_dim :: proc(self: *Tensor, p: torch_bindings.Scalar, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_norm_scalaropt_dim(&outs[0], self.t, p, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.norm_scalaropt_dim_dtype :: proc(self: *Tensor, p: torch_bindings.Scalar, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_norm_scalaropt_dim_dtype(&outs[0], self.t, p, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.norm_scalaropt_dtype :: proc(self: *Tensor, p: torch_bindings.Scalar, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_norm_scalaropt_dtype(&outs[0], self.t, p, dtype)
    return wrap_and_track(outs[0])
}

Tensor.norm_scalaropt_dtype_out :: proc(self: *Tensor, out: Tensor, p: torch_bindings.Scalar, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_norm_scalaropt_dtype_out(&outs[0], out.t, self.t, p, dtype)
    return wrap_and_track(outs[0])
}

Tensor.normal_ :: proc(self: *Tensor, mean: f64, std: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_normal_(&outs[0], self.t, mean, std)
    return wrap_and_track(outs[0])
}

Tensor.normal_functional :: proc(self: *Tensor, mean: f64, std: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_normal_functional(&outs[0], self.t, mean, std)
    return wrap_and_track(outs[0])
}

Tensor.not_equal :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_not_equal(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.not_equal_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_not_equal_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.not_equal_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_not_equal_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.not_equal_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_not_equal_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.not_equal_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_not_equal_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.not_equal_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_not_equal_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.nuclear_norm :: proc(self: *Tensor, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nuclear_norm(&outs[0], self.t, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.nuclear_norm_dim :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nuclear_norm_dim(&outs[0], self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.nuclear_norm_dim_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nuclear_norm_dim_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.nuclear_norm_out :: proc(self: *Tensor, out: Tensor, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_nuclear_norm_out(&outs[0], out.t, self.t, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.numpy_t :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_numpy_t(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.one_hot :: proc(self: *Tensor, num_classes: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_one_hot(&outs[0], self.t, num_classes)
    return wrap_and_track(outs[0])
}

Tensor.ones :: proc(size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ones(&outs[0], size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.ones_like :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ones_like(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.ones_like_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ones_like_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.ones_out :: proc(out: Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ones_out(&outs[0], out.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.orgqr :: proc(self: *Tensor, input2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_orgqr(&outs[0], self.t, input2.t)
    return wrap_and_track(outs[0])
}

Tensor.orgqr_out :: proc(self: *Tensor, out: Tensor, input2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_orgqr_out(&outs[0], out.t, self.t, input2.t)
    return wrap_and_track(outs[0])
}

Tensor.ormqr :: proc(self: *Tensor, input2: Tensor, input3: Tensor, left: bool, transpose: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ormqr(&outs[0], self.t, input2.t, input3.t, left, transpose)
    return wrap_and_track(outs[0])
}

Tensor.ormqr_out :: proc(self: *Tensor, out: Tensor, input2: Tensor, input3: Tensor, left: bool, transpose: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ormqr_out(&outs[0], out.t, self.t, input2.t, input3.t, left, transpose)
    return wrap_and_track(outs[0])
}

Tensor.outer :: proc(self: *Tensor, vec2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_outer(&outs[0], self.t, vec2.t)
    return wrap_and_track(outs[0])
}

Tensor.outer_out :: proc(self: *Tensor, out: Tensor, vec2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_outer_out(&outs[0], out.t, self.t, vec2.t)
    return wrap_and_track(outs[0])
}

Tensor.output_nr :: proc(self: *Tensor) i64 {
    return torch_bindings.atg_output_nr(self.t)
}

Tensor.pad :: proc(self: *Tensor, pad_data: *i64, pad_len: i32, mode_ptr: *u8, mode_len: i32, value_v: f64, value_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pad(&outs[0], self.t, pad_data, pad_len, mode_ptr, mode_len, value_v, value_null)
    return wrap_and_track(outs[0])
}

Tensor.pad_sequence :: proc(sequences_data: *torch_bindings.Tensor, sequences_len: i32, batch_first: bool, padding_value: f64, padding_side_ptr: *u8, padding_side_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pad_sequence(&outs[0], sequences_data, sequences_len, batch_first, padding_value, padding_side_ptr, padding_side_len)
    return wrap_and_track(outs[0])
}

Tensor.pairwise_distance :: proc(x1: Tensor, x2: Tensor, p: f64, eps: f64, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pairwise_distance(&outs[0], x1.t, x2.t, p, eps, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.pdist :: proc(self: *Tensor, p: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pdist(&outs[0], self.t, p)
    return wrap_and_track(outs[0])
}

Tensor.permute :: proc(self: *Tensor, dims_data: *i64, dims_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_permute(&outs[0], self.t, dims_data, dims_len)
    return wrap_and_track(outs[0])
}

Tensor.permute_copy :: proc(self: *Tensor, dims_data: *i64, dims_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_permute_copy(&outs[0], self.t, dims_data, dims_len)
    return wrap_and_track(outs[0])
}

Tensor.permute_copy_out :: proc(self: *Tensor, out: Tensor, dims_data: *i64, dims_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_permute_copy_out(&outs[0], out.t, self.t, dims_data, dims_len)
    return wrap_and_track(outs[0])
}

Tensor.pin_memory :: proc(self: *Tensor, device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pin_memory(&outs[0], self.t, device)
    return wrap_and_track(outs[0])
}

Tensor.pinverse :: proc(self: *Tensor, rcond: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pinverse(&outs[0], self.t, rcond)
    return wrap_and_track(outs[0])
}

Tensor.pixel_shuffle :: proc(self: *Tensor, upscale_factor: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pixel_shuffle(&outs[0], self.t, upscale_factor)
    return wrap_and_track(outs[0])
}

Tensor.pixel_shuffle_out :: proc(self: *Tensor, out: Tensor, upscale_factor: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pixel_shuffle_out(&outs[0], out.t, self.t, upscale_factor)
    return wrap_and_track(outs[0])
}

Tensor.pixel_unshuffle :: proc(self: *Tensor, downscale_factor: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pixel_unshuffle(&outs[0], self.t, downscale_factor)
    return wrap_and_track(outs[0])
}

Tensor.pixel_unshuffle_out :: proc(self: *Tensor, out: Tensor, downscale_factor: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pixel_unshuffle_out(&outs[0], out.t, self.t, downscale_factor)
    return wrap_and_track(outs[0])
}

Tensor.poisson :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_poisson(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.poisson_nll_loss :: proc(self: *Tensor, target: Tensor, log_input: bool, full: bool, eps: f64, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_poisson_nll_loss(&outs[0], self.t, target.t, log_input, full, eps, reduction)
    return wrap_and_track(outs[0])
}

Tensor.poisson_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_poisson_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.polar :: proc(abs: Tensor, angle: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_polar(&outs[0], abs.t, angle.t)
    return wrap_and_track(outs[0])
}

Tensor.polar_out :: proc(out: Tensor, abs: Tensor, angle: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_polar_out(&outs[0], out.t, abs.t, angle.t)
    return wrap_and_track(outs[0])
}

Tensor.polygamma :: proc(self: *Tensor, n: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_polygamma(&outs[0], n, self.t)
    return wrap_and_track(outs[0])
}

Tensor.polygamma_ :: proc(self: *Tensor, n: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_polygamma_(&outs[0], self.t, n)
    return wrap_and_track(outs[0])
}

Tensor.polygamma_out :: proc(self: *Tensor, out: Tensor, n: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_polygamma_out(&outs[0], out.t, n, self.t)
    return wrap_and_track(outs[0])
}

Tensor.positive :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_positive(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.pow :: proc(self: *Tensor, exponent: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pow(&outs[0], self.t, exponent.t)
    return wrap_and_track(outs[0])
}

Tensor.pow_ :: proc(self: *Tensor, exponent: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pow_(&outs[0], self.t, exponent)
    return wrap_and_track(outs[0])
}

Tensor.pow_scalar :: proc(self_scalar: torch_bindings.Scalar, exponent: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pow_scalar(&outs[0], self_scalar, exponent.t)
    return wrap_and_track(outs[0])
}

Tensor.pow_scalar_out :: proc(out: Tensor, self_scalar: torch_bindings.Scalar, exponent: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pow_scalar_out(&outs[0], out.t, self_scalar, exponent.t)
    return wrap_and_track(outs[0])
}

Tensor.pow_tensor_ :: proc(self: *Tensor, exponent: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pow_tensor_(&outs[0], self.t, exponent.t)
    return wrap_and_track(outs[0])
}

Tensor.pow_tensor_scalar :: proc(self: *Tensor, exponent: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pow_tensor_scalar(&outs[0], self.t, exponent)
    return wrap_and_track(outs[0])
}

Tensor.pow_tensor_scalar_out :: proc(self: *Tensor, out: Tensor, exponent: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pow_tensor_scalar_out(&outs[0], out.t, self.t, exponent)
    return wrap_and_track(outs[0])
}

Tensor.pow_tensor_tensor_out :: proc(self: *Tensor, out: Tensor, exponent: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_pow_tensor_tensor_out(&outs[0], out.t, self.t, exponent.t)
    return wrap_and_track(outs[0])
}

Tensor.prelu :: proc(self: *Tensor, weight: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_prelu(&outs[0], self.t, weight.t)
    return wrap_and_track(outs[0])
}

Tensor.prod :: proc(self: *Tensor, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_prod(&outs[0], self.t, dtype)
    return wrap_and_track(outs[0])
}

Tensor.prod_dim_int :: proc(self: *Tensor, dim: i64, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_prod_dim_int(&outs[0], self.t, dim, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.prod_int_out :: proc(self: *Tensor, out: Tensor, dim: i64, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_prod_int_out(&outs[0], out.t, self.t, dim, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.prod_out :: proc(self: *Tensor, out: Tensor, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_prod_out(&outs[0], out.t, self.t, dtype)
    return wrap_and_track(outs[0])
}

Tensor.put :: proc(self: *Tensor, index: Tensor, source: Tensor, accumulate: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_put(&outs[0], self.t, index.t, source.t, accumulate)
    return wrap_and_track(outs[0])
}

Tensor.put_ :: proc(self: *Tensor, index: Tensor, source: Tensor, accumulate: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_put_(&outs[0], self.t, index.t, source.t, accumulate)
    return wrap_and_track(outs[0])
}

Tensor.put_out :: proc(self: *Tensor, out: Tensor, index: Tensor, source: Tensor, accumulate: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_put_out(&outs[0], out.t, self.t, index.t, source.t, accumulate)
    return wrap_and_track(outs[0])
}

Tensor.q_per_channel_axis :: proc(self: *Tensor) i64 {
    return torch_bindings.atg_q_per_channel_axis(self.t)
}

Tensor.q_per_channel_scales :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_q_per_channel_scales(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.q_per_channel_scales_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_q_per_channel_scales_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.q_per_channel_zero_points :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_q_per_channel_zero_points(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.q_per_channel_zero_points_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_q_per_channel_zero_points_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.q_scale :: proc(self: *Tensor) f64 {
    return torch_bindings.atg_q_scale(self.t)
}

Tensor.q_zero_point :: proc(self: *Tensor) i64 {
    return torch_bindings.atg_q_zero_point(self.t)
}

Tensor.qr :: proc(self: *Tensor, some: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_qr(&outs[0], self.t, some)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.qr_q :: proc(self: *Tensor, Q: Tensor, R: Tensor, some: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_qr_q(&outs[0], Q.t, R.t, self.t, some)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.quantile :: proc(self: *Tensor, q: Tensor, dim_v: i64, dim_null: u8, keepdim: bool, interpolation_ptr: *u8, interpolation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantile(&outs[0], self.t, q.t, dim_v, dim_null, keepdim, interpolation_ptr, interpolation_len)
    return wrap_and_track(outs[0])
}

Tensor.quantile_out :: proc(self: *Tensor, out: Tensor, q: Tensor, dim_v: i64, dim_null: u8, keepdim: bool, interpolation_ptr: *u8, interpolation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantile_out(&outs[0], out.t, self.t, q.t, dim_v, dim_null, keepdim, interpolation_ptr, interpolation_len)
    return wrap_and_track(outs[0])
}

Tensor.quantile_scalar :: proc(self: *Tensor, q: f64, dim_v: i64, dim_null: u8, keepdim: bool, interpolation_ptr: *u8, interpolation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantile_scalar(&outs[0], self.t, q, dim_v, dim_null, keepdim, interpolation_ptr, interpolation_len)
    return wrap_and_track(outs[0])
}

Tensor.quantile_scalar_out :: proc(self: *Tensor, out: Tensor, q: f64, dim_v: i64, dim_null: u8, keepdim: bool, interpolation_ptr: *u8, interpolation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantile_scalar_out(&outs[0], out.t, self.t, q, dim_v, dim_null, keepdim, interpolation_ptr, interpolation_len)
    return wrap_and_track(outs[0])
}

Tensor.quantize_per_channel :: proc(self: *Tensor, scales: Tensor, zero_points: Tensor, axis: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantize_per_channel(&outs[0], self.t, scales.t, zero_points.t, axis, dtype)
    return wrap_and_track(outs[0])
}

Tensor.quantize_per_channel_out :: proc(self: *Tensor, out: Tensor, scales: Tensor, zero_points: Tensor, axis: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantize_per_channel_out(&outs[0], out.t, self.t, scales.t, zero_points.t, axis, dtype)
    return wrap_and_track(outs[0])
}

Tensor.quantize_per_tensor :: proc(self: *Tensor, scale: f64, zero_point: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantize_per_tensor(&outs[0], self.t, scale, zero_point, dtype)
    return wrap_and_track(outs[0])
}

Tensor.quantize_per_tensor_dynamic :: proc(self: *Tensor, dtype: i32, reduce_range: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantize_per_tensor_dynamic(&outs[0], self.t, dtype, reduce_range)
    return wrap_and_track(outs[0])
}

Tensor.quantize_per_tensor_dynamic_out :: proc(self: *Tensor, out: Tensor, dtype: i32, reduce_range: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantize_per_tensor_dynamic_out(&outs[0], out.t, self.t, dtype, reduce_range)
    return wrap_and_track(outs[0])
}

Tensor.quantize_per_tensor_out :: proc(self: *Tensor, out: Tensor, scale: f64, zero_point: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantize_per_tensor_out(&outs[0], out.t, self.t, scale, zero_point, dtype)
    return wrap_and_track(outs[0])
}

Tensor.quantize_per_tensor_tensor_qparams :: proc(self: *Tensor, scale: Tensor, zero_point: Tensor, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantize_per_tensor_tensor_qparams(&outs[0], self.t, scale.t, zero_point.t, dtype)
    return wrap_and_track(outs[0])
}

Tensor.quantize_per_tensor_tensor_qparams_out :: proc(self: *Tensor, out: Tensor, scale: Tensor, zero_point: Tensor, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantize_per_tensor_tensor_qparams_out(&outs[0], out.t, self.t, scale.t, zero_point.t, dtype)
    return wrap_and_track(outs[0])
}

Tensor.quantize_per_tensor_tensors :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32, scales: Tensor, zero_points: Tensor, dtype: i32) [dyn] Tensor {
    ptr := torch_bindings.atg_quantize_per_tensor_tensors(tensors_data, tensors_len, scales.t, zero_points.t, dtype)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.quantize_per_tensor_tensors_out :: proc(out_data: *torch_bindings.Tensor, out_len: i32, tensors_data: *torch_bindings.Tensor, tensors_len: i32, scales: Tensor, zero_points: Tensor, dtype: i32) void {
    torch_bindings.atg_quantize_per_tensor_tensors_out(out_data, out_len, tensors_data, tensors_len, scales.t, zero_points.t, dtype)
}

Tensor.quantized_batch_norm :: proc(self: *Tensor, weight: Tensor, bias: Tensor, mean: Tensor, var: Tensor, eps: f64, output_scale: f64, output_zero_point: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantized_batch_norm(&outs[0], self.t, weight.t, bias.t, mean.t, var.t, eps, output_scale, output_zero_point)
    return wrap_and_track(outs[0])
}

Tensor.quantized_batch_norm_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, bias: Tensor, mean: Tensor, var: Tensor, eps: f64, output_scale: f64, output_zero_point: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantized_batch_norm_out(&outs[0], out.t, self.t, weight.t, bias.t, mean.t, var.t, eps, output_scale, output_zero_point)
    return wrap_and_track(outs[0])
}

Tensor.quantized_gru_cell :: proc(self: *Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: torch_bindings.Scalar, scale_hh: torch_bindings.Scalar, zero_point_ih: torch_bindings.Scalar, zero_point_hh: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantized_gru_cell(&outs[0], self.t, hx.t, w_ih.t, w_hh.t, b_ih.t, b_hh.t, packed_ih.t, packed_hh.t, col_offsets_ih.t, col_offsets_hh.t, scale_ih, scale_hh, zero_point_ih, zero_point_hh)
    return wrap_and_track(outs[0])
}

Tensor.quantized_lstm_cell :: proc(self: *Tensor, hx_data: *torch_bindings.Tensor, hx_len: i32, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: torch_bindings.Scalar, scale_hh: torch_bindings.Scalar, zero_point_ih: torch_bindings.Scalar, zero_point_hh: torch_bindings.Scalar) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantized_lstm_cell(&outs[0], self.t, hx_data, hx_len, w_ih.t, w_hh.t, b_ih.t, b_hh.t, packed_ih.t, packed_hh.t, col_offsets_ih.t, col_offsets_hh.t, scale_ih, scale_hh, zero_point_ih, zero_point_hh)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.quantized_max_pool1d :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantized_max_pool1d(&outs[0], self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.quantized_max_pool1d_out :: proc(self: *Tensor, out: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantized_max_pool1d_out(&outs[0], out.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.quantized_max_pool2d :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantized_max_pool2d(&outs[0], self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.quantized_max_pool2d_out :: proc(self: *Tensor, out: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantized_max_pool2d_out(&outs[0], out.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.quantized_max_pool3d :: proc(self: *Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantized_max_pool3d(&outs[0], self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.quantized_max_pool3d_out :: proc(self: *Tensor, out: Tensor, kernel_size_data: *i64, kernel_size_len: i32, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32, ceil_mode: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantized_max_pool3d_out(&outs[0], out.t, self.t, kernel_size_data, kernel_size_len, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len, ceil_mode)
    return wrap_and_track(outs[0])
}

Tensor.quantized_rnn_relu_cell :: proc(self: *Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: torch_bindings.Scalar, scale_hh: torch_bindings.Scalar, zero_point_ih: torch_bindings.Scalar, zero_point_hh: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantized_rnn_relu_cell(&outs[0], self.t, hx.t, w_ih.t, w_hh.t, b_ih.t, b_hh.t, packed_ih.t, packed_hh.t, col_offsets_ih.t, col_offsets_hh.t, scale_ih, scale_hh, zero_point_ih, zero_point_hh)
    return wrap_and_track(outs[0])
}

Tensor.quantized_rnn_tanh_cell :: proc(self: *Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: torch_bindings.Scalar, scale_hh: torch_bindings.Scalar, zero_point_ih: torch_bindings.Scalar, zero_point_hh: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_quantized_rnn_tanh_cell(&outs[0], self.t, hx.t, w_ih.t, w_hh.t, b_ih.t, b_hh.t, packed_ih.t, packed_hh.t, col_offsets_ih.t, col_offsets_hh.t, scale_ih, scale_hh, zero_point_ih, zero_point_hh)
    return wrap_and_track(outs[0])
}

Tensor.rad2deg :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rad2deg(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.rad2deg_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rad2deg_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.rad2deg_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rad2deg_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.rand :: proc(size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rand(&outs[0], size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.rand_like :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rand_like(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.rand_like_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rand_like_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.rand_out :: proc(out: Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rand_out(&outs[0], out.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.randint :: proc(high: i64, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randint(&outs[0], high, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.randint_like :: proc(self: *Tensor, high: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randint_like(&outs[0], self.t, high)
    return wrap_and_track(outs[0])
}

Tensor.randint_like_low_dtype :: proc(self: *Tensor, low: i64, high: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randint_like_low_dtype(&outs[0], self.t, low, high)
    return wrap_and_track(outs[0])
}

Tensor.randint_like_low_dtype_out :: proc(self: *Tensor, out: Tensor, low: i64, high: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randint_like_low_dtype_out(&outs[0], out.t, self.t, low, high)
    return wrap_and_track(outs[0])
}

Tensor.randint_like_out :: proc(self: *Tensor, out: Tensor, high: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randint_like_out(&outs[0], out.t, self.t, high)
    return wrap_and_track(outs[0])
}

Tensor.randint_like_tensor :: proc(self: *Tensor, high: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randint_like_tensor(&outs[0], self.t, high.t)
    return wrap_and_track(outs[0])
}

Tensor.randint_like_tensor_out :: proc(self: *Tensor, out: Tensor, high: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randint_like_tensor_out(&outs[0], out.t, self.t, high.t)
    return wrap_and_track(outs[0])
}

Tensor.randint_low :: proc(low: i64, high: i64, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randint_low(&outs[0], low, high, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.randint_low_out :: proc(out: Tensor, low: i64, high: i64, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randint_low_out(&outs[0], out.t, low, high, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.randint_out :: proc(out: Tensor, high: i64, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randint_out(&outs[0], out.t, high, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.randn :: proc(size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randn(&outs[0], size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.randn_like :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randn_like(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.randn_like_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randn_like_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.randn_out :: proc(out: Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randn_out(&outs[0], out.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.random :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_random(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.random_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_random_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.random_from :: proc(self: *Tensor, from: i64, to_v: i64, to_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_random_from(&outs[0], self.t, from, to_v, to_null)
    return wrap_and_track(outs[0])
}

Tensor.random_from_ :: proc(self: *Tensor, from: i64, to_v: i64, to_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_random_from_(&outs[0], self.t, from, to_v, to_null)
    return wrap_and_track(outs[0])
}

Tensor.random_from_out :: proc(self: *Tensor, out: Tensor, from: i64, to_v: i64, to_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_random_from_out(&outs[0], out.t, self.t, from, to_v, to_null)
    return wrap_and_track(outs[0])
}

Tensor.random_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_random_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.random_to :: proc(self: *Tensor, to: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_random_to(&outs[0], self.t, to)
    return wrap_and_track(outs[0])
}

Tensor.random_to_ :: proc(self: *Tensor, to: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_random_to_(&outs[0], self.t, to)
    return wrap_and_track(outs[0])
}

Tensor.random_to_out :: proc(self: *Tensor, out: Tensor, to: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_random_to_out(&outs[0], out.t, self.t, to)
    return wrap_and_track(outs[0])
}

Tensor.randperm :: proc(n: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randperm(&outs[0], n, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.randperm_out :: proc(out: Tensor, n: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_randperm_out(&outs[0], out.t, n)
    return wrap_and_track(outs[0])
}

Tensor.range :: proc(start: torch_bindings.Scalar, end: torch_bindings.Scalar, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_range(&outs[0], start, end, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.range_out :: proc(out: Tensor, start: torch_bindings.Scalar, end: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_range_out(&outs[0], out.t, start, end)
    return wrap_and_track(outs[0])
}

Tensor.range_out_ :: proc(out: Tensor, start: torch_bindings.Scalar, end: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_range_out_(&outs[0], out.t, start, end)
    return wrap_and_track(outs[0])
}

Tensor.range_step :: proc(start: torch_bindings.Scalar, end: torch_bindings.Scalar, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_range_step(&outs[0], start, end, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.ravel :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_ravel(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.real :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_real(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.reciprocal :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reciprocal(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.reciprocal_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reciprocal_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.reciprocal_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reciprocal_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.reflection_pad1d :: proc(self: *Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reflection_pad1d(&outs[0], self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.reflection_pad1d_backward :: proc(self: *Tensor, grad_output: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reflection_pad1d_backward(&outs[0], grad_output.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.reflection_pad1d_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reflection_pad1d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.reflection_pad1d_out :: proc(self: *Tensor, out: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reflection_pad1d_out(&outs[0], out.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.reflection_pad2d :: proc(self: *Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reflection_pad2d(&outs[0], self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.reflection_pad2d_backward :: proc(self: *Tensor, grad_output: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reflection_pad2d_backward(&outs[0], grad_output.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.reflection_pad2d_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reflection_pad2d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.reflection_pad2d_out :: proc(self: *Tensor, out: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reflection_pad2d_out(&outs[0], out.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.reflection_pad3d :: proc(self: *Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reflection_pad3d(&outs[0], self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.reflection_pad3d_backward :: proc(self: *Tensor, grad_output: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reflection_pad3d_backward(&outs[0], grad_output.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.reflection_pad3d_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reflection_pad3d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.reflection_pad3d_out :: proc(self: *Tensor, out: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reflection_pad3d_out(&outs[0], out.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.relu :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_relu(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.relu6 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_relu6(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.relu6_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_relu6_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.relu_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_relu_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.relu_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_relu_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.remainder :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_remainder(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.remainder_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_remainder_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.remainder_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_remainder_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.remainder_scalar_tensor :: proc(self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_remainder_scalar_tensor(&outs[0], self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.remainder_scalar_tensor_out :: proc(out: Tensor, self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_remainder_scalar_tensor_out(&outs[0], out.t, self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.remainder_tensor :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_remainder_tensor(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.remainder_tensor_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_remainder_tensor_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.remainder_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_remainder_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.renorm :: proc(self: *Tensor, p: torch_bindings.Scalar, dim: i64, maxnorm: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_renorm(&outs[0], self.t, p, dim, maxnorm)
    return wrap_and_track(outs[0])
}

Tensor.renorm_ :: proc(self: *Tensor, p: torch_bindings.Scalar, dim: i64, maxnorm: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_renorm_(&outs[0], self.t, p, dim, maxnorm)
    return wrap_and_track(outs[0])
}

Tensor.renorm_out :: proc(self: *Tensor, out: Tensor, p: torch_bindings.Scalar, dim: i64, maxnorm: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_renorm_out(&outs[0], out.t, self.t, p, dim, maxnorm)
    return wrap_and_track(outs[0])
}

Tensor.repeat :: proc(self: *Tensor, repeats_data: *i64, repeats_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_repeat(&outs[0], self.t, repeats_data, repeats_len)
    return wrap_and_track(outs[0])
}

Tensor.repeat_interleave :: proc(repeats: Tensor, output_size_v: i64, output_size_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_repeat_interleave(&outs[0], repeats.t, output_size_v, output_size_null)
    return wrap_and_track(outs[0])
}

Tensor.repeat_interleave_self_int :: proc(self: *Tensor, repeats: i64, dim_v: i64, dim_null: u8, output_size_v: i64, output_size_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_repeat_interleave_self_int(&outs[0], self.t, repeats, dim_v, dim_null, output_size_v, output_size_null)
    return wrap_and_track(outs[0])
}

Tensor.repeat_interleave_self_tensor :: proc(self: *Tensor, repeats: Tensor, dim_v: i64, dim_null: u8, output_size_v: i64, output_size_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_repeat_interleave_self_tensor(&outs[0], self.t, repeats.t, dim_v, dim_null, output_size_v, output_size_null)
    return wrap_and_track(outs[0])
}

Tensor.repeat_interleave_tensor_out :: proc(out: Tensor, repeats: Tensor, output_size_v: i64, output_size_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_repeat_interleave_tensor_out(&outs[0], out.t, repeats.t, output_size_v, output_size_null)
    return wrap_and_track(outs[0])
}

Tensor.repeat_out :: proc(self: *Tensor, out: Tensor, repeats_data: *i64, repeats_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_repeat_out(&outs[0], out.t, self.t, repeats_data, repeats_len)
    return wrap_and_track(outs[0])
}

Tensor.replication_pad1d :: proc(self: *Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_replication_pad1d(&outs[0], self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.replication_pad1d_backward :: proc(self: *Tensor, grad_output: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_replication_pad1d_backward(&outs[0], grad_output.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.replication_pad1d_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_replication_pad1d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.replication_pad1d_out :: proc(self: *Tensor, out: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_replication_pad1d_out(&outs[0], out.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.replication_pad2d :: proc(self: *Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_replication_pad2d(&outs[0], self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.replication_pad2d_backward :: proc(self: *Tensor, grad_output: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_replication_pad2d_backward(&outs[0], grad_output.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.replication_pad2d_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_replication_pad2d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.replication_pad2d_out :: proc(self: *Tensor, out: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_replication_pad2d_out(&outs[0], out.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.replication_pad3d :: proc(self: *Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_replication_pad3d(&outs[0], self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.replication_pad3d_backward :: proc(self: *Tensor, grad_output: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_replication_pad3d_backward(&outs[0], grad_output.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.replication_pad3d_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_replication_pad3d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.replication_pad3d_out :: proc(self: *Tensor, out: Tensor, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_replication_pad3d_out(&outs[0], out.t, self.t, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.requires_grad_ :: proc(self: *Tensor, requires_grad: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_requires_grad_(&outs[0], self.t, if requires_grad { 1 } else { 0 })
    return wrap_and_track(outs[0])
}

Tensor.reshape :: proc(self: *Tensor, shape_data: *i64, shape_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reshape(&outs[0], self.t, shape_data, shape_len)
    return wrap_and_track(outs[0])
}

Tensor.reshape_as :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_reshape_as(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.resize :: proc(self: *Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_resize(&outs[0], self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.resize_ :: proc(self: *Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_resize_(&outs[0], self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.resize_as :: proc(self: *Tensor, the_template: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_resize_as(&outs[0], self.t, the_template.t)
    return wrap_and_track(outs[0])
}

Tensor.resize_as_ :: proc(self: *Tensor, the_template: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_resize_as_(&outs[0], self.t, the_template.t)
    return wrap_and_track(outs[0])
}

Tensor.resize_as_out :: proc(self: *Tensor, out: Tensor, the_template: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_resize_as_out(&outs[0], out.t, self.t, the_template.t)
    return wrap_and_track(outs[0])
}

Tensor.resize_as_sparse :: proc(self: *Tensor, the_template: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_resize_as_sparse(&outs[0], self.t, the_template.t)
    return wrap_and_track(outs[0])
}

Tensor.resize_as_sparse_ :: proc(self: *Tensor, the_template: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_resize_as_sparse_(&outs[0], self.t, the_template.t)
    return wrap_and_track(outs[0])
}

Tensor.resize_as_sparse_out :: proc(self: *Tensor, out: Tensor, the_template: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_resize_as_sparse_out(&outs[0], out.t, self.t, the_template.t)
    return wrap_and_track(outs[0])
}

Tensor.resize_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_resize_out(&outs[0], out.t, self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.resolve_conj :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_resolve_conj(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.resolve_neg :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_resolve_neg(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.retains_grad :: proc(self: *Tensor) bool {
    return torch_bindings.atg_retains_grad(self.t)
}

Tensor.rms_norm :: proc(self: *Tensor, normalized_shape_data: *i64, normalized_shape_len: i32, weight: Tensor, eps_v: f64, eps_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rms_norm(&outs[0], self.t, normalized_shape_data, normalized_shape_len, weight.t, eps_v, eps_null)
    return wrap_and_track(outs[0])
}

Tensor.rnn_relu :: proc(self: *Tensor, hx: Tensor, params_data: *torch_bindings.Tensor, params_len: i32, has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool, batch_first: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_rnn_relu(&outs[0], self.t, hx.t, params_data, params_len, has_biases, num_layers, dropout, train, bidirectional, batch_first)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.rnn_relu_cell :: proc(self: *Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rnn_relu_cell(&outs[0], self.t, hx.t, w_ih.t, w_hh.t, b_ih.t, b_hh.t)
    return wrap_and_track(outs[0])
}

Tensor.rnn_relu_data :: proc(data: Tensor, batch_sizes: Tensor, hx: Tensor, params_data: *torch_bindings.Tensor, params_len: i32, has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_rnn_relu_data(&outs[0], data.t, batch_sizes.t, hx.t, params_data, params_len, has_biases, num_layers, dropout, train, bidirectional)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.rnn_tanh :: proc(self: *Tensor, hx: Tensor, params_data: *torch_bindings.Tensor, params_len: i32, has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool, batch_first: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_rnn_tanh(&outs[0], self.t, hx.t, params_data, params_len, has_biases, num_layers, dropout, train, bidirectional, batch_first)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.rnn_tanh_cell :: proc(self: *Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rnn_tanh_cell(&outs[0], self.t, hx.t, w_ih.t, w_hh.t, b_ih.t, b_hh.t)
    return wrap_and_track(outs[0])
}

Tensor.rnn_tanh_data :: proc(data: Tensor, batch_sizes: Tensor, hx: Tensor, params_data: *torch_bindings.Tensor, params_len: i32, has_biases: bool, num_layers: i64, dropout: f64, train: bool, bidirectional: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_rnn_tanh_data(&outs[0], data.t, batch_sizes.t, hx.t, params_data, params_len, has_biases, num_layers, dropout, train, bidirectional)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.roll :: proc(self: *Tensor, shifts_data: *i64, shifts_len: i32, dims_data: *i64, dims_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_roll(&outs[0], self.t, shifts_data, shifts_len, dims_data, dims_len)
    return wrap_and_track(outs[0])
}

Tensor.roll_out :: proc(self: *Tensor, out: Tensor, shifts_data: *i64, shifts_len: i32, dims_data: *i64, dims_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_roll_out(&outs[0], out.t, self.t, shifts_data, shifts_len, dims_data, dims_len)
    return wrap_and_track(outs[0])
}

Tensor.rot90 :: proc(self: *Tensor, k: i64, dims_data: *i64, dims_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rot90(&outs[0], self.t, k, dims_data, dims_len)
    return wrap_and_track(outs[0])
}

Tensor.rot90_out :: proc(self: *Tensor, out: Tensor, k: i64, dims_data: *i64, dims_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rot90_out(&outs[0], out.t, self.t, k, dims_data, dims_len)
    return wrap_and_track(outs[0])
}

Tensor.round :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_round(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.round_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_round_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.round_decimals :: proc(self: *Tensor, decimals: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_round_decimals(&outs[0], self.t, decimals)
    return wrap_and_track(outs[0])
}

Tensor.round_decimals_ :: proc(self: *Tensor, decimals: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_round_decimals_(&outs[0], self.t, decimals)
    return wrap_and_track(outs[0])
}

Tensor.round_decimals_out :: proc(self: *Tensor, out: Tensor, decimals: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_round_decimals_out(&outs[0], out.t, self.t, decimals)
    return wrap_and_track(outs[0])
}

Tensor.round_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_round_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.row_indices :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_row_indices(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.row_indices_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_row_indices_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.row_indices_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_row_indices_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.row_stack :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_row_stack(&outs[0], tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.row_stack_out :: proc(out: Tensor, tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_row_stack_out(&outs[0], out.t, tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.rrelu :: proc(self: *Tensor, training: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rrelu(&outs[0], self.t, training)
    return wrap_and_track(outs[0])
}

Tensor.rrelu_ :: proc(self: *Tensor, training: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rrelu_(&outs[0], self.t, training)
    return wrap_and_track(outs[0])
}

Tensor.rrelu_with_noise :: proc(self: *Tensor, noise: Tensor, training: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rrelu_with_noise(&outs[0], self.t, noise.t, training)
    return wrap_and_track(outs[0])
}

Tensor.rrelu_with_noise_ :: proc(self: *Tensor, noise: Tensor, training: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rrelu_with_noise_(&outs[0], self.t, noise.t, training)
    return wrap_and_track(outs[0])
}

Tensor.rrelu_with_noise_backward :: proc(self: *Tensor, grad_output: Tensor, noise: Tensor, lower: torch_bindings.Scalar, upper: torch_bindings.Scalar, training: bool, self_is_result: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rrelu_with_noise_backward(&outs[0], grad_output.t, self.t, noise.t, lower, upper, training, self_is_result)
    return wrap_and_track(outs[0])
}

Tensor.rrelu_with_noise_backward_out :: proc(self: *Tensor, out: Tensor, grad_output: Tensor, noise: Tensor, lower: torch_bindings.Scalar, upper: torch_bindings.Scalar, training: bool, self_is_result: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rrelu_with_noise_backward_out(&outs[0], out.t, grad_output.t, self.t, noise.t, lower, upper, training, self_is_result)
    return wrap_and_track(outs[0])
}

Tensor.rrelu_with_noise_functional :: proc(self: *Tensor, noise: Tensor, training: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_rrelu_with_noise_functional(&outs[0], self.t, noise.t, training)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.rrelu_with_noise_out :: proc(self: *Tensor, out: Tensor, noise: Tensor, training: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rrelu_with_noise_out(&outs[0], out.t, self.t, noise.t, training)
    return wrap_and_track(outs[0])
}

Tensor.rsqrt :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rsqrt(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.rsqrt_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rsqrt_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.rsqrt_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rsqrt_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.rsub :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rsub(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.rsub_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rsub_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.rsub_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rsub_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.rsub_tensor_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_rsub_tensor_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.scalar_tensor :: proc(s: torch_bindings.Scalar, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scalar_tensor(&outs[0], s, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.scalar_tensor_out :: proc(out: Tensor, s: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scalar_tensor_out(&outs[0], out.t, s)
    return wrap_and_track(outs[0])
}

Tensor.scaled_dot_product_attention :: proc(query: Tensor, key: Tensor, value: Tensor, attn_mask: Tensor, dropout_p: f64, is_causal: bool, scale_v: f64, scale_null: u8, enable_gqa: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scaled_dot_product_attention(&outs[0], query.t, key.t, value.t, attn_mask.t, dropout_p, is_causal, scale_v, scale_null, enable_gqa)
    return wrap_and_track(outs[0])
}

Tensor.scatter :: proc(self: *Tensor, dim: i64, index: Tensor, src: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter(&outs[0], self.t, dim, index.t, src.t)
    return wrap_and_track(outs[0])
}

Tensor.scatter_ :: proc(self: *Tensor, dim: i64, index: Tensor, src: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter_(&outs[0], self.t, dim, index.t, src.t)
    return wrap_and_track(outs[0])
}

Tensor.scatter_add :: proc(self: *Tensor, dim: i64, index: Tensor, src: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter_add(&outs[0], self.t, dim, index.t, src.t)
    return wrap_and_track(outs[0])
}

Tensor.scatter_add_ :: proc(self: *Tensor, dim: i64, index: Tensor, src: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter_add_(&outs[0], self.t, dim, index.t, src.t)
    return wrap_and_track(outs[0])
}

Tensor.scatter_add_out :: proc(self: *Tensor, out: Tensor, dim: i64, index: Tensor, src: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter_add_out(&outs[0], out.t, self.t, dim, index.t, src.t)
    return wrap_and_track(outs[0])
}

Tensor.scatter_reduce :: proc(self: *Tensor, dim: i64, index: Tensor, src: Tensor, reduce_ptr: *u8, reduce_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter_reduce(&outs[0], self.t, dim, index.t, src.t, reduce_ptr, reduce_len)
    return wrap_and_track(outs[0])
}

Tensor.scatter_reduce_ :: proc(self: *Tensor, dim: i64, index: Tensor, src: Tensor, reduce_ptr: *u8, reduce_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter_reduce_(&outs[0], self.t, dim, index.t, src.t, reduce_ptr, reduce_len)
    return wrap_and_track(outs[0])
}

Tensor.scatter_reduce_out :: proc(self: *Tensor, out: Tensor, dim: i64, index: Tensor, src: Tensor, reduce_ptr: *u8, reduce_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter_reduce_out(&outs[0], out.t, self.t, dim, index.t, src.t, reduce_ptr, reduce_len)
    return wrap_and_track(outs[0])
}

Tensor.scatter_src_out :: proc(self: *Tensor, out: Tensor, dim: i64, index: Tensor, src: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter_src_out(&outs[0], out.t, self.t, dim, index.t, src.t)
    return wrap_and_track(outs[0])
}

Tensor.scatter_value :: proc(self: *Tensor, dim: i64, index: Tensor, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter_value(&outs[0], self.t, dim, index.t, value)
    return wrap_and_track(outs[0])
}

Tensor.scatter_value_ :: proc(self: *Tensor, dim: i64, index: Tensor, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter_value_(&outs[0], self.t, dim, index.t, value)
    return wrap_and_track(outs[0])
}

Tensor.scatter_value_out :: proc(self: *Tensor, out: Tensor, dim: i64, index: Tensor, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter_value_out(&outs[0], out.t, self.t, dim, index.t, value)
    return wrap_and_track(outs[0])
}

Tensor.scatter_value_reduce :: proc(self: *Tensor, dim: i64, index: Tensor, value: torch_bindings.Scalar, reduce_ptr: *u8, reduce_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter_value_reduce(&outs[0], self.t, dim, index.t, value, reduce_ptr, reduce_len)
    return wrap_and_track(outs[0])
}

Tensor.scatter_value_reduce_ :: proc(self: *Tensor, dim: i64, index: Tensor, value: torch_bindings.Scalar, reduce_ptr: *u8, reduce_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter_value_reduce_(&outs[0], self.t, dim, index.t, value, reduce_ptr, reduce_len)
    return wrap_and_track(outs[0])
}

Tensor.scatter_value_reduce_out :: proc(self: *Tensor, out: Tensor, dim: i64, index: Tensor, value: torch_bindings.Scalar, reduce_ptr: *u8, reduce_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_scatter_value_reduce_out(&outs[0], out.t, self.t, dim, index.t, value, reduce_ptr, reduce_len)
    return wrap_and_track(outs[0])
}

Tensor.searchsorted :: proc(self: *Tensor, sorted_sequence: Tensor, out_int32: bool, right: bool, side_ptr: *u8, side_len: i32, sorter: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_searchsorted(&outs[0], sorted_sequence.t, self.t, out_int32, right, side_ptr, side_len, sorter.t)
    return wrap_and_track(outs[0])
}

Tensor.searchsorted_scalar :: proc(sorted_sequence: Tensor, self_scalar: torch_bindings.Scalar, out_int32: bool, right: bool, side_ptr: *u8, side_len: i32, sorter: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_searchsorted_scalar(&outs[0], sorted_sequence.t, self_scalar, out_int32, right, side_ptr, side_len, sorter.t)
    return wrap_and_track(outs[0])
}

Tensor.searchsorted_scalar_out :: proc(out: Tensor, sorted_sequence: Tensor, self_scalar: torch_bindings.Scalar, out_int32: bool, right: bool, side_ptr: *u8, side_len: i32, sorter: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_searchsorted_scalar_out(&outs[0], out.t, sorted_sequence.t, self_scalar, out_int32, right, side_ptr, side_len, sorter.t)
    return wrap_and_track(outs[0])
}

Tensor.searchsorted_tensor_out :: proc(self: *Tensor, out: Tensor, sorted_sequence: Tensor, out_int32: bool, right: bool, side_ptr: *u8, side_len: i32, sorter: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_searchsorted_tensor_out(&outs[0], out.t, sorted_sequence.t, self.t, out_int32, right, side_ptr, side_len, sorter.t)
    return wrap_and_track(outs[0])
}

Tensor.segment_reduce :: proc(data: Tensor, reduce_ptr: *u8, reduce_len: i32, lengths: Tensor, indices: Tensor, offsets: Tensor, axis: i64, unsafe: bool, initial: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_segment_reduce(&outs[0], data.t, reduce_ptr, reduce_len, lengths.t, indices.t, offsets.t, axis, unsafe, initial)
    return wrap_and_track(outs[0])
}

Tensor.segment_reduce_out :: proc(out: Tensor, data: Tensor, reduce_ptr: *u8, reduce_len: i32, lengths: Tensor, indices: Tensor, offsets: Tensor, axis: i64, unsafe: bool, initial: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_segment_reduce_out(&outs[0], out.t, data.t, reduce_ptr, reduce_len, lengths.t, indices.t, offsets.t, axis, unsafe, initial)
    return wrap_and_track(outs[0])
}

Tensor.select :: proc(self: *Tensor, dim: i64, index: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_select(&outs[0], self.t, dim, index)
    return wrap_and_track(outs[0])
}

Tensor.select_backward :: proc(grad_output: Tensor, input_sizes_data: *i64, input_sizes_len: i32, dim: i64, index: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_select_backward(&outs[0], grad_output.t, input_sizes_data, input_sizes_len, dim, index)
    return wrap_and_track(outs[0])
}

Tensor.select_backward_out :: proc(out: Tensor, grad_output: Tensor, input_sizes_data: *i64, input_sizes_len: i32, dim: i64, index: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_select_backward_out(&outs[0], out.t, grad_output.t, input_sizes_data, input_sizes_len, dim, index)
    return wrap_and_track(outs[0])
}

Tensor.select_copy :: proc(self: *Tensor, dim: i64, index: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_select_copy(&outs[0], self.t, dim, index)
    return wrap_and_track(outs[0])
}

Tensor.select_copy_int_out :: proc(self: *Tensor, out: Tensor, dim: i64, index: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_select_copy_int_out(&outs[0], out.t, self.t, dim, index)
    return wrap_and_track(outs[0])
}

Tensor.select_scatter :: proc(self: *Tensor, src: Tensor, dim: i64, index: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_select_scatter(&outs[0], self.t, src.t, dim, index)
    return wrap_and_track(outs[0])
}

Tensor.select_scatter_out :: proc(self: *Tensor, out: Tensor, src: Tensor, dim: i64, index: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_select_scatter_out(&outs[0], out.t, self.t, src.t, dim, index)
    return wrap_and_track(outs[0])
}

Tensor.selu :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_selu(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.selu_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_selu_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.set :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_set(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.set_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_set_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.set_data :: proc(self: *Tensor, new_data: Tensor) void {
    torch_bindings.atg_set_data(self.t, new_data.t)
}

Tensor.set_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_set_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.set_requires_grad :: proc(self: *Tensor, r: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_set_requires_grad(&outs[0], self.t, r)
    return wrap_and_track(outs[0])
}

Tensor.set_source_tensor :: proc(self: *Tensor, source: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_set_source_tensor(&outs[0], self.t, source.t)
    return wrap_and_track(outs[0])
}

Tensor.set_source_tensor_ :: proc(self: *Tensor, source: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_set_source_tensor_(&outs[0], self.t, source.t)
    return wrap_and_track(outs[0])
}

Tensor.set_source_tensor_out :: proc(self: *Tensor, out: Tensor, source: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_set_source_tensor_out(&outs[0], out.t, self.t, source.t)
    return wrap_and_track(outs[0])
}

Tensor.set_source_tensor_storage_offset_ :: proc(self: *Tensor, source: Tensor, storage_offset: i64, size_data: *i64, size_len: i32, stride_data: *i64, stride_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_set_source_tensor_storage_offset_(&outs[0], self.t, source.t, storage_offset, size_data, size_len, stride_data, stride_len)
    return wrap_and_track(outs[0])
}

Tensor.sgn :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sgn(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.sgn_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sgn_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.sgn_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sgn_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.sigmoid :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sigmoid(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.sigmoid_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sigmoid_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.sigmoid_backward :: proc(grad_output: Tensor, output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sigmoid_backward(&outs[0], grad_output.t, output.t)
    return wrap_and_track(outs[0])
}

Tensor.sigmoid_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sigmoid_backward_grad_input(&outs[0], grad_input.t, grad_output.t, output.t)
    return wrap_and_track(outs[0])
}

Tensor.sigmoid_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sigmoid_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.sign :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sign(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.sign_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sign_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.sign_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sign_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.signbit :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_signbit(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.signbit_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_signbit_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.silu :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_silu(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.silu_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_silu_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.silu_backward :: proc(self: *Tensor, grad_output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_silu_backward(&outs[0], grad_output.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.silu_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_silu_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.silu_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_silu_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.sin :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sin(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.sin_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sin_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.sin_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sin_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.sinc :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sinc(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.sinc_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sinc_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.sinc_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sinc_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.sinh :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sinh(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.sinh_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sinh_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.sinh_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sinh_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.slice :: proc(self: *Tensor, dim: i64, start_v: i64, start_null: u8, end_v: i64, end_null: u8, step: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slice(&outs[0], self.t, dim, start_v, start_null, end_v, end_null, step)
    return wrap_and_track(outs[0])
}

Tensor.slice_backward :: proc(grad_output: Tensor, input_sizes_data: *i64, input_sizes_len: i32, dim: i64, start: i64, end: i64, step: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slice_backward(&outs[0], grad_output.t, input_sizes_data, input_sizes_len, dim, start, end, step)
    return wrap_and_track(outs[0])
}

Tensor.slice_backward_out :: proc(out: Tensor, grad_output: Tensor, input_sizes_data: *i64, input_sizes_len: i32, dim: i64, start: i64, end: i64, step: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slice_backward_out(&outs[0], out.t, grad_output.t, input_sizes_data, input_sizes_len, dim, start, end, step)
    return wrap_and_track(outs[0])
}

Tensor.slice_copy :: proc(self: *Tensor, dim: i64, start_v: i64, start_null: u8, end_v: i64, end_null: u8, step: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slice_copy(&outs[0], self.t, dim, start_v, start_null, end_v, end_null, step)
    return wrap_and_track(outs[0])
}

Tensor.slice_copy_tensor_out :: proc(self: *Tensor, out: Tensor, dim: i64, start_v: i64, start_null: u8, end_v: i64, end_null: u8, step: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slice_copy_tensor_out(&outs[0], out.t, self.t, dim, start_v, start_null, end_v, end_null, step)
    return wrap_and_track(outs[0])
}

Tensor.slice_inverse :: proc(self: *Tensor, src: Tensor, dim: i64, start_v: i64, start_null: u8, end_v: i64, end_null: u8, step: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slice_inverse(&outs[0], self.t, src.t, dim, start_v, start_null, end_v, end_null, step)
    return wrap_and_track(outs[0])
}

Tensor.slice_scatter :: proc(self: *Tensor, src: Tensor, dim: i64, start_v: i64, start_null: u8, end_v: i64, end_null: u8, step: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slice_scatter(&outs[0], self.t, src.t, dim, start_v, start_null, end_v, end_null, step)
    return wrap_and_track(outs[0])
}

Tensor.slice_scatter_out :: proc(self: *Tensor, out: Tensor, src: Tensor, dim: i64, start_v: i64, start_null: u8, end_v: i64, end_null: u8, step: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slice_scatter_out(&outs[0], out.t, self.t, src.t, dim, start_v, start_null, end_v, end_null, step)
    return wrap_and_track(outs[0])
}

Tensor.slogdet :: proc(self: *Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_slogdet(&outs[0], self.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.slogdet_out :: proc(self: *Tensor, sign: Tensor, logabsdet: Tensor) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_slogdet_out(&outs[0], sign.t, logabsdet.t, self.t)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.slow_conv3d :: proc(self: *Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slow_conv3d(&outs[0], self.t, weight.t, kernel_size_data, kernel_size_len, bias.t, stride_data, stride_len, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.slow_conv3d_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slow_conv3d_out(&outs[0], out.t, self.t, weight.t, kernel_size_data, kernel_size_len, bias.t, stride_data, stride_len, padding_data, padding_len)
    return wrap_and_track(outs[0])
}

Tensor.slow_conv_dilated2d :: proc(self: *Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slow_conv_dilated2d(&outs[0], self.t, weight.t, kernel_size_data, kernel_size_len, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor.slow_conv_dilated2d_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slow_conv_dilated2d_out(&outs[0], out.t, self.t, weight.t, kernel_size_data, kernel_size_len, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor.slow_conv_dilated3d :: proc(self: *Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slow_conv_dilated3d(&outs[0], self.t, weight.t, kernel_size_data, kernel_size_len, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor.slow_conv_dilated3d_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slow_conv_dilated3d_out(&outs[0], out.t, self.t, weight.t, kernel_size_data, kernel_size_len, bias.t, stride_data, stride_len, padding_data, padding_len, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor.slow_conv_transpose2d :: proc(self: *Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, output_padding_data: *i64, output_padding_len: i32, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slow_conv_transpose2d(&outs[0], self.t, weight.t, kernel_size_data, kernel_size_len, bias.t, stride_data, stride_len, padding_data, padding_len, output_padding_data, output_padding_len, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor.slow_conv_transpose2d_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, output_padding_data: *i64, output_padding_len: i32, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slow_conv_transpose2d_out(&outs[0], out.t, self.t, weight.t, kernel_size_data, kernel_size_len, bias.t, stride_data, stride_len, padding_data, padding_len, output_padding_data, output_padding_len, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor.slow_conv_transpose3d :: proc(self: *Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, output_padding_data: *i64, output_padding_len: i32, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slow_conv_transpose3d(&outs[0], self.t, weight.t, kernel_size_data, kernel_size_len, bias.t, stride_data, stride_len, padding_data, padding_len, output_padding_data, output_padding_len, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor.slow_conv_transpose3d_out :: proc(self: *Tensor, out: Tensor, weight: Tensor, kernel_size_data: *i64, kernel_size_len: i32, bias: Tensor, stride_data: *i64, stride_len: i32, padding_data: *i64, padding_len: i32, output_padding_data: *i64, output_padding_len: i32, dilation_data: *i64, dilation_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_slow_conv_transpose3d_out(&outs[0], out.t, self.t, weight.t, kernel_size_data, kernel_size_len, bias.t, stride_data, stride_len, padding_data, padding_len, output_padding_data, output_padding_len, dilation_data, dilation_len)
    return wrap_and_track(outs[0])
}

Tensor.smm :: proc(self: *Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_smm(&outs[0], self.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor.smooth_l1_loss :: proc(self: *Tensor, target: Tensor, reduction: i64, beta: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_smooth_l1_loss(&outs[0], self.t, target.t, reduction, beta)
    return wrap_and_track(outs[0])
}

Tensor.smooth_l1_loss_backward :: proc(self: *Tensor, grad_output: Tensor, target: Tensor, reduction: i64, beta: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_smooth_l1_loss_backward(&outs[0], grad_output.t, self.t, target.t, reduction, beta)
    return wrap_and_track(outs[0])
}

Tensor.smooth_l1_loss_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, target: Tensor, reduction: i64, beta: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_smooth_l1_loss_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, target.t, reduction, beta)
    return wrap_and_track(outs[0])
}

Tensor.smooth_l1_loss_out :: proc(self: *Tensor, out: Tensor, target: Tensor, reduction: i64, beta: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_smooth_l1_loss_out(&outs[0], out.t, self.t, target.t, reduction, beta)
    return wrap_and_track(outs[0])
}

Tensor.soft_margin_loss :: proc(self: *Tensor, target: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_soft_margin_loss(&outs[0], self.t, target.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.soft_margin_loss_backward :: proc(self: *Tensor, grad_output: Tensor, target: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_soft_margin_loss_backward(&outs[0], grad_output.t, self.t, target.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.soft_margin_loss_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, target: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_soft_margin_loss_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, target.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.soft_margin_loss_out :: proc(self: *Tensor, out: Tensor, target: Tensor, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_soft_margin_loss_out(&outs[0], out.t, self.t, target.t, reduction)
    return wrap_and_track(outs[0])
}

Tensor.softmax :: proc(self: *Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_softmax(&outs[0], self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.softmax_int_out :: proc(self: *Tensor, out: Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_softmax_int_out(&outs[0], out.t, self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.softplus :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_softplus(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.softplus_backward :: proc(self: *Tensor, grad_output: Tensor, beta: torch_bindings.Scalar, threshold: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_softplus_backward(&outs[0], grad_output.t, self.t, beta, threshold)
    return wrap_and_track(outs[0])
}

Tensor.softplus_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, beta: torch_bindings.Scalar, threshold: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_softplus_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, beta, threshold)
    return wrap_and_track(outs[0])
}

Tensor.softplus_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_softplus_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.softshrink :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_softshrink(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.softshrink_backward :: proc(self: *Tensor, grad_output: Tensor, lambd: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_softshrink_backward(&outs[0], grad_output.t, self.t, lambd)
    return wrap_and_track(outs[0])
}

Tensor.softshrink_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, lambd: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_softshrink_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, lambd)
    return wrap_and_track(outs[0])
}

Tensor.softshrink_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_softshrink_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.sort :: proc(self: *Tensor, dim: i64, descending: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_sort(&outs[0], self.t, dim, descending)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.sort_stable :: proc(self: *Tensor, stable: bool, dim: i64, descending: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_sort_stable(&outs[0], self.t, stable, dim, descending)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.sort_values :: proc(self: *Tensor, values: Tensor, indices: Tensor, dim: i64, descending: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_sort_values(&outs[0], values.t, indices.t, self.t, dim, descending)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.sort_values_stable :: proc(self: *Tensor, values: Tensor, indices: Tensor, stable: bool, dim: i64, descending: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_sort_values_stable(&outs[0], values.t, indices.t, self.t, stable, dim, descending)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.sparse_bsc_tensor :: proc(ccol_indices: Tensor, row_indices: Tensor, values: Tensor, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_bsc_tensor(&outs[0], ccol_indices.t, row_indices.t, values.t, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.sparse_bsc_tensor_ccol_row_value_size :: proc(ccol_indices: Tensor, row_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_bsc_tensor_ccol_row_value_size(&outs[0], ccol_indices.t, row_indices.t, values.t, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.sparse_bsr_tensor :: proc(crow_indices: Tensor, col_indices: Tensor, values: Tensor, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_bsr_tensor(&outs[0], crow_indices.t, col_indices.t, values.t, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.sparse_bsr_tensor_crow_col_value_size :: proc(crow_indices: Tensor, col_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_bsr_tensor_crow_col_value_size(&outs[0], crow_indices.t, col_indices.t, values.t, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.sparse_compressed_tensor :: proc(compressed_indices: Tensor, plain_indices: Tensor, values: Tensor, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_compressed_tensor(&outs[0], compressed_indices.t, plain_indices.t, values.t, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.sparse_compressed_tensor_comp_plain_value_size :: proc(compressed_indices: Tensor, plain_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_compressed_tensor_comp_plain_value_size(&outs[0], compressed_indices.t, plain_indices.t, values.t, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.sparse_coo_tensor :: proc(size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_coo_tensor(&outs[0], size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.sparse_coo_tensor_indices :: proc(indices: Tensor, values: Tensor, options_kind: i32, options_device: i32, is_coalesced: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_coo_tensor_indices(&outs[0], indices.t, values.t, options_kind, options_device, is_coalesced)
    return wrap_and_track(outs[0])
}

Tensor.sparse_coo_tensor_indices_size :: proc(indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32, is_coalesced: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_coo_tensor_indices_size(&outs[0], indices.t, values.t, size_data, size_len, options_kind, options_device, is_coalesced)
    return wrap_and_track(outs[0])
}

Tensor.sparse_coo_tensor_size_out :: proc(out: Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_coo_tensor_size_out(&outs[0], out.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.sparse_csc_tensor :: proc(ccol_indices: Tensor, row_indices: Tensor, values: Tensor, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_csc_tensor(&outs[0], ccol_indices.t, row_indices.t, values.t, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.sparse_csc_tensor_ccol_row_value_size :: proc(ccol_indices: Tensor, row_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_csc_tensor_ccol_row_value_size(&outs[0], ccol_indices.t, row_indices.t, values.t, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.sparse_csr_tensor :: proc(crow_indices: Tensor, col_indices: Tensor, values: Tensor, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_csr_tensor(&outs[0], crow_indices.t, col_indices.t, values.t, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.sparse_csr_tensor_crow_col_value_size :: proc(crow_indices: Tensor, col_indices: Tensor, values: Tensor, size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_csr_tensor_crow_col_value_size(&outs[0], crow_indices.t, col_indices.t, values.t, size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.sparse_dim :: proc(self: *Tensor) i64 {
    return torch_bindings.atg_sparse_dim(self.t)
}

Tensor.sparse_mask :: proc(self: *Tensor, mask: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_mask(&outs[0], self.t, mask.t)
    return wrap_and_track(outs[0])
}

Tensor.sparse_mask_out :: proc(self: *Tensor, out: Tensor, mask: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_mask_out(&outs[0], out.t, self.t, mask.t)
    return wrap_and_track(outs[0])
}

Tensor.sparse_resize :: proc(self: *Tensor, size_data: *i64, size_len: i32, sparse_dim: i64, dense_dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_resize(&outs[0], self.t, size_data, size_len, sparse_dim, dense_dim)
    return wrap_and_track(outs[0])
}

Tensor.sparse_resize_ :: proc(self: *Tensor, size_data: *i64, size_len: i32, sparse_dim: i64, dense_dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_resize_(&outs[0], self.t, size_data, size_len, sparse_dim, dense_dim)
    return wrap_and_track(outs[0])
}

Tensor.sparse_resize_and_clear :: proc(self: *Tensor, size_data: *i64, size_len: i32, sparse_dim: i64, dense_dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_resize_and_clear(&outs[0], self.t, size_data, size_len, sparse_dim, dense_dim)
    return wrap_and_track(outs[0])
}

Tensor.sparse_resize_and_clear_ :: proc(self: *Tensor, size_data: *i64, size_len: i32, sparse_dim: i64, dense_dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_resize_and_clear_(&outs[0], self.t, size_data, size_len, sparse_dim, dense_dim)
    return wrap_and_track(outs[0])
}

Tensor.sparse_resize_and_clear_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32, sparse_dim: i64, dense_dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_resize_and_clear_out(&outs[0], out.t, self.t, size_data, size_len, sparse_dim, dense_dim)
    return wrap_and_track(outs[0])
}

Tensor.sparse_resize_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32, sparse_dim: i64, dense_dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_resize_out(&outs[0], out.t, self.t, size_data, size_len, sparse_dim, dense_dim)
    return wrap_and_track(outs[0])
}

Tensor.sparse_sampled_addmm :: proc(self: *Tensor, mat1: Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_sampled_addmm(&outs[0], self.t, mat1.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor.sparse_sampled_addmm_out :: proc(self: *Tensor, out: Tensor, mat1: Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sparse_sampled_addmm_out(&outs[0], out.t, self.t, mat1.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor.special_airy_ai :: proc(x: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_airy_ai(&outs[0], x.t)
    return wrap_and_track(outs[0])
}

Tensor.special_airy_ai_out :: proc(out: Tensor, x: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_airy_ai_out(&outs[0], out.t, x.t)
    return wrap_and_track(outs[0])
}

Tensor.special_bessel_j0 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_bessel_j0(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_bessel_j0_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_bessel_j0_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_bessel_j1 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_bessel_j1(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_bessel_j1_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_bessel_j1_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_bessel_y0 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_bessel_y0(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_bessel_y0_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_bessel_y0_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_bessel_y1 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_bessel_y1(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_bessel_y1_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_bessel_y1_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_t :: proc(x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_t(&outs[0], x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_t_n_scalar :: proc(x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_t_n_scalar(&outs[0], x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_t_n_scalar_out :: proc(out: Tensor, x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_t_n_scalar_out(&outs[0], out.t, x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_t_out :: proc(out: Tensor, x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_t_out(&outs[0], out.t, x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_t_x_scalar :: proc(x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_t_x_scalar(&outs[0], x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_t_x_scalar_out :: proc(out: Tensor, x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_t_x_scalar_out(&outs[0], out.t, x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_u :: proc(x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_u(&outs[0], x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_u_n_scalar :: proc(x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_u_n_scalar(&outs[0], x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_u_n_scalar_out :: proc(out: Tensor, x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_u_n_scalar_out(&outs[0], out.t, x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_u_out :: proc(out: Tensor, x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_u_out(&outs[0], out.t, x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_u_x_scalar :: proc(x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_u_x_scalar(&outs[0], x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_u_x_scalar_out :: proc(out: Tensor, x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_u_x_scalar_out(&outs[0], out.t, x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_v :: proc(x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_v(&outs[0], x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_v_n_scalar :: proc(x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_v_n_scalar(&outs[0], x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_v_n_scalar_out :: proc(out: Tensor, x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_v_n_scalar_out(&outs[0], out.t, x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_v_out :: proc(out: Tensor, x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_v_out(&outs[0], out.t, x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_v_x_scalar :: proc(x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_v_x_scalar(&outs[0], x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_v_x_scalar_out :: proc(out: Tensor, x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_v_x_scalar_out(&outs[0], out.t, x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_w :: proc(x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_w(&outs[0], x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_w_n_scalar :: proc(x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_w_n_scalar(&outs[0], x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_w_n_scalar_out :: proc(out: Tensor, x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_w_n_scalar_out(&outs[0], out.t, x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_w_out :: proc(out: Tensor, x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_w_out(&outs[0], out.t, x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_w_x_scalar :: proc(x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_w_x_scalar(&outs[0], x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_chebyshev_polynomial_w_x_scalar_out :: proc(out: Tensor, x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_chebyshev_polynomial_w_x_scalar_out(&outs[0], out.t, x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_digamma :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_digamma(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_digamma_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_digamma_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_entr :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_entr(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_entr_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_entr_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_erf :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_erf(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_erf_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_erf_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_erfc :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_erfc(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_erfc_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_erfc_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_erfcx :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_erfcx(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_erfcx_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_erfcx_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_erfinv :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_erfinv(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_erfinv_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_erfinv_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_exp2 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_exp2(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_exp2_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_exp2_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_expit :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_expit(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_expit_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_expit_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_expm1 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_expm1(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_expm1_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_expm1_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_gammainc :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_gammainc(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_gammainc_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_gammainc_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_gammaincc :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_gammaincc(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_gammaincc_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_gammaincc_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_gammaln :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_gammaln(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_gammaln_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_gammaln_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_hermite_polynomial_h :: proc(x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_hermite_polynomial_h(&outs[0], x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_hermite_polynomial_h_n_scalar :: proc(x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_hermite_polynomial_h_n_scalar(&outs[0], x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_hermite_polynomial_h_n_scalar_out :: proc(out: Tensor, x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_hermite_polynomial_h_n_scalar_out(&outs[0], out.t, x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_hermite_polynomial_h_out :: proc(out: Tensor, x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_hermite_polynomial_h_out(&outs[0], out.t, x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_hermite_polynomial_h_x_scalar :: proc(x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_hermite_polynomial_h_x_scalar(&outs[0], x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_hermite_polynomial_h_x_scalar_out :: proc(out: Tensor, x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_hermite_polynomial_h_x_scalar_out(&outs[0], out.t, x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_hermite_polynomial_he :: proc(x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_hermite_polynomial_he(&outs[0], x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_hermite_polynomial_he_n_scalar :: proc(x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_hermite_polynomial_he_n_scalar(&outs[0], x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_hermite_polynomial_he_n_scalar_out :: proc(out: Tensor, x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_hermite_polynomial_he_n_scalar_out(&outs[0], out.t, x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_hermite_polynomial_he_out :: proc(out: Tensor, x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_hermite_polynomial_he_out(&outs[0], out.t, x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_hermite_polynomial_he_x_scalar :: proc(x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_hermite_polynomial_he_x_scalar(&outs[0], x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_hermite_polynomial_he_x_scalar_out :: proc(out: Tensor, x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_hermite_polynomial_he_x_scalar_out(&outs[0], out.t, x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_i0 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_i0(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_i0_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_i0_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_i0e :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_i0e(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_i0e_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_i0e_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_i1 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_i1(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_i1_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_i1_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_i1e :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_i1e(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_i1e_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_i1e_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_laguerre_polynomial_l :: proc(x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_laguerre_polynomial_l(&outs[0], x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_laguerre_polynomial_l_n_scalar :: proc(x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_laguerre_polynomial_l_n_scalar(&outs[0], x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_laguerre_polynomial_l_n_scalar_out :: proc(out: Tensor, x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_laguerre_polynomial_l_n_scalar_out(&outs[0], out.t, x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_laguerre_polynomial_l_out :: proc(out: Tensor, x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_laguerre_polynomial_l_out(&outs[0], out.t, x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_laguerre_polynomial_l_x_scalar :: proc(x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_laguerre_polynomial_l_x_scalar(&outs[0], x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_laguerre_polynomial_l_x_scalar_out :: proc(out: Tensor, x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_laguerre_polynomial_l_x_scalar_out(&outs[0], out.t, x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_legendre_polynomial_p :: proc(x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_legendre_polynomial_p(&outs[0], x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_legendre_polynomial_p_n_scalar :: proc(x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_legendre_polynomial_p_n_scalar(&outs[0], x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_legendre_polynomial_p_n_scalar_out :: proc(out: Tensor, x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_legendre_polynomial_p_n_scalar_out(&outs[0], out.t, x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_legendre_polynomial_p_out :: proc(out: Tensor, x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_legendre_polynomial_p_out(&outs[0], out.t, x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_legendre_polynomial_p_x_scalar :: proc(x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_legendre_polynomial_p_x_scalar(&outs[0], x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_legendre_polynomial_p_x_scalar_out :: proc(out: Tensor, x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_legendre_polynomial_p_x_scalar_out(&outs[0], out.t, x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_log1p :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_log1p(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_log1p_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_log1p_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_log_ndtr :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_log_ndtr(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_log_ndtr_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_log_ndtr_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_log_softmax :: proc(self: *Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_log_softmax(&outs[0], self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.special_logit :: proc(self: *Tensor, eps_v: f64, eps_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_logit(&outs[0], self.t, eps_v, eps_null)
    return wrap_and_track(outs[0])
}

Tensor.special_logit_out :: proc(self: *Tensor, out: Tensor, eps_v: f64, eps_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_logit_out(&outs[0], out.t, self.t, eps_v, eps_null)
    return wrap_and_track(outs[0])
}

Tensor.special_logsumexp :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_logsumexp(&outs[0], self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.special_logsumexp_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_logsumexp_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.special_modified_bessel_i0 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_modified_bessel_i0(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_modified_bessel_i0_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_modified_bessel_i0_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_modified_bessel_i1 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_modified_bessel_i1(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_modified_bessel_i1_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_modified_bessel_i1_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_modified_bessel_k0 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_modified_bessel_k0(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_modified_bessel_k0_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_modified_bessel_k0_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_modified_bessel_k1 :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_modified_bessel_k1(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_modified_bessel_k1_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_modified_bessel_k1_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_multigammaln :: proc(self: *Tensor, p: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_multigammaln(&outs[0], self.t, p)
    return wrap_and_track(outs[0])
}

Tensor.special_multigammaln_out :: proc(self: *Tensor, out: Tensor, p: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_multigammaln_out(&outs[0], out.t, self.t, p)
    return wrap_and_track(outs[0])
}

Tensor.special_ndtr :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_ndtr(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_ndtr_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_ndtr_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_ndtri :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_ndtri(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_ndtri_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_ndtri_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_polygamma :: proc(self: *Tensor, n: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_polygamma(&outs[0], n, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_polygamma_out :: proc(self: *Tensor, out: Tensor, n: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_polygamma_out(&outs[0], out.t, n, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_psi :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_psi(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_psi_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_psi_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_round :: proc(self: *Tensor, decimals: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_round(&outs[0], self.t, decimals)
    return wrap_and_track(outs[0])
}

Tensor.special_round_out :: proc(self: *Tensor, out: Tensor, decimals: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_round_out(&outs[0], out.t, self.t, decimals)
    return wrap_and_track(outs[0])
}

Tensor.special_scaled_modified_bessel_k0 :: proc(x: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_scaled_modified_bessel_k0(&outs[0], x.t)
    return wrap_and_track(outs[0])
}

Tensor.special_scaled_modified_bessel_k0_out :: proc(out: Tensor, x: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_scaled_modified_bessel_k0_out(&outs[0], out.t, x.t)
    return wrap_and_track(outs[0])
}

Tensor.special_scaled_modified_bessel_k1 :: proc(x: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_scaled_modified_bessel_k1(&outs[0], x.t)
    return wrap_and_track(outs[0])
}

Tensor.special_scaled_modified_bessel_k1_out :: proc(out: Tensor, x: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_scaled_modified_bessel_k1_out(&outs[0], out.t, x.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_t :: proc(x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_t(&outs[0], x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_t_n_scalar :: proc(x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_t_n_scalar(&outs[0], x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_t_n_scalar_out :: proc(out: Tensor, x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_t_n_scalar_out(&outs[0], out.t, x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_t_out :: proc(out: Tensor, x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_t_out(&outs[0], out.t, x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_t_x_scalar :: proc(x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_t_x_scalar(&outs[0], x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_t_x_scalar_out :: proc(out: Tensor, x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_t_x_scalar_out(&outs[0], out.t, x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_u :: proc(x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_u(&outs[0], x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_u_n_scalar :: proc(x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_u_n_scalar(&outs[0], x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_u_n_scalar_out :: proc(out: Tensor, x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_u_n_scalar_out(&outs[0], out.t, x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_u_out :: proc(out: Tensor, x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_u_out(&outs[0], out.t, x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_u_x_scalar :: proc(x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_u_x_scalar(&outs[0], x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_u_x_scalar_out :: proc(out: Tensor, x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_u_x_scalar_out(&outs[0], out.t, x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_v :: proc(x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_v(&outs[0], x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_v_n_scalar :: proc(x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_v_n_scalar(&outs[0], x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_v_n_scalar_out :: proc(out: Tensor, x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_v_n_scalar_out(&outs[0], out.t, x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_v_out :: proc(out: Tensor, x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_v_out(&outs[0], out.t, x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_v_x_scalar :: proc(x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_v_x_scalar(&outs[0], x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_v_x_scalar_out :: proc(out: Tensor, x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_v_x_scalar_out(&outs[0], out.t, x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_w :: proc(x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_w(&outs[0], x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_w_n_scalar :: proc(x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_w_n_scalar(&outs[0], x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_w_n_scalar_out :: proc(out: Tensor, x: Tensor, n: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_w_n_scalar_out(&outs[0], out.t, x.t, n)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_w_out :: proc(out: Tensor, x: Tensor, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_w_out(&outs[0], out.t, x.t, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_w_x_scalar :: proc(x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_w_x_scalar(&outs[0], x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_shifted_chebyshev_polynomial_w_x_scalar_out :: proc(out: Tensor, x: torch_bindings.Scalar, n: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_shifted_chebyshev_polynomial_w_x_scalar_out(&outs[0], out.t, x, n.t)
    return wrap_and_track(outs[0])
}

Tensor.special_sinc :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_sinc(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_sinc_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_sinc_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.special_softmax :: proc(self: *Tensor, dim: i64, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_softmax(&outs[0], self.t, dim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.special_spherical_bessel_j0 :: proc(x: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_spherical_bessel_j0(&outs[0], x.t)
    return wrap_and_track(outs[0])
}

Tensor.special_spherical_bessel_j0_out :: proc(out: Tensor, x: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_spherical_bessel_j0_out(&outs[0], out.t, x.t)
    return wrap_and_track(outs[0])
}

Tensor.special_xlog1py :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_xlog1py(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_xlog1py_other_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_xlog1py_other_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.special_xlog1py_other_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_xlog1py_other_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.special_xlog1py_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_xlog1py_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_xlog1py_self_scalar :: proc(self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_xlog1py_self_scalar(&outs[0], self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_xlog1py_self_scalar_out :: proc(out: Tensor, self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_xlog1py_self_scalar_out(&outs[0], out.t, self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_xlogy :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_xlogy(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_xlogy_other_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_xlogy_other_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.special_xlogy_other_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_xlogy_other_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.special_xlogy_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_xlogy_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_xlogy_self_scalar :: proc(self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_xlogy_self_scalar(&outs[0], self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_xlogy_self_scalar_out :: proc(out: Tensor, self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_xlogy_self_scalar_out(&outs[0], out.t, self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_zeta :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_zeta(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_zeta_other_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_zeta_other_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.special_zeta_other_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_zeta_other_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.special_zeta_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_zeta_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_zeta_self_scalar :: proc(self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_zeta_self_scalar(&outs[0], self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.special_zeta_self_scalar_out :: proc(out: Tensor, self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_special_zeta_self_scalar_out(&outs[0], out.t, self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.split :: proc(self: *Tensor, split_size: i64, dim: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_split(self.t, split_size, dim)
    arr: [dyn] Tensor = []
    i := 0
    while true {
        c := ptr[i]
        if (c == null) { break }
        arr.append(Tensor{t: c})
        i += 1
    }
    return arr
}

Tensor.split_copy :: proc(self: *Tensor, split_size: i64, dim: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_split_copy(self.t, split_size, dim)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.split_copy_tensor_out :: proc(self: *Tensor, out_data: *torch_bindings.Tensor, out_len: i32, split_size: i64, dim: i64) void {
    torch_bindings.atg_split_copy_tensor_out(out_data, out_len, self.t, split_size, dim)
}

Tensor.split_sizes :: proc(self: *Tensor, split_size_data: *i64, split_size_len: i32, dim: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_split_sizes(self.t, split_size_data, split_size_len, dim)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.split_with_sizes :: proc(self: *Tensor, split_sizes_data: *i64, split_sizes_len: i32, dim: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_split_with_sizes(self.t, split_sizes_data, split_sizes_len, dim)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.split_with_sizes_copy :: proc(self: *Tensor, split_sizes_data: *i64, split_sizes_len: i32, dim: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_split_with_sizes_copy(self.t, split_sizes_data, split_sizes_len, dim)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.split_with_sizes_copy_out :: proc(self: *Tensor, out_data: *torch_bindings.Tensor, out_len: i32, split_sizes_data: *i64, split_sizes_len: i32, dim: i64) void {
    torch_bindings.atg_split_with_sizes_copy_out(out_data, out_len, self.t, split_sizes_data, split_sizes_len, dim)
}

Tensor.sqrt :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sqrt(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.sqrt_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sqrt_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.sqrt_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sqrt_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.square :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_square(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.square_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_square_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.square_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_square_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.squeeze :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_squeeze(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.squeeze_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_squeeze_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.squeeze_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_squeeze_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.squeeze_copy_dim :: proc(self: *Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_squeeze_copy_dim(&outs[0], self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.squeeze_copy_dim_out :: proc(self: *Tensor, out: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_squeeze_copy_dim_out(&outs[0], out.t, self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.squeeze_copy_dims :: proc(self: *Tensor, dim_data: *i64, dim_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_squeeze_copy_dims(&outs[0], self.t, dim_data, dim_len)
    return wrap_and_track(outs[0])
}

Tensor.squeeze_copy_dims_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_squeeze_copy_dims_out(&outs[0], out.t, self.t, dim_data, dim_len)
    return wrap_and_track(outs[0])
}

Tensor.squeeze_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_squeeze_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.squeeze_dim :: proc(self: *Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_squeeze_dim(&outs[0], self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.squeeze_dim_ :: proc(self: *Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_squeeze_dim_(&outs[0], self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.squeeze_dims :: proc(self: *Tensor, dim_data: *i64, dim_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_squeeze_dims(&outs[0], self.t, dim_data, dim_len)
    return wrap_and_track(outs[0])
}

Tensor.squeeze_dims_ :: proc(self: *Tensor, dim_data: *i64, dim_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_squeeze_dims_(&outs[0], self.t, dim_data, dim_len)
    return wrap_and_track(outs[0])
}

Tensor.sspaddmm :: proc(self: *Tensor, mat1: Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sspaddmm(&outs[0], self.t, mat1.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor.sspaddmm_out :: proc(self: *Tensor, out: Tensor, mat1: Tensor, mat2: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sspaddmm_out(&outs[0], out.t, self.t, mat1.t, mat2.t)
    return wrap_and_track(outs[0])
}

Tensor.stack :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_stack(&outs[0], tensors_data, tensors_len, dim)
    return wrap_and_track(outs[0])
}

Tensor.stack_out :: proc(out: Tensor, tensors_data: *torch_bindings.Tensor, tensors_len: i32, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_stack_out(&outs[0], out.t, tensors_data, tensors_len, dim)
    return wrap_and_track(outs[0])
}

Tensor.std :: proc(self: *Tensor, unbiased: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_std(&outs[0], self.t, unbiased)
    return wrap_and_track(outs[0])
}

Tensor.std_correction :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, correction: torch_bindings.Scalar, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_std_correction(&outs[0], self.t, dim_data, dim_len, correction, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.std_correction_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, correction: torch_bindings.Scalar, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_std_correction_out(&outs[0], out.t, self.t, dim_data, dim_len, correction, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.std_dim :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, unbiased: bool, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_std_dim(&outs[0], self.t, dim_data, dim_len, unbiased, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.std_mean :: proc(self: *Tensor, unbiased: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_std_mean(&outs[0], self.t, unbiased)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.std_mean_correction :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, correction: torch_bindings.Scalar, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_std_mean_correction(&outs[0], self.t, dim_data, dim_len, correction, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.std_mean_correction_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, dim_data: *i64, dim_len: i32, correction: torch_bindings.Scalar, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_std_mean_correction_out(&outs[0], out0.t, out1.t, self.t, dim_data, dim_len, correction, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.std_mean_dim :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, unbiased: bool, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_std_mean_dim(&outs[0], self.t, dim_data, dim_len, unbiased, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.std_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, unbiased: bool, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_std_out(&outs[0], out.t, self.t, dim_data, dim_len, unbiased, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.stft :: proc(self: *Tensor, n_fft: i64, hop_length_v: i64, hop_length_null: u8, win_length_v: i64, win_length_null: u8, window: Tensor, normalized: bool, onesided: bool, return_complex: bool, align_to_window: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_stft(&outs[0], self.t, n_fft, hop_length_v, hop_length_null, win_length_v, win_length_null, window.t, normalized, onesided, return_complex, align_to_window)
    return wrap_and_track(outs[0])
}

Tensor.stft_center :: proc(self: *Tensor, n_fft: i64, hop_length_v: i64, hop_length_null: u8, win_length_v: i64, win_length_null: u8, window: Tensor, center: bool, pad_mode_ptr: *u8, pad_mode_len: i32, normalized: bool, onesided: bool, return_complex: bool, align_to_window: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_stft_center(&outs[0], self.t, n_fft, hop_length_v, hop_length_null, win_length_v, win_length_null, window.t, center, pad_mode_ptr, pad_mode_len, normalized, onesided, return_complex, align_to_window)
    return wrap_and_track(outs[0])
}

Tensor.sub :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sub(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.sub_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sub_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.sub_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sub_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.sub_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sub_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.sub_scalar_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sub_scalar_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.sub_scalar_out :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sub_scalar_out(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.subtract :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_subtract(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.subtract_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_subtract_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.subtract_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_subtract_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.subtract_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_subtract_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.subtract_scalar_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_subtract_scalar_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.sum :: proc(self: *Tensor, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sum(&outs[0], self.t, dtype)
    return wrap_and_track(outs[0])
}

Tensor.sum_dim_intlist :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sum_dim_intlist(&outs[0], self.t, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.sum_intlist_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, keepdim: bool, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sum_intlist_out(&outs[0], out.t, self.t, dim_data, dim_len, keepdim, dtype)
    return wrap_and_track(outs[0])
}

Tensor.sum_out :: proc(self: *Tensor, out: Tensor, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sum_out(&outs[0], out.t, self.t, dtype)
    return wrap_and_track(outs[0])
}

Tensor.sum_to_size :: proc(self: *Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_sum_to_size(&outs[0], self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.svd :: proc(self: *Tensor, some: bool, compute_uv: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_svd(&outs[0], self.t, some, compute_uv)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.svd_u :: proc(self: *Tensor, U: Tensor, S: Tensor, V: Tensor, some: bool, compute_uv: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_svd_u(&outs[0], U.t, S.t, V.t, self.t, some, compute_uv)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.swapaxes :: proc(self: *Tensor, axis0: i64, axis1: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_swapaxes(&outs[0], self.t, axis0, axis1)
    return wrap_and_track(outs[0])
}

Tensor.swapaxes_ :: proc(self: *Tensor, axis0: i64, axis1: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_swapaxes_(&outs[0], self.t, axis0, axis1)
    return wrap_and_track(outs[0])
}

Tensor.swapdims :: proc(self: *Tensor, dim0: i64, dim1: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_swapdims(&outs[0], self.t, dim0, dim1)
    return wrap_and_track(outs[0])
}

Tensor.swapdims_ :: proc(self: *Tensor, dim0: i64, dim1: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_swapdims_(&outs[0], self.t, dim0, dim1)
    return wrap_and_track(outs[0])
}

Tensor.t :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_t(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.t_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_t_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.t_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_t_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.t_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_t_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.take :: proc(self: *Tensor, index: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_take(&outs[0], self.t, index.t)
    return wrap_and_track(outs[0])
}

Tensor.take_along_dim :: proc(self: *Tensor, indices: Tensor, dim_v: i64, dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_take_along_dim(&outs[0], self.t, indices.t, dim_v, dim_null)
    return wrap_and_track(outs[0])
}

Tensor.take_along_dim_out :: proc(self: *Tensor, out: Tensor, indices: Tensor, dim_v: i64, dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_take_along_dim_out(&outs[0], out.t, self.t, indices.t, dim_v, dim_null)
    return wrap_and_track(outs[0])
}

Tensor.take_out :: proc(self: *Tensor, out: Tensor, index: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_take_out(&outs[0], out.t, self.t, index.t)
    return wrap_and_track(outs[0])
}

Tensor.tan :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tan(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.tan_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tan_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.tan_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tan_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.tanh :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tanh(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.tanh_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tanh_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.tanh_backward :: proc(grad_output: Tensor, output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tanh_backward(&outs[0], grad_output.t, output.t)
    return wrap_and_track(outs[0])
}

Tensor.tanh_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, output: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tanh_backward_grad_input(&outs[0], grad_input.t, grad_output.t, output.t)
    return wrap_and_track(outs[0])
}

Tensor.tanh_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tanh_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.tensor_split :: proc(self: *Tensor, sections: i64, dim: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_tensor_split(self.t, sections, dim)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.tensor_split_indices :: proc(self: *Tensor, indices_data: *i64, indices_len: i32, dim: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_tensor_split_indices(self.t, indices_data, indices_len, dim)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.tensor_split_tensor_indices_or_sections :: proc(self: *Tensor, tensor_indices_or_sections: Tensor, dim: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_tensor_split_tensor_indices_or_sections(self.t, tensor_indices_or_sections.t, dim)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.tensordot :: proc(self: *Tensor, other: Tensor, dims_self_data: *i64, dims_self_len: i32, dims_other_data: *i64, dims_other_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tensordot(&outs[0], self.t, other.t, dims_self_data, dims_self_len, dims_other_data, dims_other_len)
    return wrap_and_track(outs[0])
}

Tensor.tensordot_out :: proc(self: *Tensor, out: Tensor, other: Tensor, dims_self_data: *i64, dims_self_len: i32, dims_other_data: *i64, dims_other_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tensordot_out(&outs[0], out.t, self.t, other.t, dims_self_data, dims_self_len, dims_other_data, dims_other_len)
    return wrap_and_track(outs[0])
}

Tensor.threshold :: proc(self: *Tensor, threshold: torch_bindings.Scalar, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_threshold(&outs[0], self.t, threshold, value)
    return wrap_and_track(outs[0])
}

Tensor.threshold_ :: proc(self: *Tensor, threshold: torch_bindings.Scalar, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_threshold_(&outs[0], self.t, threshold, value)
    return wrap_and_track(outs[0])
}

Tensor.threshold_backward :: proc(self: *Tensor, grad_output: Tensor, threshold: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_threshold_backward(&outs[0], grad_output.t, self.t, threshold)
    return wrap_and_track(outs[0])
}

Tensor.threshold_backward_grad_input :: proc(self: *Tensor, grad_input: Tensor, grad_output: Tensor, threshold: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_threshold_backward_grad_input(&outs[0], grad_input.t, grad_output.t, self.t, threshold)
    return wrap_and_track(outs[0])
}

Tensor.threshold_out :: proc(self: *Tensor, out: Tensor, threshold: torch_bindings.Scalar, value: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_threshold_out(&outs[0], out.t, self.t, threshold, value)
    return wrap_and_track(outs[0])
}

Tensor.tile :: proc(self: *Tensor, dims_data: *i64, dims_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tile(&outs[0], self.t, dims_data, dims_len)
    return wrap_and_track(outs[0])
}

Tensor.to :: proc(self: *Tensor, device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to(&outs[0], self.t, device)
    return wrap_and_track(outs[0])
}

Tensor.to_dense :: proc(self: *Tensor, dtype: i32, masked_grad: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_dense(&outs[0], self.t, dtype, masked_grad)
    return wrap_and_track(outs[0])
}

Tensor.to_dense_backward :: proc(self: *Tensor, grad: Tensor, masked_grad: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_dense_backward(&outs[0], grad.t, self.t, masked_grad)
    return wrap_and_track(outs[0])
}

Tensor.to_device :: proc(self: *Tensor, device: i32, dtype: i32, non_blocking: bool, copy: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_device(&outs[0], self.t, device, dtype, non_blocking, copy)
    return wrap_and_track(outs[0])
}

Tensor.to_dtype :: proc(self: *Tensor, dtype: i32, non_blocking: bool, copy: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_dtype(&outs[0], self.t, dtype, non_blocking, copy)
    return wrap_and_track(outs[0])
}

Tensor.to_dtype_layout :: proc(self: *Tensor, options_kind: i32, options_device: i32, non_blocking: bool, copy: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_dtype_layout(&outs[0], self.t, options_kind, options_device, non_blocking, copy)
    return wrap_and_track(outs[0])
}

Tensor.to_mkldnn :: proc(self: *Tensor, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_mkldnn(&outs[0], self.t, dtype)
    return wrap_and_track(outs[0])
}

Tensor.to_mkldnn_backward :: proc(self: *Tensor, grad: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_mkldnn_backward(&outs[0], grad.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.to_mkldnn_out :: proc(self: *Tensor, out: Tensor, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_mkldnn_out(&outs[0], out.t, self.t, dtype)
    return wrap_and_track(outs[0])
}

Tensor.to_other :: proc(self: *Tensor, other: Tensor, non_blocking: bool, copy: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_other(&outs[0], self.t, other.t, non_blocking, copy)
    return wrap_and_track(outs[0])
}

Tensor.to_padded_tensor :: proc(self: *Tensor, padding: f64, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_padded_tensor(&outs[0], self.t, padding, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor.to_padded_tensor_out :: proc(self: *Tensor, out: Tensor, padding: f64, output_size_data: *i64, output_size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_padded_tensor_out(&outs[0], out.t, self.t, padding, output_size_data, output_size_len)
    return wrap_and_track(outs[0])
}

Tensor.to_sparse :: proc(self: *Tensor, layout: i8, blocksize_data: *i64, blocksize_len: i32, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_sparse(&outs[0], self.t, layout, blocksize_data, blocksize_len, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor.to_sparse_bsc :: proc(self: *Tensor, blocksize_data: *i64, blocksize_len: i32, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_sparse_bsc(&outs[0], self.t, blocksize_data, blocksize_len, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor.to_sparse_bsr :: proc(self: *Tensor, blocksize_data: *i64, blocksize_len: i32, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_sparse_bsr(&outs[0], self.t, blocksize_data, blocksize_len, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor.to_sparse_csc :: proc(self: *Tensor, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_sparse_csc(&outs[0], self.t, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor.to_sparse_csr :: proc(self: *Tensor, dense_dim_v: i64, dense_dim_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_sparse_csr(&outs[0], self.t, dense_dim_v, dense_dim_null)
    return wrap_and_track(outs[0])
}

Tensor.to_sparse_sparse_dim :: proc(self: *Tensor, sparse_dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_to_sparse_sparse_dim(&outs[0], self.t, sparse_dim)
    return wrap_and_track(outs[0])
}

Tensor.topk :: proc(self: *Tensor, k: i64, dim: i64, largest: bool, sorted: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_topk(&outs[0], self.t, k, dim, largest, sorted)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.topk_values :: proc(self: *Tensor, values: Tensor, indices: Tensor, k: i64, dim: i64, largest: bool, sorted: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_topk_values(&outs[0], values.t, indices.t, self.t, k, dim, largest, sorted)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.totype :: proc(self: *Tensor, scalar_type: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_totype(&outs[0], self.t, scalar_type)
    return wrap_and_track(outs[0])
}

Tensor.trace :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_trace(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.trace_backward :: proc(grad: Tensor, sizes_data: *i64, sizes_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_trace_backward(&outs[0], grad.t, sizes_data, sizes_len)
    return wrap_and_track(outs[0])
}

Tensor.trace_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_trace_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.transpose :: proc(self: *Tensor, dim0: i64, dim1: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_transpose(&outs[0], self.t, dim0, dim1)
    return wrap_and_track(outs[0])
}

Tensor.transpose_ :: proc(self: *Tensor, dim0: i64, dim1: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_transpose_(&outs[0], self.t, dim0, dim1)
    return wrap_and_track(outs[0])
}

Tensor.transpose_copy :: proc(self: *Tensor, dim0: i64, dim1: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_transpose_copy(&outs[0], self.t, dim0, dim1)
    return wrap_and_track(outs[0])
}

Tensor.transpose_copy_int_out :: proc(self: *Tensor, out: Tensor, dim0: i64, dim1: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_transpose_copy_int_out(&outs[0], out.t, self.t, dim0, dim1)
    return wrap_and_track(outs[0])
}

Tensor.trapezoid :: proc(y: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_trapezoid(&outs[0], y.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.trapezoid_x :: proc(y: Tensor, x: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_trapezoid_x(&outs[0], y.t, x.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.trapz :: proc(y: Tensor, x: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_trapz(&outs[0], y.t, x.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.trapz_dx :: proc(y: Tensor, dx: f64, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_trapz_dx(&outs[0], y.t, dx, dim)
    return wrap_and_track(outs[0])
}

Tensor.triangular_solve :: proc(self: *Tensor, A: Tensor, upper: bool, transpose: bool, unitriangular: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_triangular_solve(&outs[0], self.t, A.t, upper, transpose, unitriangular)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.triangular_solve_x :: proc(self: *Tensor, X: Tensor, M: Tensor, A: Tensor, upper: bool, transpose: bool, unitriangular: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_triangular_solve_x(&outs[0], X.t, M.t, self.t, A.t, upper, transpose, unitriangular)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.tril :: proc(self: *Tensor, diagonal: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tril(&outs[0], self.t, diagonal)
    return wrap_and_track(outs[0])
}

Tensor.tril_ :: proc(self: *Tensor, diagonal: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tril_(&outs[0], self.t, diagonal)
    return wrap_and_track(outs[0])
}

Tensor.tril_indices :: proc(row: i64, col: i64, offset: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tril_indices(&outs[0], row, col, offset, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.tril_indices_out :: proc(out: Tensor, row: i64, col: i64, offset: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tril_indices_out(&outs[0], out.t, row, col, offset)
    return wrap_and_track(outs[0])
}

Tensor.tril_out :: proc(self: *Tensor, out: Tensor, diagonal: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_tril_out(&outs[0], out.t, self.t, diagonal)
    return wrap_and_track(outs[0])
}

Tensor.triplet_margin_loss :: proc(anchor: Tensor, positive: Tensor, negative: Tensor, margin: f64, p: f64, eps: f64, swap: bool, reduction: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_triplet_margin_loss(&outs[0], anchor.t, positive.t, negative.t, margin, p, eps, swap, reduction)
    return wrap_and_track(outs[0])
}

Tensor.triu :: proc(self: *Tensor, diagonal: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_triu(&outs[0], self.t, diagonal)
    return wrap_and_track(outs[0])
}

Tensor.triu_ :: proc(self: *Tensor, diagonal: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_triu_(&outs[0], self.t, diagonal)
    return wrap_and_track(outs[0])
}

Tensor.triu_indices :: proc(row: i64, col: i64, offset: i64, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_triu_indices(&outs[0], row, col, offset, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.triu_indices_out :: proc(out: Tensor, row: i64, col: i64, offset: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_triu_indices_out(&outs[0], out.t, row, col, offset)
    return wrap_and_track(outs[0])
}

Tensor.triu_out :: proc(self: *Tensor, out: Tensor, diagonal: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_triu_out(&outs[0], out.t, self.t, diagonal)
    return wrap_and_track(outs[0])
}

Tensor.true_divide :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_true_divide(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.true_divide_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_true_divide_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.true_divide_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_true_divide_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.true_divide_scalar :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_true_divide_scalar(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.true_divide_scalar_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_true_divide_scalar_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.trunc :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_trunc(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.trunc_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_trunc_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.trunc_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_trunc_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.type_as :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_type_as(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.unbind :: proc(self: *Tensor, dim: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_unbind(self.t, dim)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.unbind_copy :: proc(self: *Tensor, dim: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_unbind_copy(self.t, dim)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.unbind_copy_int_out :: proc(self: *Tensor, out_data: *torch_bindings.Tensor, out_len: i32, dim: i64) void {
    torch_bindings.atg_unbind_copy_int_out(out_data, out_len, self.t, dim)
}

Tensor.unflatten :: proc(self: *Tensor, dim: i64, sizes_data: *i64, sizes_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_unflatten(&outs[0], self.t, dim, sizes_data, sizes_len)
    return wrap_and_track(outs[0])
}

Tensor.unflatten_dense_tensors :: proc(flat: Tensor, tensors_data: *torch_bindings.Tensor, tensors_len: i32) [dyn] Tensor {
    ptr := torch_bindings.atg_unflatten_dense_tensors(flat.t, tensors_data, tensors_len)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.unfold :: proc(self: *Tensor, dimension: i64, size: i64, step: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_unfold(&outs[0], self.t, dimension, size, step)
    return wrap_and_track(outs[0])
}

Tensor.unfold_backward :: proc(grad_in: Tensor, input_sizes_data: *i64, input_sizes_len: i32, dim: i64, size: i64, step: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_unfold_backward(&outs[0], grad_in.t, input_sizes_data, input_sizes_len, dim, size, step)
    return wrap_and_track(outs[0])
}

Tensor.unfold_backward_out :: proc(out: Tensor, grad_in: Tensor, input_sizes_data: *i64, input_sizes_len: i32, dim: i64, size: i64, step: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_unfold_backward_out(&outs[0], out.t, grad_in.t, input_sizes_data, input_sizes_len, dim, size, step)
    return wrap_and_track(outs[0])
}

Tensor.unfold_copy :: proc(self: *Tensor, dimension: i64, size: i64, step: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_unfold_copy(&outs[0], self.t, dimension, size, step)
    return wrap_and_track(outs[0])
}

Tensor.unfold_copy_out :: proc(self: *Tensor, out: Tensor, dimension: i64, size: i64, step: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_unfold_copy_out(&outs[0], out.t, self.t, dimension, size, step)
    return wrap_and_track(outs[0])
}

Tensor.uniform :: proc(self: *Tensor, from: f64, to: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_uniform(&outs[0], self.t, from, to)
    return wrap_and_track(outs[0])
}

Tensor.uniform_ :: proc(self: *Tensor, from: f64, to: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_uniform_(&outs[0], self.t, from, to)
    return wrap_and_track(outs[0])
}

Tensor.uniform_out :: proc(self: *Tensor, out: Tensor, from: f64, to: f64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_uniform_out(&outs[0], out.t, self.t, from, to)
    return wrap_and_track(outs[0])
}

Tensor.unique_consecutive :: proc(self: *Tensor, return_inverse: bool, return_counts: bool, dim_v: i64, dim_null: u8) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_unique_consecutive(&outs[0], self.t, return_inverse, return_counts, dim_v, dim_null)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.unique_consecutive_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, return_inverse: bool, return_counts: bool, dim_v: i64, dim_null: u8) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_unique_consecutive_out(&outs[0], out0.t, out1.t, out2.t, self.t, return_inverse, return_counts, dim_v, dim_null)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.unique_dim :: proc(self: *Tensor, dim: i64, sorted: bool, return_inverse: bool, return_counts: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_unique_dim(&outs[0], self.t, dim, sorted, return_inverse, return_counts)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.unique_dim_consecutive :: proc(self: *Tensor, dim: i64, return_inverse: bool, return_counts: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_unique_dim_consecutive(&outs[0], self.t, dim, return_inverse, return_counts)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.unique_dim_consecutive_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, dim: i64, return_inverse: bool, return_counts: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_unique_dim_consecutive_out(&outs[0], out0.t, out1.t, out2.t, self.t, dim, return_inverse, return_counts)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.unique_dim_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, out2: Tensor, dim: i64, sorted: bool, return_inverse: bool, return_counts: bool) (Tensor, Tensor, Tensor) {
    outs: [3] torch_bindings.Tensor = undefined
    torch_bindings.atg_unique_dim_out(&outs[0], out0.t, out1.t, out2.t, self.t, dim, sorted, return_inverse, return_counts)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]), wrap_and_track(outs[2]))
}

Tensor.unsafe_chunk :: proc(self: *Tensor, chunks: i64, dim: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_unsafe_chunk(self.t, chunks, dim)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.unsafe_split :: proc(self: *Tensor, split_size: i64, dim: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_unsafe_split(self.t, split_size, dim)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.unsafe_split_tensor_out :: proc(self: *Tensor, out_data: *torch_bindings.Tensor, out_len: i32, split_size: i64, dim: i64) void {
    torch_bindings.atg_unsafe_split_tensor_out(out_data, out_len, self.t, split_size, dim)
}

Tensor.unsafe_split_with_sizes :: proc(self: *Tensor, split_sizes_data: *i64, split_sizes_len: i32, dim: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_unsafe_split_with_sizes(self.t, split_sizes_data, split_sizes_len, dim)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.unsafe_split_with_sizes_out :: proc(self: *Tensor, out_data: *torch_bindings.Tensor, out_len: i32, split_sizes_data: *i64, split_sizes_len: i32, dim: i64) void {
    torch_bindings.atg_unsafe_split_with_sizes_out(out_data, out_len, self.t, split_sizes_data, split_sizes_len, dim)
}

Tensor.unsqueeze :: proc(self: *Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_unsqueeze(&outs[0], self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.unsqueeze_ :: proc(self: *Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_unsqueeze_(&outs[0], self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.unsqueeze_copy :: proc(self: *Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_unsqueeze_copy(&outs[0], self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.unsqueeze_copy_out :: proc(self: *Tensor, out: Tensor, dim: i64) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_unsqueeze_copy_out(&outs[0], out.t, self.t, dim)
    return wrap_and_track(outs[0])
}

Tensor.upsample_bicubic2d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_bicubic2d(&outs[0], self.t, output_size_data, output_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_bicubic2d_backward :: proc(grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_bicubic2d_backward(&outs[0], grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_bicubic2d_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_bicubic2d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_bicubic2d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_bicubic2d_out(&outs[0], out.t, self.t, output_size_data, output_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_bicubic2d_vec :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scale_factors_data: *f64, scale_factors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_bicubic2d_vec(&outs[0], self.t, output_size_data, output_size_len, align_corners, scale_factors_data, scale_factors_len)
    return wrap_and_track(outs[0])
}

Tensor.upsample_bilinear2d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_bilinear2d(&outs[0], self.t, output_size_data, output_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_bilinear2d_backward :: proc(grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_bilinear2d_backward(&outs[0], grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_bilinear2d_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_bilinear2d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_bilinear2d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_bilinear2d_out(&outs[0], out.t, self.t, output_size_data, output_size_len, align_corners, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_bilinear2d_vec :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scale_factors_data: *f64, scale_factors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_bilinear2d_vec(&outs[0], self.t, output_size_data, output_size_len, align_corners, scale_factors_data, scale_factors_len)
    return wrap_and_track(outs[0])
}

Tensor.upsample_bilinear2d_vec_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scale_factors_data: *f64, scale_factors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_bilinear2d_vec_out(&outs[0], out.t, self.t, output_size_data, output_size_len, align_corners, scale_factors_data, scale_factors_len)
    return wrap_and_track(outs[0])
}

Tensor.upsample_linear1d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scales_v: f64, scales_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_linear1d(&outs[0], self.t, output_size_data, output_size_len, align_corners, scales_v, scales_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_linear1d_backward :: proc(grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, align_corners: bool, scales_v: f64, scales_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_linear1d_backward(&outs[0], grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, align_corners, scales_v, scales_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_linear1d_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, align_corners: bool, scales_v: f64, scales_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_linear1d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, align_corners, scales_v, scales_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_linear1d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scales_v: f64, scales_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_linear1d_out(&outs[0], out.t, self.t, output_size_data, output_size_len, align_corners, scales_v, scales_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_linear1d_vec :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scale_factors_data: *f64, scale_factors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_linear1d_vec(&outs[0], self.t, output_size_data, output_size_len, align_corners, scale_factors_data, scale_factors_len)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest1d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, scales_v: f64, scales_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest1d(&outs[0], self.t, output_size_data, output_size_len, scales_v, scales_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest1d_backward :: proc(grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, scales_v: f64, scales_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest1d_backward(&outs[0], grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, scales_v, scales_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest1d_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, scales_v: f64, scales_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest1d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, scales_v, scales_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest1d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, scales_v: f64, scales_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest1d_out(&outs[0], out.t, self.t, output_size_data, output_size_len, scales_v, scales_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest1d_vec :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, scale_factors_data: *f64, scale_factors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest1d_vec(&outs[0], self.t, output_size_data, output_size_len, scale_factors_data, scale_factors_len)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest2d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest2d(&outs[0], self.t, output_size_data, output_size_len, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest2d_backward :: proc(grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest2d_backward(&outs[0], grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest2d_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest2d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest2d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest2d_out(&outs[0], out.t, self.t, output_size_data, output_size_len, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest2d_vec :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, scale_factors_data: *f64, scale_factors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest2d_vec(&outs[0], self.t, output_size_data, output_size_len, scale_factors_data, scale_factors_len)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest2d_vec_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, scale_factors_data: *f64, scale_factors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest2d_vec_out(&outs[0], out.t, self.t, output_size_data, output_size_len, scale_factors_data, scale_factors_len)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest3d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, scales_d_v: f64, scales_d_null: u8, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest3d(&outs[0], self.t, output_size_data, output_size_len, scales_d_v, scales_d_null, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest3d_backward :: proc(grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, scales_d_v: f64, scales_d_null: u8, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest3d_backward(&outs[0], grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, scales_d_v, scales_d_null, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest3d_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, scales_d_v: f64, scales_d_null: u8, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest3d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, scales_d_v, scales_d_null, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest3d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, scales_d_v: f64, scales_d_null: u8, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest3d_out(&outs[0], out.t, self.t, output_size_data, output_size_len, scales_d_v, scales_d_null, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_nearest3d_vec :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, scale_factors_data: *f64, scale_factors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_nearest3d_vec(&outs[0], self.t, output_size_data, output_size_len, scale_factors_data, scale_factors_len)
    return wrap_and_track(outs[0])
}

Tensor.upsample_trilinear3d :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scales_d_v: f64, scales_d_null: u8, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_trilinear3d(&outs[0], self.t, output_size_data, output_size_len, align_corners, scales_d_v, scales_d_null, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_trilinear3d_backward :: proc(grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, align_corners: bool, scales_d_v: f64, scales_d_null: u8, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_trilinear3d_backward(&outs[0], grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, align_corners, scales_d_v, scales_d_null, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_trilinear3d_backward_grad_input :: proc(grad_input: Tensor, grad_output: Tensor, output_size_data: *i64, output_size_len: i32, input_size_data: *i64, input_size_len: i32, align_corners: bool, scales_d_v: f64, scales_d_null: u8, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_trilinear3d_backward_grad_input(&outs[0], grad_input.t, grad_output.t, output_size_data, output_size_len, input_size_data, input_size_len, align_corners, scales_d_v, scales_d_null, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_trilinear3d_out :: proc(self: *Tensor, out: Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scales_d_v: f64, scales_d_null: u8, scales_h_v: f64, scales_h_null: u8, scales_w_v: f64, scales_w_null: u8) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_trilinear3d_out(&outs[0], out.t, self.t, output_size_data, output_size_len, align_corners, scales_d_v, scales_d_null, scales_h_v, scales_h_null, scales_w_v, scales_w_null)
    return wrap_and_track(outs[0])
}

Tensor.upsample_trilinear3d_vec :: proc(self: *Tensor, output_size_data: *i64, output_size_len: i32, align_corners: bool, scale_factors_data: *f64, scale_factors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_upsample_trilinear3d_vec(&outs[0], self.t, output_size_data, output_size_len, align_corners, scale_factors_data, scale_factors_len)
    return wrap_and_track(outs[0])
}

Tensor.value_selecting_reduction_backward :: proc(grad: Tensor, dim: i64, indices: Tensor, sizes_data: *i64, sizes_len: i32, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_value_selecting_reduction_backward(&outs[0], grad.t, dim, indices.t, sizes_data, sizes_len, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.values :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_values(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.values_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_values_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.values_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_values_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.vander :: proc(x: Tensor, n_v: i64, n_null: u8, increasing: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_vander(&outs[0], x.t, n_v, n_null, increasing)
    return wrap_and_track(outs[0])
}

Tensor.var :: proc(self: *Tensor, unbiased: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_var(&outs[0], self.t, unbiased)
    return wrap_and_track(outs[0])
}

Tensor.var_correction :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, correction: torch_bindings.Scalar, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_var_correction(&outs[0], self.t, dim_data, dim_len, correction, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.var_correction_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, correction: torch_bindings.Scalar, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_var_correction_out(&outs[0], out.t, self.t, dim_data, dim_len, correction, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.var_dim :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, unbiased: bool, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_var_dim(&outs[0], self.t, dim_data, dim_len, unbiased, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.var_mean :: proc(self: *Tensor, unbiased: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_var_mean(&outs[0], self.t, unbiased)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.var_mean_correction :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, correction: torch_bindings.Scalar, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_var_mean_correction(&outs[0], self.t, dim_data, dim_len, correction, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.var_mean_correction_out :: proc(self: *Tensor, out0: Tensor, out1: Tensor, dim_data: *i64, dim_len: i32, correction: torch_bindings.Scalar, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_var_mean_correction_out(&outs[0], out0.t, out1.t, self.t, dim_data, dim_len, correction, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.var_mean_dim :: proc(self: *Tensor, dim_data: *i64, dim_len: i32, unbiased: bool, keepdim: bool) (Tensor, Tensor) {
    outs: [2] torch_bindings.Tensor = undefined
    torch_bindings.atg_var_mean_dim(&outs[0], self.t, dim_data, dim_len, unbiased, keepdim)
    return (wrap_and_track(outs[0]), wrap_and_track(outs[1]))
}

Tensor.var_out :: proc(self: *Tensor, out: Tensor, dim_data: *i64, dim_len: i32, unbiased: bool, keepdim: bool) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_var_out(&outs[0], out.t, self.t, dim_data, dim_len, unbiased, keepdim)
    return wrap_and_track(outs[0])
}

Tensor.vdot :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_vdot(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.vdot_out :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_vdot_out(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.view :: proc(self: *Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_view(&outs[0], self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.view_as :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_view_as(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.view_as_complex :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_view_as_complex(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.view_as_complex_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_view_as_complex_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.view_as_complex_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_view_as_complex_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.view_as_real :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_view_as_real(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.view_as_real_copy :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_view_as_real_copy(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.view_as_real_copy_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_view_as_real_copy_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.view_copy :: proc(self: *Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_view_copy(&outs[0], self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.view_copy_dtype :: proc(self: *Tensor, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_view_copy_dtype(&outs[0], self.t, dtype)
    return wrap_and_track(outs[0])
}

Tensor.view_copy_dtype_out :: proc(self: *Tensor, out: Tensor, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_view_copy_dtype_out(&outs[0], out.t, self.t, dtype)
    return wrap_and_track(outs[0])
}

Tensor.view_copy_out :: proc(self: *Tensor, out: Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_view_copy_out(&outs[0], out.t, self.t, size_data, size_len)
    return wrap_and_track(outs[0])
}

Tensor.view_dtype :: proc(self: *Tensor, dtype: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_view_dtype(&outs[0], self.t, dtype)
    return wrap_and_track(outs[0])
}

Tensor.vsplit :: proc(self: *Tensor, sections: i64) [dyn] Tensor {
    ptr := torch_bindings.atg_vsplit(self.t, sections)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.vsplit_array :: proc(self: *Tensor, indices_data: *i64, indices_len: i32) [dyn] Tensor {
    ptr := torch_bindings.atg_vsplit_array(self.t, indices_data, indices_len)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.vstack :: proc(tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_vstack(&outs[0], tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.vstack_out :: proc(out: Tensor, tensors_data: *torch_bindings.Tensor, tensors_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_vstack_out(&outs[0], out.t, tensors_data, tensors_len)
    return wrap_and_track(outs[0])
}

Tensor.where :: proc(condition: Tensor) [dyn] Tensor {
    ptr := torch_bindings.atg_where(condition.t)
    arr: [dyn] Tensor = []
    ptr_addr := ptr.^i64
    i := 0
    while true {
        elem_addr := ptr_addr + i * 8
        elem_ptr := elem_addr.^*torch_bindings.Tensor
        c := elem_ptr.*
        if (c == null) { break }
        arr.append(wrap_and_track(c))
        i += 1
    }
    torch_bindings.free(ptr.^*void)
    return arr
}

Tensor.where_scalar :: proc(condition: Tensor, self_scalar: torch_bindings.Scalar, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_where_scalar(&outs[0], condition.t, self_scalar, other)
    return wrap_and_track(outs[0])
}

Tensor.where_scalarother :: proc(self: *Tensor, condition: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_where_scalarother(&outs[0], condition.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.where_scalarself :: proc(condition: Tensor, self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_where_scalarself(&outs[0], condition.t, self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.where_self :: proc(self: *Tensor, condition: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_where_self(&outs[0], condition.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.where_self_out :: proc(self: *Tensor, out: Tensor, condition: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_where_self_out(&outs[0], out.t, condition.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.xlogy :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_xlogy(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.xlogy_ :: proc(self: *Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_xlogy_(&outs[0], self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.xlogy_outscalar_other :: proc(self: *Tensor, out: Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_xlogy_outscalar_other(&outs[0], out.t, self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.xlogy_outscalar_self :: proc(out: Tensor, self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_xlogy_outscalar_self(&outs[0], out.t, self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.xlogy_outtensor :: proc(self: *Tensor, out: Tensor, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_xlogy_outtensor(&outs[0], out.t, self.t, other.t)
    return wrap_and_track(outs[0])
}

Tensor.xlogy_scalar_other :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_xlogy_scalar_other(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.xlogy_scalar_other_ :: proc(self: *Tensor, other: torch_bindings.Scalar) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_xlogy_scalar_other_(&outs[0], self.t, other)
    return wrap_and_track(outs[0])
}

Tensor.xlogy_scalar_self :: proc(self_scalar: torch_bindings.Scalar, other: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_xlogy_scalar_self(&outs[0], self_scalar, other.t)
    return wrap_and_track(outs[0])
}

Tensor.zero :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_zero(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.zero_ :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_zero_(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.zero_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_zero_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.zeros :: proc(size_data: *i64, size_len: i32, options_kind: i32, options_device: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_zeros(&outs[0], size_data, size_len, options_kind, options_device)
    return wrap_and_track(outs[0])
}

Tensor.zeros_like :: proc(self: *Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_zeros_like(&outs[0], self.t)
    return wrap_and_track(outs[0])
}

Tensor.zeros_like_out :: proc(self: *Tensor, out: Tensor) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_zeros_like_out(&outs[0], out.t, self.t)
    return wrap_and_track(outs[0])
}

Tensor.zeros_out :: proc(out: Tensor, size_data: *i64, size_len: i32) Tensor {
    outs: [1] torch_bindings.Tensor = undefined
    torch_bindings.atg_zeros_out(&outs[0], out.t, size_data, size_len)
    return wrap_and_track(outs[0])
}
