package main

triton :: import "vendor/triton"
cuda :: import "vendor/cuda"
io :: import "std/io"
alloc :: import "std/alloc"
fs :: import "std/fs"

// Triton pointer type alias for i32
Ptr :: triton.Ptr(i32)

ptrcast :: proc(ptr: any) ?*void {
    return ptr.^?*void
}

add_kernel :: @[triton_kernel, triton_target="cuda:75", triton_ptx_version=80] proc(x_ptr: Ptr, y_ptr: Ptr, output_ptr: Ptr, n_elements: i32, comptime BLOCK: i32 = 1024) {
    pid :: triton.program_id(0)
    range :: triton.make_range(0, BLOCK)
    offsets := pid * BLOCK + range
    mask := offsets < n_elements
    zero := triton.splat(0.(i32), BLOCK)
    x := triton.load(x_ptr + offsets, mask, zero)
    y := triton.load(y_ptr + offsets, mask, zero)
    triton.store(output_ptr + offsets, x + y, mask)
}

main :: proc() {
    // 1. Initialize CUDA
    res := cuda.cuInit(0)
    if res != cuda.SUCCESS {
        io.println("Failed to init CUDA: %d", res)
        return
    }

    dev: cuda.CUdevice = undefined
    res = cuda.cuDeviceGet(&dev, 0)
    if res != cuda.SUCCESS {
        io.println("Failed to get device", ())
        return
    }

    ctx: cuda.CUcontext = null
    res = cuda.cuCtxCreate(&ctx, 0, dev)
    if res != cuda.SUCCESS {
        io.println("Failed to create context", ())
        return
    }

    // 2. Load PTX Module
    ptx_path := "out/triton_kernels.ptx"

    mod: cuda.CUmodule = null
    res = cuda.cuModuleLoad(&mod, ptx_path.ptr)
    if res != cuda.SUCCESS {
        io.println("Failed to load module: %d", res)
        return
    }

    kernel: cuda.CUfunction = null
    res = cuda.cuModuleGetFunction(&kernel, mod, "add_kernel".ptr)
    if res != cuda.SUCCESS {
        io.println("Failed to get kernel function", ())
        return
    }

    // 3. Prepare Data
    N: i32 = 1024
    size := N.(usize) * 4 // i32 is 4 bytes

    // Host data
    x_h_ptr := alloc.alloc(size) orelse return
    x_h := x_h_ptr.^*i32

    y_h_ptr := alloc.alloc(size) orelse return
    y_h := y_h_ptr.^(*i32)

    out_h_ptr := alloc.alloc(size) orelse return
    out_h := out_h_ptr.^(*i32)

    i: i32 = 0
    while i < N {
        x_h[i] = i
        y_h[i] = N - i
        i = i + 1
    }

    // Device memory
    x_d: cuda.CUdeviceptr = 0
    y_d: cuda.CUdeviceptr = 0
    out_d: cuda.CUdeviceptr = 0

    _ = cuda.cuMemAlloc(&x_d, size)
    _ = cuda.cuMemAlloc(&y_d, size)
    _ = cuda.cuMemAlloc(&out_d, size)

    _ = cuda.cuMemcpyHtoD(x_d, x_h.^?*void, size)
    _ = cuda.cuMemcpyHtoD(y_d, y_h.^?*void, size)

    block_size: i32 = 1024
    grid_size: i32 = (N + block_size - 1) / block_size

    scratch := triton.alloc_scratch(0, 0)

    // 4. Launch Kernel
    // Params: x_ptr, y_ptr, output_ptr, n_elements, block_size
    base_params: [5]?*void = [
        (&x_d).^?*void,
        (&y_d).^?*void,
        (&out_d).^?*void,
        (&N).^?*void,
        (&block_size).^?*void,
    ]
    params := triton.make_params(5.(usize), base_params, &scratch)

    res = cuda.cuLaunchKernel(kernel, grid_size, 1, 1, block_size, 1, 1, 0, null, (&params).^(?**void), null)

    if res != cuda.SUCCESS {
        io.println("Kernel launch failed: %d", res)
        return
    }

    // 5. Copy back and verify
    _ = cuda.cuMemcpyDtoH(out_h.^?*void, out_d, size)

    // Verify
    success := true
    i = 0
    while i < N {
        expected := x_h[i] + y_h[i]
        if out_h[i] != expected {
            io.print("Error at index %d: expected %d, got %d\n", (i, expected, out_h[i]))
            success = false
            break
        }
        i = i + 1
    }

    if success {
        io.println("Kernel execution successful! Results verified.", ())
    } else {
        io.println("Kernel execution failed verification.", ())
    }

    // Cleanup
    _ = cuda.cuMemFree(x_d)
    _ = cuda.cuMemFree(y_d)
    _ = cuda.cuMemFree(out_d)
    triton.free_scratch(&scratch)
}
