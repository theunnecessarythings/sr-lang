package main

triton :: import "vendor/triton"
cuda :: import "vendor/cuda"
io :: import "std/io"
alloc :: import "std/alloc"

PtrF32 :: triton.Ptr(f32)

// Regression test: tensor value updated in an if-branch, then used after.
// This previously produced MLIR dominance errors when lowered via control-flow.
if_tensor_kernel :: @[triton_kernel, triton_target = "cuda:75", triton_ptx_version = 80] proc(
    x_ptr: PtrF32,
    y_ptr: PtrF32,
    beta_ptr: PtrF32,
    have_beta: i32,
    n_cols: i32,
    comptime BLOCK_SIZE: i32 = 128,
) {
    row_id := triton.program_id(0)
    offs := triton.make_range(0, BLOCK_SIZE)
    mask := offs < triton.splat(n_cols, BLOCK_SIZE)

    base := triton.splat(row_id * n_cols, BLOCK_SIZE)
    x_ptrs := x_ptr + (base + offs)
    y_ptrs := y_ptr + (base + offs)

    x := triton.load(x_ptrs, mask, triton.splat(0.(f32), BLOCK_SIZE))
    y := x

    if have_beta != 0 {
        beta := triton.load(beta_ptr + offs, mask, triton.splat(0.(f32), BLOCK_SIZE))
        y = y + beta
    }

    triton.store(y_ptrs, y, mask)
}

main :: proc() {
    res := cuda.cuInit(0)
    if res != cuda.SUCCESS {
        io.println("Failed to init CUDA: %d", res)
        return
    }

    dev: cuda.CUdevice = undefined
    res = cuda.cuDeviceGet(&dev, 0)
    if res != cuda.SUCCESS {
        io.println("Failed to get device", ())
        return
    }

    ctx: cuda.CUcontext = null
    res = cuda.cuCtxCreate(&ctx, 0, dev)
    if res != cuda.SUCCESS {
        io.println("Failed to create context", ())
        return
    }

    n_rows: i32 = 4
    n_cols: i32 = 128
    size := n_rows.(usize) * n_cols.(usize) * 4

    x_h_ptr := alloc.alloc(size) orelse return
    x_h := x_h_ptr.^*f32
    beta_h_ptr := alloc.alloc(n_cols.(usize) * 4) orelse return
    beta_h := beta_h_ptr.^*f32
    y_h_ptr := alloc.alloc(size) orelse return
    y_h := y_h_ptr.^*f32

    i: i32 = 0
    total: i32 = n_rows * n_cols
    while i < total {
        x_h[i] = (i % 17).(f32) * 0.1
        i = i + 1
    }
    i = 0
    while i < n_cols {
        beta_h[i] = 1.0.(f32)
        i = i + 1
    }

    x_d: cuda.CUdeviceptr = 0
    y_d: cuda.CUdeviceptr = 0
    beta_d: cuda.CUdeviceptr = 0
    _ = cuda.cuMemAlloc(&x_d, size)
    _ = cuda.cuMemAlloc(&y_d, size)
    _ = cuda.cuMemAlloc(&beta_d, n_cols.(usize) * 4)
    _ = cuda.cuMemcpyHtoD(x_d, x_h.^?*void, size)
    _ = cuda.cuMemcpyHtoD(beta_d, beta_h.^?*void, n_cols.(usize) * 4)

    triton.launch(
        if_tensor_kernel,
        grid = (n_rows, 1, 1),
        block = (128, 1, 1),
        BLOCK_SIZE = 128,
        x_d,
        y_d,
        beta_d,
        1,
        n_cols,
    )

    _ = cuda.cuMemcpyDtoH(y_h.^?*void, y_d, size)
    io.println("if-tensor kernel ran", ())

    _ = cuda.cuMemFree(x_d)
    _ = cuda.cuMemFree(y_d)
    _ = cuda.cuMemFree(beta_d)
}
