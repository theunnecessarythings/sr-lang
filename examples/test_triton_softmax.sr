package main

triton :: import "vendor/triton"
cuda :: import "vendor/cuda"
io :: import "std/io"
alloc :: import "std/alloc"
math :: import "std/math"

PtrF32 :: triton.Ptr(f32)

softmax_kernel :: @[triton_kernel, triton_target = "cuda:75", triton_ptx_version = 80] proc(
    output_ptr: PtrF32,
    input_ptr: PtrF32,
    input_row_stride: i32,
    output_row_stride: i32,
    n_rows: i32,
    n_cols: i32,
    comptime BLOCK_SIZE: i32 = 128,
) {
    row_start := triton.program_id(0)
    row_step := triton.num_programs(0)
    row_idx := row_start
    while row_idx < n_rows {
        col_offsets := triton.make_range(0, BLOCK_SIZE)
        row_offset := row_idx * input_row_stride
        offsets := col_offsets + row_offset
        input_ptrs := input_ptr + offsets

        n_cols_vec := triton.splat(n_cols, BLOCK_SIZE)
        mask := col_offsets < n_cols_vec
        neg_inf := triton.splat(-3.4028235e38.(f32), BLOCK_SIZE)

        row := triton.load(input_ptrs, mask, neg_inf)
        row_max := triton.reduce_max(f32, row, 0)
        row_max_bcast := triton.splat(row_max, BLOCK_SIZE)
        row_minus_max := row - row_max_bcast

        numerator := triton.exp(row_minus_max)
        denom := triton.reduce_sum(f32, numerator, 0)
        denom_bcast := triton.splat(denom, BLOCK_SIZE)
        softmax_output := numerator / denom_bcast

        out_row_offset := row_idx * output_row_stride
        out_offsets := col_offsets + out_row_offset
        output_ptrs := output_ptr + out_offsets
        triton.store(output_ptrs, softmax_output, mask)

        row_idx = row_idx + row_step
    }
}

main :: proc() {
    res := cuda.cuInit(0)
    if res != cuda.SUCCESS {
        io.println("Failed to init CUDA: %d", res)
        return
    }

    dev: cuda.CUdevice = undefined
    res = cuda.cuDeviceGet(&dev, 0)
    if res != cuda.SUCCESS {
        io.println("Failed to get device", ())
        return
    }

    ctx: cuda.CUcontext = null
    res = cuda.cuCtxCreate(&ctx, 0, dev)
    if res != cuda.SUCCESS {
        io.println("Failed to create context", ())
        return
    }

    n_rows: i32 = 128
    n_cols: i32 = 128
    size := n_rows.(usize) * n_cols.(usize) * 4

    x_h_ptr := alloc.alloc(size) orelse return
    x_h := x_h_ptr.^*f32

    out_h_ptr := alloc.alloc(size) orelse return
    out_h := out_h_ptr.^*f32

    i: i32 = 0
    total: i32 = n_rows * n_cols
    while i < total {
        x_h[i] = (i % n_cols).(f32)
        i = i + 1
    }

    x_d: cuda.CUdeviceptr = 0
    out_d: cuda.CUdeviceptr = 0

    _ = cuda.cuMemAlloc(&x_d, size)
    _ = cuda.cuMemAlloc(&out_d, size)
    _ = cuda.cuMemcpyHtoD(x_d, x_h.^?*void, size)

    input_row_stride: i32 = n_cols
    output_row_stride: i32 = n_cols

    block_size: i32 = 128
    grid_size: i32 = n_rows
    warps := (block_size + 31) / 32
    shared_mem_bytes: i32 = warps * 4

    triton.launch(
        softmax_kernel,
        grid = (grid_size, 1, 1),
        block = (block_size, 1, 1),
        shared_mem = shared_mem_bytes,
        BLOCK_SIZE = 128,
        out_d,
        x_d,
        input_row_stride,
        output_row_stride,
        n_rows,
        n_cols,
    )

    _ = cuda.cuMemcpyDtoH(out_h.^?*void, out_d, size)

    success := true
    epsilon: f32 = 1.e-3
    row: i32 = 0
    while row < n_rows and success {
        row_base := row * n_cols
        row_max := x_h[row_base]
        col: i32 = 1
        while col < n_cols {
            v := x_h[row_base + col]
            if v > row_max { row_max = v }
            col = col + 1
        }
        sum_exp: f32 = 0
        col = 0
        while col < n_cols {
            v := x_h[row_base + col]
            sum_exp = sum_exp + math.expf(v - row_max)
            col = col + 1
        }
        col = 0
        while col < n_cols {
            v := x_h[row_base + col]
            expected := math.expf(v - row_max) / sum_exp
            actual := out_h[row_base + col]
            diff := actual - expected
            if diff < 0 { diff = -diff }
            if diff > epsilon {
                io.println("Mismatch at row %d col %d: expected %f got %f", row, col, expected, actual)
                success = false
                break
            }
            col = col + 1
        }
        row = row + 1
    }
    if success { io.println("Softmax verification passed", ()) }

    _ = cuda.cuMemFree(x_d)
    _ = cuda.cuMemFree(out_d)
}
